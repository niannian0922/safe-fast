#!/usr/bin/env python3
"""
ä¸»è®­ç»ƒè„šæœ¬ï¼šå®‰å…¨æ•æ·é£è¡Œç«¯åˆ°ç«¯å­¦ä¹ ç³»ç»Ÿ
ç»“åˆGCBF+å’ŒDiffPhysDroneçš„æ–¹æ³•è®º
"""

import jax
import jax.numpy as jnp
import optax
import time
from typing import Dict, List, Tuple
import argparse
import os
import pickle
from pathlib import Path

from core.physics import create_initial_state, create_default_params
from core.perception import create_dummy_pointcloud
from core.safety import SafetyParams
from core.training import (
    CompleteTrainingConfig,
    initialize_complete_training,
    create_complete_training_step,
    test_complete_gradient_flow
)


def create_training_batch(batch_size: int,
                         trajectory_length: int,
                         rng_key: jax.random.PRNGKey,
                         config: CompleteTrainingConfig) -> Dict:
    """
    åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡æ•°æ®
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        trajectory_length: è½¨è¿¹é•¿åº¦
        rng_key: éšæœºæ•°ç§å­
        config: è®­ç»ƒé…ç½®
        
    Returns:
        batch: åŒ…å«æ‰¹æ¬¡æ•°æ®çš„å­—å…¸
    """
    
    keys = jax.random.split(rng_key, batch_size + 3)
    
    # ç”Ÿæˆåˆå§‹çŠ¶æ€æ‰¹æ¬¡
    initial_positions = jax.random.uniform(
        keys[0], (batch_size, 3), minval=-2.0, maxval=2.0
    )
    initial_velocities = jax.random.uniform(
        keys[1], (batch_size, 3), minval=-1.0, maxval=1.0
    )
    
    initial_states = []
    for i in range(batch_size):
        state = create_initial_state(
            position=initial_positions[i],
            velocity=initial_velocities[i]
        )
        initial_states.append(state)
    
    # ç”Ÿæˆç›®æ ‡ä½ç½®å’Œé€Ÿåº¦
    target_positions = jax.random.uniform(
        keys[2], (batch_size, 3), minval=5.0, maxval=12.0
    )
    target_velocities = jax.random.uniform(
        keys[3], (batch_size, 3), minval=0.5, maxval=3.0
    )
    
    # ç”Ÿæˆç‚¹äº‘åºåˆ—
    point_clouds = []
    for i in range(batch_size):
        cloud_seq = []
        for t in range(trajectory_length):
            key_idx = (i * trajectory_length + t) % len(keys)
            cloud = create_dummy_pointcloud(
                keys[key_idx], 
                num_points=config.num_obstacles,
                bounds=config.obstacle_bounds
            )
            cloud_seq.append(cloud)
        point_clouds.append(jnp.stack(cloud_seq))
    
    return {
        'initial_states': initial_states,
        'point_cloud_sequences': jnp.stack(point_clouds),
        'target_positions': target_positions,
        'target_velocities': target_velocities
    }


def evaluate_model(policy_model, gnn_model,
                  policy_params, gnn_params,
                  physics_params, safety_params,
                  config: CompleteTrainingConfig,
                  rng_key: jax.random.PRNGKey,
                  num_eval_episodes: int = 10) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    
    from core.loop import complete_rollout_trajectory
    
    eval_metrics = []
    
    for episode in range(num_eval_episodes):
        episode_key = jax.random.fold_in(rng_key, episode)
        
        # åˆ›å»ºè¯„ä¼°åœºæ™¯
        batch = create_training_batch(1, config.trajectory_length, episode_key, config)
        
        initial_state = batch['initial_states'][0]
        point_cloud_seq = batch['point_cloud_sequences'][0]
        target_pos = batch['target_positions'][0]
        
        # æ‰§è¡Œè½¨è¿¹rollout
        final_carry, trajectory_outputs = complete_rollout_trajectory(
            initial_state=initial_state,
            point_cloud_sequence=point_cloud_seq,
            policy_params=policy_params,
            policy_model=policy_model,
            gnn_params=gnn_params,
            gnn_model=gnn_model,
            physics_params=physics_params,
            safety_params=safety_params,
            trajectory_length=config.trajectory_length,
            dt=config.dt,
            use_rnn=False
        )
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        final_distance = jnp.linalg.norm(final_carry.drone_state.position - target_pos)
        safety_violations = jnp.sum(trajectory_outputs.h < 0)
        mean_cbf = jnp.mean(trajectory_outputs.h)
        success = (final_distance < 2.0) and (safety_violations == 0)
        
        episode_metrics = {
            'final_distance': float(final_distance),
            'safety_violations': float(safety_violations),
            'mean_cbf_value': float(mean_cbf),
            'success': float(success),
            'trajectory_length': config.trajectory_length
        }
        
        eval_metrics.append(episode_metrics)
    
    # èšåˆæŒ‡æ ‡
    aggregated_metrics = {}
    for key in eval_metrics[0].keys():
        if key != 'trajectory_length':
            values = [m[key] for m in eval_metrics]
            aggregated_metrics[f'eval/{key}_mean'] = float(jnp.mean(jnp.array(values)))
            aggregated_metrics[f'eval/{key}_std'] = float(jnp.std(jnp.array(values)))
    
    aggregated_metrics['eval/success_rate'] = aggregated_metrics['eval/success_mean']
    
    return aggregated_metrics


def save_checkpoint(policy_params, gnn_params,
                   policy_opt_state, gnn_opt_state,
                   step: int, save_dir: str):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        'policy_params': policy_params,
        'gnn_params': gnn_params,
        'policy_optimizer_state': policy_opt_state,
        'gnn_optimizer_state': gnn_opt_state,
        'step': step
    }
    
    checkpoint_path = save_path / f'checkpoint_{step}.pkl'
    with open(checkpoint_path, 'wb') as f:
        pickle.dump(checkpoint, f)
    
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {checkpoint_path}")


def main(args):
    """ä¸»è®­ç»ƒå¾ªç¯"""
    
    print("ğŸš å®‰å…¨æ•æ·é£è¡Œç«¯åˆ°ç«¯å­¦ä¹ ç³»ç»Ÿ")
    print("=" * 50)
    
    # è®¾ç½®JAX
    if args.gpu:
        print("ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        jax.config.update("jax_platform_name", "cpu")
        print("ä½¿ç”¨CPUè®­ç»ƒ")
    
    # é…ç½®
    config = CompleteTrainingConfig(
        learning_rate=args.learning_rate,
        trajectory_length=args.trajectory_length,
        batch_size=args.batch_size,
        gradient_clip_norm=args.grad_clip,
    )
    
    physics_params = create_default_params()
    safety_params = SafetyParams()
    
    print(f"è½¨è¿¹é•¿åº¦: {config.trajectory_length}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"å­¦ä¹ ç‡: {config.learning_rate}")
    
    # è¿è¡Œæ¢¯åº¦æµæµ‹è¯•
    print("\næ­¥éª¤1: éªŒè¯ç³»ç»Ÿå®Œæ•´æ€§...")
    if not test_complete_gradient_flow():
        print("âŒ ç³»ç»ŸéªŒè¯å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return
    
    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    print("\næ­¥éª¤2: åˆå§‹åŒ–è®­ç»ƒç»„ä»¶...")
    rng_key = jax.random.PRNGKey(args.seed)
    
    (policy_model, gnn_model,
     policy_params, gnn_params,
     policy_optimizer, gnn_optimizer,
     policy_opt_state, gnn_opt_state) = initialize_complete_training(config, rng_key)
    
    train_step = create_complete_training_step(config, physics_params, safety_params)
    
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    # è®­ç»ƒå¾ªç¯
    print(f"\næ­¥éª¤3: å¼€å§‹è®­ç»ƒ ({args.num_steps} æ­¥)...")
    
    training_metrics = []
    best_success_rate = 0.0
    
    for step in range(args.num_steps):
        step_start_time = time.time()
        
        # ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
        batch_key = jax.random.fold_in(rng_key, step)
        batch = create_training_batch(
            config.batch_size, config.trajectory_length, batch_key, config
        )
        
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤ï¼ˆæš‚æ—¶ä½¿ç”¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        train_key = jax.random.fold_in(rng_key, step + 10000)
        
        (policy_params, gnn_params,
         policy_opt_state, gnn_opt_state,
         train_info) = train_step(
            policy_params, policy_model,
            gnn_params, gnn_model,
            policy_opt_state, gnn_opt_state,
            policy_optimizer, gnn_optimizer,
            batch['initial_states'][0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬
            batch['point_cloud_sequences'][0],
            batch['target_positions'][0],
            batch['target_velocities'][0],
            train_key
        )
        
        step_time = time.time() - step_start_time
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        train_info['step'] = step
        train_info['step_time'] = step_time
        training_metrics.append(train_info)
        
        # æ‰“å°è¿›åº¦
        if step % args.log_interval == 0:
            print(f"æ­¥éª¤ {step:4d}: "
                  f"æŸå¤±={train_info['total_loss']:.4f}, "
                  f"CBFæŸå¤±={train_info.get('cbf_unsafe_penalty', 0):.4f}, "
                  f"å®‰å…¨è¿è§„={train_info.get('safety_violations', 0)}, "
                  f"æ—¶é—´={step_time:.2f}s")
        
        # è¯„ä¼°æ¨¡å‹
        if step % args.eval_interval == 0 and step > 0:
            print(f"\nè¯„ä¼°æ¨¡å‹ (æ­¥éª¤ {step})...")
            eval_key = jax.random.fold_in(rng_key, step + 20000)
            eval_metrics = evaluate_model(
                policy_model, gnn_model,
                policy_params, gnn_params,
                physics_params, safety_params,
                config, eval_key, num_eval_episodes=5
            )
            
            success_rate = eval_metrics['eval/success_rate']
            print(f"æˆåŠŸç‡: {success_rate:.2%}")
            print(f"å¹³å‡æœ€ç»ˆè·ç¦»: {eval_metrics['eval/final_distance_mean']:.3f}")
            print(f"å¹³å‡å®‰å…¨è¿è§„: {eval_metrics['eval/safety_violations_mean']:.1f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if success_rate > best_success_rate:
                best_success_rate = success_rate
                save_checkpoint(
                    policy_params, gnn_params,
                    policy_opt_state, gnn_opt_state,
                    step, args.save_dir
                )
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹ (æˆåŠŸç‡: {success_rate:.2%})")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if step % args.save_interval == 0 and step > 0:
            save_checkpoint(
                policy_params, gnn_params,
                policy_opt_state, gnn_opt_state,
                step, args.save_dir
            )
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æˆåŠŸç‡: {best_success_rate:.2%}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_checkpoint(
        policy_params, gnn_params,
        policy_opt_state, gnn_opt_state,
        args.num_steps, args.save_dir
    )
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = Path(args.save_dir) / 'training_history.pkl'
    with open(history_path, 'wb') as f:
        pickle.dump(training_metrics, f)
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_path}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å®‰å…¨æ•æ·é£è¡Œè®­ç»ƒ')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_steps', type=int, default=1000,
                       help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--trajectory_length', type=int, default=30,
                       help='è½¨è¿¹é•¿åº¦')
    parser.add_argument('--learning_rate', type=float, default=3e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--grad_clip', type=float, default=1.0,
                       help='æ¢¯åº¦è£å‰ªèŒƒæ•°')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºæ•°ç§å­')
    parser.add_argument('--gpu', action='store_true',
                       help='ä½¿ç”¨GPU')
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument('--log_interval', type=int, default=10,
                       help='æ—¥å¿—æ‰“å°é—´éš”')
    parser.add_argument('--eval_interval', type=int, default=100,
                       help='è¯„ä¼°é—´éš”')
    parser.add_argument('--save_interval', type=int, default=200,
                       help='ä¿å­˜é—´éš”')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                       help='ä¿å­˜ç›®å½•')
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    main(args)