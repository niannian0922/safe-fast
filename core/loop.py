"""
æ ¸å¿ƒBPTTå¾ªç¯å®ç° - å®Œå…¨ä¿®å¤ç‰ˆï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£ç¡®é›†æˆ
"""

import jax
import jax.numpy as jnp
from typing import Tuple, NamedTuple, Any, Optional
import chex

from core.physics import DroneState, DroneParams, dynamics_step
from core.policy import state_to_vector
from core.safety import safety_filter, SafetyParams


class LoopCarry(NamedTuple):
    """scanå¾ªç¯çš„carryçŠ¶æ€"""
    drone_state: DroneState
    previous_thrust: chex.Array


class LoopOutput(NamedTuple):
    """scanå¾ªç¯çš„è¾“å‡ºï¼ˆéœ€è¦è®°å½•çš„è½¨è¿¹ä¿¡æ¯ï¼‰"""
    drone_state: DroneState
    action: chex.Array
    actual_thrust: chex.Array
    reward: float
    cbf_value: float
    cbf_gradient: chex.Array
    safe_control: chex.Array
    safety_violation: float


def compute_step_reward(current_state: DroneState,
                       action: chex.Array,
                       next_state: DroneState,
                       target_position: chex.Array,
                       cbf_value: float,
                       safety_violation: float) -> float:
    """è®¡ç®—å•æ­¥å¥–åŠ±/æŸå¤±ï¼ˆåŒ…å«å®‰å…¨å¥–åŠ±ï¼‰"""
    
    # 1. è·ç¦»æŸå¤±
    distance_to_target = jnp.linalg.norm(next_state.position - target_position)
    distance_reward = -distance_to_target
    
    # 2. æ§åˆ¶æˆæœ¬
    control_cost = -0.01 * jnp.sum(action**2)
    
    # 3. é€Ÿåº¦æƒ©ç½šï¼ˆé¿å…è¿‡å¿«ï¼‰
    speed_penalty = -0.001 * jnp.sum(next_state.velocity**2)
    
    # 4. è¾¹ç•Œæƒ©ç½š
    bounds = 20.0
    out_of_bounds_penalty = -1.0 * jnp.sum(
        jnp.maximum(0, jnp.abs(next_state.position) - bounds)
    )
    
    # 5. å®‰å…¨å¥–åŠ± - å…³é”®æ–°å¢
    safety_reward = 2.0 * jnp.maximum(0, cbf_value)  # å¥–åŠ±æ­£çš„CBFå€¼
    safety_penalty = -10.0 * safety_violation  # æƒ©ç½šå®‰å…¨è¿è§„
    
    total_reward = (distance_reward + control_cost + speed_penalty + 
                   out_of_bounds_penalty + safety_reward + safety_penalty)
    
    return total_reward


def create_environment_obstacles(rng_key: chex.PRNGKey,
                               num_obstacles: int = 30,
                               bounds: float = 8.0) -> chex.Array:
    """åˆ›å»ºç¯å¢ƒéšœç¢ç‰©ç‚¹äº‘"""
    return jax.random.uniform(
        rng_key,
        (num_obstacles, 3),
        minval=-bounds,
        maxval=bounds
    )


def create_rollout_functions(policy_model: Any,
                           physics_params: DroneParams,
                           dt: float,
                           perception_fn: Any = None,
                           safety_params: SafetyParams = None,
                           environment_obstacles: chex.Array = None):
    """
    åˆ›å»ºrolloutç›¸å…³å‡½æ•° - å®Œå…¨ä¿®å¤ç‰ˆï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶å‚ä¸è®¡ç®—
    """
    
    def scan_function_with_full_integration(carry: LoopCarry,
                                          x: chex.Array,  # [target_position(3)]
                                          policy_params: Any,
                                          gnn_params: Any = None) -> Tuple[LoopCarry, LoopOutput]:
        """
        å®Œå…¨é›†æˆçš„scanå‡½æ•° - ç¡®ä¿æ‰€æœ‰æ¢¯åº¦æµé€š
        """
        
        # æå–å½“å‰çŠ¶æ€å’Œç›®æ ‡ä½ç½®
        current_state = carry.drone_state
        target_position = x
        
        # === æ„ŸçŸ¥æ¨¡å— ===
        cbf_value = 0.1  # é»˜è®¤ä¸å®‰å…¨å€¼
        grad_cbf = jnp.zeros(3)  # é»˜è®¤æ¢¯åº¦
        
        # å¦‚æœæœ‰å®Œæ•´çš„æ„ŸçŸ¥ç³»ç»Ÿ
        if perception_fn is not None and gnn_params is not None and environment_obstacles is not None:
            # è°ƒç”¨çœŸå®çš„æ„ŸçŸ¥å‡½æ•°
            cbf_value, grad_cbf = perception_fn(
                gnn_params, current_state.position, environment_obstacles
            )
        else:
            # ç®€åŒ–çš„è·ç¦»åŸºCBFï¼ˆç¡®ä¿æœ‰æ¢¯åº¦ï¼‰
            if environment_obstacles is not None:
                distances = jnp.linalg.norm(
                    environment_obstacles - current_state.position, axis=1
                )
                min_distance = jnp.min(distances)
                cbf_value = min_distance - 1.0  # å®‰å…¨è·ç¦»ä¸º1ç±³
                
                # è®¡ç®—æ¢¯åº¦ï¼ˆæŒ‡å‘æœ€è¿‘éšœç¢ç‰©ï¼‰
                closest_idx = jnp.argmin(distances)
                direction = current_state.position - environment_obstacles[closest_idx]
                distance_to_closest = distances[closest_idx]
                grad_cbf = jnp.where(
                    distance_to_closest > 1e-6,
                    direction / jnp.maximum(distance_to_closest, 1e-8),
                    jnp.zeros(3)
                )
        
        # === ç­–ç•¥ç½‘ç»œ ===
        state_vector = state_to_vector(current_state)
        nominal_action = policy_model.apply(policy_params, state_vector)
        
        # === å®‰å…¨å±‚ ===
        safe_action = nominal_action
        safety_violation = 0.0
        
        if safety_params is not None:
            # ä½¿ç”¨çœŸå®çš„å®‰å…¨æ»¤æ³¢å™¨
            safe_action = safety_filter(
                u_nom=nominal_action,
                h=cbf_value,
                grad_h=grad_cbf,
                drone_velocity=current_state.velocity,
                safety_params=safety_params
            )
            
            # è®¡ç®—å®‰å…¨è¿è§„ç¨‹åº¦
            safety_violation = jnp.maximum(0.0, -cbf_value)
        
        # === ç‰©ç†å¼•æ“æ­¥è¿› ===
        new_drone_state, actual_thrust = dynamics_step(
            current_state, safe_action, physics_params, dt, carry.previous_thrust
        )
        
        # === å¥–åŠ±è®¡ç®— ===
        reward = compute_step_reward(
            current_state, safe_action, new_drone_state, target_position, 
            cbf_value, safety_violation
        )
        
        # æ„é€ æ–°çš„carry
        new_carry = LoopCarry(
            drone_state=new_drone_state,
            previous_thrust=actual_thrust
        )
        
        # æ„é€ è¾“å‡º
        output = LoopOutput(
            drone_state=new_drone_state,
            action=nominal_action,
            actual_thrust=actual_thrust,
            reward=reward,
            cbf_value=cbf_value,
            cbf_gradient=grad_cbf,
            safe_control=safe_action,
            safety_violation=safety_violation
        )
        
        return new_carry, output
    
    def rollout_trajectory_fn(policy_params: Any,
                            initial_state: DroneState,
                            target_position: chex.Array,
                            trajectory_length: int,
                            gnn_params: Any = None) -> Tuple[LoopCarry, LoopOutput]:
        """å®Œå…¨é›†æˆçš„è½¨è¿¹rolloutå‡½æ•°"""
        
        # åˆå§‹åŒ–carry
        initial_carry = LoopCarry(
            drone_state=initial_state,
            previous_thrust=jnp.zeros(3)
        )
        
        # å¤–éƒ¨è¾“å…¥åºåˆ—
        xs = jnp.tile(target_position, (trajectory_length, 1))
        
        # é€‰æ‹©scanå‡½æ•°
        def scan_fn_with_params(carry, x):
            return scan_function_with_full_integration(carry, x, policy_params, gnn_params)
        
        # æ‰§è¡Œscan
        final_carry, trajectory_outputs = jax.lax.scan(
            scan_fn_with_params, initial_carry, xs, length=trajectory_length
        )
        
        return final_carry, trajectory_outputs
    
    # JITç¼–è¯‘rolloutå‡½æ•°
    rollout_trajectory_jit = jax.jit(
        rollout_trajectory_fn, 
        static_argnames=['trajectory_length']
    )
    
    return rollout_trajectory_jit


class CompleteBatchRolloutSystem:
    """
    å®Œå…¨é›†æˆçš„æ‰¹é‡rolloutç³»ç»Ÿ
    """
    
    def __init__(self,
                 policy_model: Any,
                 physics_params: DroneParams,
                 dt: float,
                 perception_fn: Any,
                 safety_params: SafetyParams,
                 environment_config: dict = None):
        
        self.policy_model = policy_model
        self.physics_params = physics_params
        self.dt = dt
        self.perception_fn = perception_fn
        self.safety_params = safety_params
        
        # ç¯å¢ƒé…ç½®
        if environment_config is None:
            environment_config = {
                'num_obstacles': 30,
                'obstacle_bounds': 8.0
            }
        self.environment_config = environment_config
        
        # åˆ›å»ºç¯å¢ƒéšœç¢ç‰©
        self.rng_key = jax.random.PRNGKey(42)
        self.environment_obstacles = create_environment_obstacles(
            self.rng_key,
            environment_config['num_obstacles'],
            environment_config['obstacle_bounds']
        )
        
        # é¢„ç¼–è¯‘rolloutå‡½æ•°
        self._rollout_fn = create_rollout_functions(
            policy_model, physics_params, dt, perception_fn, safety_params, self.environment_obstacles
        )
    
    def rollout_single_complete(self,
                               policy_params: Any,
                               gnn_params: Any,
                               initial_state: DroneState,
                               target_position: chex.Array,
                               trajectory_length: int) -> Tuple[LoopCarry, LoopOutput]:
        """å®Œæ•´çš„å•ä¸ªè½¨è¿¹rolloutï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶å‚ä¸"""
        return self._rollout_fn(
            policy_params, initial_state, target_position, trajectory_length, gnn_params
        )


def test_complete_integration():
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿé›†æˆ"""
    print("ğŸ”¬ æµ‹è¯•å®Œæ•´ç³»ç»Ÿé›†æˆ...")
    
    from core.physics import create_initial_state, create_default_params
    from core.policy import create_policy_model
    from core.safety import SafetyParams
    from core.perception import create_perception_system
    
    # è®¾ç½®é˜¶æ®µ
    rng_key = jax.random.PRNGKey(42)
    policy_model = create_policy_model("mlp")
    physics_params = create_default_params()
    safety_params = SafetyParams()
    dt = 0.02
    
    # åˆ›å»ºçœŸå®çš„æ„ŸçŸ¥ç³»ç»Ÿ
    gnn_model, perception_fn = create_perception_system()
    
    # åˆå§‹åŒ–å‚æ•°
    dummy_state = jnp.zeros(13)
    policy_params = policy_model.init(rng_key, dummy_state)
    
    # åˆå§‹åŒ–çœŸå®çš„GNNå‚æ•°
    from core.perception import pointcloud_to_graph, create_dummy_pointcloud
    dummy_cloud = create_dummy_pointcloud(jax.random.split(rng_key)[1], 10)
    dummy_graph = pointcloud_to_graph(jnp.zeros(3), dummy_cloud)
    gnn_params = gnn_model.init(jax.random.split(rng_key)[1], dummy_graph)
    
    # æµ‹è¯•æ•°æ®
    initial_state = create_initial_state()
    target_position = jnp.array([5.0, 5.0, 3.0])
    trajectory_length = 10
    
    # åˆ›å»ºå®Œæ•´ç³»ç»Ÿ
    complete_system = CompleteBatchRolloutSystem(
        policy_model, physics_params, dt, perception_fn, safety_params
    )
    
    print("æ‰§è¡Œå®Œæ•´rollout...")
    final_carry, trajectory_outputs = complete_system.rollout_single_complete(
        policy_params, gnn_params, initial_state, target_position, trajectory_length
    )
    
    print(f"âœ… å®Œæ•´rolloutæˆåŠŸ")
    print(f"CBFå€¼èŒƒå›´: [{jnp.min(trajectory_outputs.cbf_value):.3f}, {jnp.max(trajectory_outputs.cbf_value):.3f}]")
    print(f"å®‰å…¨è¿è§„æ¬¡æ•°: {jnp.sum(trajectory_outputs.safety_violation > 0)}")
    print(f"CBFæ¢¯åº¦èŒƒæ•°: {jnp.mean(jnp.linalg.norm(trajectory_outputs.cbf_gradient, axis=1)):.6f}")
    
    # æµ‹è¯•å®Œæ•´ç³»ç»Ÿçš„æ¢¯åº¦
    print("æµ‹è¯•å®Œæ•´æ¢¯åº¦æµ...")
    
    def complete_loss_fn(policy_params_test, gnn_params_test):
        final_c, traj_out = complete_system.rollout_single_complete(
            policy_params_test, gnn_params_test, initial_state, target_position, trajectory_length
        )
        # ç»¼åˆæŸå¤±ï¼šè·ç¦» + CBFè¿è§„
        distance_loss = jnp.linalg.norm(final_c.drone_state.position - target_position)
        safety_loss = jnp.sum(jnp.maximum(0, -traj_out.cbf_value))
        return distance_loss + 5.0 * safety_loss
    
    grad_fn = jax.grad(complete_loss_fn, argnums=[0, 1])
    policy_grads, gnn_grads = grad_fn(policy_params, gnn_params)
    
    # è®¡ç®—æ¢¯åº¦èŒƒæ•°
    def tree_norm(tree):
        return jnp.sqrt(sum(jnp.sum(leaf**2) for leaf in jax.tree_util.tree_leaves(tree)))
    
    policy_grad_norm = tree_norm(policy_grads)
    gnn_grad_norm = tree_norm(gnn_grads)
    
    print(f"ç­–ç•¥æ¢¯åº¦èŒƒæ•°: {policy_grad_norm:.8f}")
    print(f"GNNæ¢¯åº¦èŒƒæ•°: {gnn_grad_norm:.8f}")
    
    # éªŒè¯æ¢¯åº¦
    assert not jnp.isnan(policy_grad_norm), "ç­–ç•¥æ¢¯åº¦ä¸åº”åŒ…å«NaN"
    assert not jnp.isnan(gnn_grad_norm), "GNNæ¢¯åº¦ä¸åº”åŒ…å«NaN"
    assert policy_grad_norm > 1e-8, f"ç­–ç•¥æ¢¯åº¦è¿‡å°: {policy_grad_norm}"
    assert gnn_grad_norm > 1e-8, f"GNNæ¢¯åº¦è¿‡å°: {gnn_grad_norm}"
    
    print("âœ… å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•é€šè¿‡!")
    
    return True


if __name__ == "__main__":
    test_complete_integration()