"""
Perception Module for Safe Agile Flight System

This module integrates GCBF+ GNN architecture with LiDAR point cloud processing
for single-agent CBF-based safe control.

Key Components:
1. Point cloud to graph conversion (pointcloud_to_graph)
2. GNN-based CBF value and gradient computation
3. JAX native implementation with JIT compilation support

Integration from GCBF+ codebase:
- GNN architecture from gcbfplus/nn/gnn.py
- Graph construction from gcbfplus/utils/graph.py
- CBF network from gcbfplus/algo/module/cbf.py

Adaptations:
- Multi-agent neighbor discovery -> single-agent LiDAR processing
- cvxpylayers -> qpax (handled in safety.py)
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
from flax import struct
import jraph
import functools as ft
from typing import Tuple, Callable, Optional, NamedTuple
from dataclasses import dataclass

# JAX utilities
from jax import vmap, jit
from jax.lax import top_k

# =============================================================================
# DATA STRUCTURES
# =============================================================================

# Import DroneState from physics module to maintain consistency
from core.physics import DroneState

@dataclass(frozen=True)
class GraphConfig:
    """Graph construction parameters"""
    k_neighbors: int = 8  # KNN neighbor count
    max_range: float = 5.0  # Maximum sensing range
    min_points: int = 10  # Minimum point cloud size
    max_points: int = 1000  # Maximum point cloud size (for memory control)
    ego_node_features: int = 10  # Ego node feature dimension
    obstacle_node_features: int = 3  # Obstacle node feature dimension
    edge_features: int = 4  # Edge feature dimension

# =============================================================================
# POINT CLOUD TO GRAPH CONVERSION
# =============================================================================

def compute_pairwise_distances(points1: jnp.ndarray, points2: jnp.ndarray) -> jnp.ndarray:
    """
    Compute pairwise distances between two point sets
    JAX-native implementation with vmap support
    
    Args:
        points1: (N, 3) - First point set
        points2: (M, 3) - Second point set
    Returns:
        distances: (N, M) - Pairwise distance matrix
    """
    # Vectorized distance computation: ||p1 - p2||_2
    diff = points1[:, None, :] - points2[None, :, :]  # (N, M, 3)
    distances = jnp.linalg.norm(diff, axis=2)  # (N, M)
    return distances

def filter_points_by_range(points: jnp.ndarray, drone_pos: jnp.ndarray, max_range: float) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """
    Filter point cloud by distance range from drone
    """
    distances = jnp.linalg.norm(points - drone_pos, axis=1)
    mask = distances <= max_range
    # Use jnp.where to avoid boolean indexing issues
    valid_indices = jnp.where(mask, size=points.shape[0], fill_value=0)[0]
    valid_points = points[valid_indices]
    return valid_points, mask

def find_knn_edges(drone_pos: jnp.ndarray, obstacle_points: jnp.ndarray, k: int) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:
    """
    Build KNN edges for ego-centric graph construction
    JAX/JIT compatible version with fixed-size outputs
    
    Args:
        drone_pos: (3,) - Drone position
        obstacle_points: (N, 3) - Obstacle point cloud
        k: KNN neighbor count (must be static for JIT compilation)
        
    Returns:
        senders: (fixed_size,) - Sender node indices (padded with -1 for invalid)
        receivers: (fixed_size,) - Receiver node indices (padded with -1 for invalid) 
        edge_features: (fixed_size, 4) - Edge features (padded with zeros for invalid)
    """
    n_obstacles = obstacle_points.shape[0]
    n_total = n_obstacles + 1  # +1 for ego drone node
    
    # Combined position array: [drone, obstacle1, obstacle2, ...]
    all_positions = jnp.concatenate([drone_pos[None, :], obstacle_points], axis=0)
    
    # Compute all pairwise distances
    distances = compute_pairwise_distances(all_positions, all_positions)
    
    # Mask diagonal to exclude self-connections
    distances_masked = jnp.where(jnp.eye(n_total), jnp.inf, distances)
    
    # Use static k value
    k_use = min(k, n_total - 1)  # Static computation
    
    # Get k nearest neighbors for each node
    _, top_k_indices = jax.vmap(lambda row: jax.lax.top_k(-row, k_use))(distances_masked)
    
    # Create fixed-size edge arrays 
    max_edges = n_total * k_use
    
    # Create all potential edges (dense format)
    all_senders = jnp.repeat(jnp.arange(n_total), k_use)
    all_receivers = top_k_indices.flatten()
    
    # Compute all edge features
    sender_positions = all_positions[all_senders]
    receiver_positions = all_positions[all_receivers]
    rel_positions = receiver_positions - sender_positions
    edge_distances = jnp.linalg.norm(rel_positions, axis=1, keepdims=True)
    all_edge_features = jnp.concatenate([edge_distances, rel_positions], axis=1)
    
    # Create validity mask (but don't use for indexing)
    # Edge is valid if sender != receiver
    validity_mask = (all_senders != all_receivers) & (all_receivers < n_total)
    
    # Instead of dynamic filtering, use fixed-size arrays and mark invalid edges
    # Invalid edges will have sender/receiver = -1 and zero features
    final_senders = jnp.where(validity_mask, all_senders, -1)
    final_receivers = jnp.where(validity_mask, all_receivers, -1)
    final_features = jnp.where(validity_mask[:, None], all_edge_features, 0.0)
    
    return final_senders, final_receivers, final_features

def pointcloud_to_graph(drone_state: DroneState, point_cloud: jnp.ndarray, config: GraphConfig) -> Tuple[jraph.GraphsTuple, jnp.ndarray]:
    """
    Convert LiDAR point cloud to jraph.GraphsTuple for GNN processing
    
    This is the core adaptation from GCBF+'s multi-agent neighbor discovery
    to single-agent LiDAR processing (replacing multi-agent environment).
    
    Args:
        drone_state: Current drone state
        point_cloud: (N, 3) LiDAR point cloud in drone body frame
        config: Graph construction configuration
    
    Returns:
        graph: jraph.GraphsTuple - Graph structure for GNN
        node_types: Node type array for processing
        
    Graph structure:
        - Node 0: Ego drone node with state features
        - Nodes 1~N: Obstacle nodes (LiDAR points) with position features
        - Edges: Spatial connectivity based on KNN
    """
    # 1. Transform points to world frame
    # point_cloud is in body frame, transform to world frame
    world_points = (drone_state.orientation.T @ point_cloud.T).T + drone_state.position
    
    # 2. Filter points by range (simplified approach)
    distances = jnp.linalg.norm(world_points - drone_state.position, axis=1)
    valid_mask = distances <= config.max_range
    
    # Use padding approach instead of filtering to avoid dynamic shapes
    n_total_points = world_points.shape[0]
    valid_points = world_points  # Keep all points for now
    n_valid = n_total_points
    if n_valid < config.min_points:
        # Pad with dummy points if too few
        dummy_points = jnp.zeros((config.min_points, 3))
        valid_points = dummy_points
        n_valid = config.min_points
    elif n_valid > config.max_points:
        # Subsample if too many points
        key = jax.random.PRNGKey(0)  # Use proper JAX random API
        indices = jax.random.choice(
            key, n_valid, (config.max_points,), replace=False
        )
        valid_points = valid_points[indices]
        n_valid = config.max_points
    
    # 3. Construct node features
    # Ego node features: [position(3), velocity(3), acceleration(3), forward_direction(1)]
    # Simplified for point-mass model (no angular_velocity)
    ego_features = jnp.concatenate([
        drone_state.position,
        drone_state.velocity, 
        drone_state.acceleration,
        jnp.array([0.0])  # Dummy forward direction for point mass
    ])  # (10,) -> truncate to ego_node_features dimension
    ego_features = ego_features[:config.ego_node_features]
    
    # Obstacle node features: relative position in drone body frame
    obstacle_features = (drone_state.orientation @ (valid_points - drone_state.position).T).T
    obstacle_features = obstacle_features[:, :config.obstacle_node_features]  # Keep only x,y,z
    
    # Combine all node features (pad ego features to match obstacle feature dimension)
    ego_features_padded = jnp.pad(ego_features, (0, max(0, config.obstacle_node_features - len(ego_features))))[:config.obstacle_node_features]
    all_node_features = jnp.concatenate([
        ego_features_padded[None, :],  # (1, obstacle_node_features)
        obstacle_features  # (n_valid, obstacle_node_features)
    ], axis=0)
    
    # 5. Build edges with fixed-size arrays
    senders, receivers, edge_features = find_knn_edges(
        drone_state.position, valid_points, config.k_neighbors
    )
    
    # Filter out invalid edges (marked with -1) without dynamic slicing
    valid_edge_mask = (senders >= 0) & (receivers >= 0)
    
    # Count valid edges
    n_valid_edges = jnp.sum(valid_edge_mask.astype(jnp.int32))
    
    # For GraphsTuple, we'll use the full arrays but jraph can handle -1 indices
    # The GNN will ignore edges with negative indices
    
    # 6. Node type classification
    # 0 = ego drone, 1 = obstacle
    node_types = jnp.concatenate([
        jnp.array([0]),  # ego node
        jnp.ones(n_valid)  # obstacle nodes
    ]).astype(jnp.int32)
    
    # 7. Create GraphsTuple - use full arrays, jraph handles invalid edges gracefully
    n_nodes = n_valid + 1
    n_total_edges = len(senders)  # Include both valid and invalid edges
    
    graph = jraph.GraphsTuple(
        n_node=jnp.array([n_nodes]),  # Single graph
        n_edge=jnp.array([n_total_edges]),  # Total edges (including invalid)
        nodes=all_node_features,  # (n_nodes, node_features)
        edges=edge_features,  # (n_total_edges, edge_features) 
        globals=None,
        senders=senders,  # (n_total_edges,) - includes -1 for invalid
        receivers=receivers,  # (n_total_edges,) - includes -1 for invalid
    )
    
    return graph, node_types

# =============================================================================  
# GNN ARCHITECTURE (Based on GCBF+)
# =============================================================================

def default_nn_init():
    """Default network initialization following GCBF+ convention"""
    return nn.initializers.xavier_uniform()

class MLP(nn.Module):
    """Multi-layer perceptron from GCBF+ codebase"""
    hid_sizes: Tuple[int, ...]
    act: Callable = nn.relu
    act_final: bool = False
    
    @nn.compact
    def __call__(self, x):
        for i, size in enumerate(self.hid_sizes):
            x = nn.Dense(size, kernel_init=default_nn_init())(x)
            if i < len(self.hid_sizes) - 1 or self.act_final:
                x = self.act(x)
        return x

class GNNUpdate(NamedTuple):
    """Graph update functions following GCBF+ exact implementation pattern"""
    message: Callable
    aggregate: Callable  
    update: Callable
    
    def __call__(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:
        """Apply GNN update step with safe indexing"""
        # Allow both single graph (scalar n_node) and batched single graph ((1,) n_node)
        assert graph.n_node.shape == tuple() or graph.n_node.shape == (1,), f"Expected single graph, got shape {graph.n_node.shape}"
        
        # Safe indexing that handles -1 indices (invalid edges)
        def safe_get(array, indices):
            valid_mask = indices >= 0
            safe_indices = jnp.where(valid_mask, indices, 0)  # Replace -1 with 0
            result = array[safe_indices]
            # Zero out invalid entries
            return jnp.where(valid_mask[:, None], result, 0.0)
        
        # Extract node features for senders and receivers
        node_feats_send = safe_get(graph.nodes, graph.senders)
        node_feats_recv = safe_get(graph.nodes, graph.receivers)
        
        # Message passing
        edges = self.message(graph.edges, node_feats_send, node_feats_recv)
        
        # Aggregate messages
        aggr_msg = self.aggregate(edges, graph.receivers, graph.nodes.shape[0])
        
        # Update nodes
        new_node_feats = self.update(graph.nodes, aggr_msg)
        
        return graph._replace(nodes=new_node_feats)

class GNNLayer(nn.Module):
    """
    Single GNN layer - EXACT replication of GCBF+ GNNLayer 
    From gcbfplus/nn/gnn.py with JAX-native implementation
    """
    msg_net_cls: Callable
    aggr_net_cls: Callable
    update_net_cls: Callable
    msg_dim: int
    out_dim: int
    
    @nn.compact
    def __call__(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:
        def message(edge_feats, sender_feats, receiver_feats):
            """Message function - exact GCBF+ implementation"""
            feats = jnp.concatenate([edge_feats, sender_feats, receiver_feats], axis=-1)
            feats = self.msg_net_cls()(feats)
            feats = nn.Dense(self.msg_dim, kernel_init=default_nn_init())(feats)
            return feats
        
        def update(node_feats, msgs):
            """Node update function - exact GCBF+ implementation"""
            feats = jnp.concatenate([node_feats, msgs], axis=-1)
            feats = self.update_net_cls()(feats)
            feats = nn.Dense(self.out_dim, kernel_init=default_nn_init())(feats)
            return feats
            
        def aggregate(msgs, recv_idx, num_segments):
            """Attention-based aggregation - exact GCBF+ implementation"""
            gate_feats = self.aggr_net_cls()(msgs)
            gate_feats = nn.Dense(1, kernel_init=default_nn_init())(gate_feats).squeeze(-1)
            
            # Segment softmax for attention weights
            attn = jraph.segment_softmax(gate_feats, segment_ids=recv_idx, num_segments=num_segments)
            assert attn.shape[0] == msgs.shape[0]
            
            # Weighted sum aggregation
            aggr_msg = jraph.segment_sum(
                attn[:, None] * msgs, segment_ids=recv_idx, num_segments=num_segments
            )
            return aggr_msg
        
        # Create update function and apply
        update_fn = GNNUpdate(message, aggregate, update)
        return update_fn(graph)

class GNN(nn.Module):
    """
    Multi-layer GNN - EXACT replication of GCBF+ GNN architecture
    From gcbfplus/nn/gnn.py with support for single-agent adaptation
    """
    msg_dim: int = 128
    hid_size_msg: Tuple[int, ...] = (64, 64)
    hid_size_aggr: Tuple[int, ...] = (64,)
    hid_size_update: Tuple[int, ...] = (64, 64)
    out_dim: int = 128  
    n_layers: int = 3
    
    @nn.compact  
    def __call__(self, graph: jraph.GraphsTuple, node_type: Optional[int] = None, n_type: Optional[int] = None) -> jnp.ndarray:
        """
        Forward pass following GCBF+ exact implementation
        
        Args:
            graph: Input graph structure
            node_type: Filter specific node type (0=ego, 1=obstacle)  
            n_type: Number of nodes of specified type
        """
        current_graph = graph
        
        # Multi-layer GNN processing
        for i in range(self.n_layers):
            out_dim = self.out_dim if i == self.n_layers - 1 else self.msg_dim
            
            # Create network classes using partial application (GCBF+ pattern)
            msg_net = ft.partial(MLP, hid_sizes=self.hid_size_msg, act=nn.relu, act_final=False)
            attn_net = ft.partial(MLP, hid_sizes=self.hid_size_aggr, act=nn.relu, act_final=False)  
            update_net = ft.partial(MLP, hid_sizes=self.hid_size_update, act=nn.relu, act_final=False)
            
            # Create and apply GNN layer
            gnn_layer = GNNLayer(
                msg_net_cls=msg_net,
                aggr_net_cls=attn_net,
                update_net_cls=update_net,
                msg_dim=self.msg_dim,
                out_dim=out_dim
            )
            current_graph = gnn_layer(current_graph)
            
        # Return results based on node type filtering (GCBF+ compatibility)
        if node_type is None:
            return current_graph.nodes
        else:
            # For ego drone (node_type=0), return first node
            if node_type == 0 and n_type is not None:
                return current_graph.nodes[:n_type]
            # For obstacles (node_type=1), return remaining nodes  
            elif node_type == 1 and n_type is not None:
                return current_graph.nodes[1:1+n_type] if current_graph.nodes.shape[0] > 1 else jnp.zeros((n_type, current_graph.nodes.shape[1]))
            else:
                return current_graph.nodes

class CBFNet(nn.Module):
    """
    CBF network - EXACT replication of GCBF+ CBF module
    From gcbfplus/algo/module/cbf.py adapted for single-agent scenarios
    """
    # Graph neural network parameters (matching GCBF+ CBF defaults)
    node_dim: int = 3  # Node feature dimension
    edge_dim: int = 4  # Edge feature dimension  
    n_agents: int = 1  # Single agent (ego drone)
    gnn_layers: int = 3
    
    # GNN architecture parameters (GCBF+ defaults)
    msg_dim: int = 64
    hid_size_msg: Tuple[int, ...] = (64, 64)
    hid_size_aggr: Tuple[int, ...] = (64,)
    hid_size_update: Tuple[int, ...] = (64, 64)
    
    @nn.compact
    def __call__(self, graph: jraph.GraphsTuple, n_type: Optional[int] = None) -> jnp.ndarray:
        """
        CBF computation following GCBF+ exact implementation
        
        Args:
            graph: Input graph with drone and obstacle nodes
            n_type: Number of agent nodes (should be 1 for single drone)
        
        Returns:
            CBF value(s) for agent node(s)
        """
        # Create GNN following GCBF+ CBF parameters
        gnn = GNN(
            msg_dim=self.msg_dim,
            hid_size_msg=self.hid_size_msg,
            hid_size_aggr=self.hid_size_aggr, 
            hid_size_update=self.hid_size_update,
            out_dim=self.msg_dim,  # Output same as message dimension
            n_layers=self.gnn_layers
        )
        
        # Process graph and extract agent node features  
        # node_type=0 means agent nodes (ego drone)
        n_agents_actual = n_type if n_type is not None else self.n_agents
        agent_embeddings = gnn(graph, node_type=0, n_type=n_agents_actual)
        
        # Final CBF head - single Dense layer (GCBF+ pattern)
        cbf_values = nn.Dense(1, kernel_init=default_nn_init())(agent_embeddings)
        
        # For single agent, return scalar; for multi-agent, return vector
        if n_agents_actual == 1:
            return cbf_values.squeeze()  # Scalar output
        else:
            return cbf_values.squeeze(-1)  # Remove last dimension but keep batch

# =============================================================================
# PERCEPTION MODULE
# =============================================================================

class PerceptionModule:
    """
    Complete perception pipeline for CBF computation
    """
    
    def __init__(self, config: GraphConfig):
        self.config = config
        self.cbf_net = CBFNet()
        
    def get_cbf_and_gradients(self, params, drone_state: DroneState, point_cloud: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Compute CBF value and gradients from point cloud input
        
        Args:
            params: Network parameters
            drone_state: Current drone state
            point_cloud: LiDAR point cloud (N, 3)
            
        Returns:
            cbf_value: CBF value (scalar)
            cbf_gradients: CBF gradients w.r.t. drone position
        """
        def cbf_fn(state):
            graph, node_types = pointcloud_to_graph(state, point_cloud, self.config)
            return self.cbf_net.apply(params, graph, node_types)
        
        # Compute CBF value
        cbf_value = cbf_fn(drone_state)
        
        # Compute gradients w.r.t. drone position
        grad_fn = jax.grad(lambda state: cbf_fn(state).sum())  # sum() for scalar output
        cbf_gradients = grad_fn(drone_state)
        
        return cbf_value, cbf_gradients

# =============================================================================
# JIT-COMPATIBLE WRAPPER FUNCTIONS
# =============================================================================

@ft.partial(jit, static_argnums=(2,))  # config is static
def pointcloud_to_graph_jit(drone_state: DroneState, point_cloud: jnp.ndarray, config: GraphConfig) -> Tuple[jraph.GraphsTuple, jnp.ndarray]:
    """JIT-compatible version of pointcloud_to_graph with static config"""
    return pointcloud_to_graph(drone_state, point_cloud, config)

@ft.partial(jit, static_argnums=(2,))  # k is static  
def find_knn_edges_jit(drone_pos: jnp.ndarray, obstacle_points: jnp.ndarray, k: int) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:
    """JIT-compatible version of find_knn_edges with static k"""
    return find_knn_edges(drone_pos, obstacle_points, k)

# =============================================================================
# FACTORY FUNCTIONS AND UTILITIES
# =============================================================================

def create_default_perception_module():
    """Create perception module with default parameters"""
    config = GraphConfig()
    return PerceptionModule(config)

def init_cbf_network(rng_key, input_graph: jraph.GraphsTuple, node_types: jnp.ndarray):
    """Initialize CBF network parameters"""
    cbf_net = CBFNet()
    return cbf_net.init(rng_key, input_graph, node_types)

# Core function for integration with loop.py scan_function
@jit
def get_cbf_from_pointcloud(params, drone_state: DroneState, point_cloud: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """
    Optimized function for CBF computation in training loop
    Used by loop.py scan_function
    JIT-compiled for performance
    """
    config = GraphConfig()
    graph, node_types = pointcloud_to_graph(drone_state, point_cloud, config)
    
    cbf_net = CBFNet()
    cbf_value = cbf_net.apply(params, graph, node_types)
    
    # Compute gradients w.r.t. drone position (needed for QP constraints)
    def cbf_wrt_position(pos):
        # Add numerical stability checks
        pos_clipped = jnp.clip(pos, -10.0, 10.0)  # Prevent extreme positions
        
        modified_state = DroneState(
            position=pos_clipped,
            velocity=drone_state.velocity,
            orientation=drone_state.orientation,
            angular_velocity=drone_state.angular_velocity
        )
        graph_mod, node_types_mod = pointcloud_to_graph(modified_state, point_cloud, config)
        cbf_raw = cbf_net.apply(params, graph_mod, node_types_mod)
        
        # Apply numerical stability: prevent extreme CBF values
        cbf_stable = jnp.clip(cbf_raw, -5.0, 5.0)
        return cbf_stable
    
    # Compute gradients with clipping
    cbf_grad_raw = jax.grad(lambda pos: cbf_wrt_position(pos).sum())(drone_state.position)
    
    # Apply gradient clipping for numerical stability
    grad_norm = jnp.linalg.norm(cbf_grad_raw)
    max_grad_norm = 10.0  # Maximum allowed gradient norm
    
    cbf_grad = jnp.where(
        grad_norm > max_grad_norm,
        cbf_grad_raw * (max_grad_norm / (grad_norm + 1e-8)),
        cbf_grad_raw
    )
    
    return cbf_value, cbf_grad

# =============================================================================
# ADVANCED PERCEPTION MODULE WITH TEMPORAL CONSISTENCY
# =============================================================================

class AdvancedPerceptionModule(PerceptionModule):
    """Advanced perception module with temporal consistency and memory"""
    
    def __init__(self, config: GraphConfig, use_temporal_smoothing: bool = True):
        super().__init__(config)
        self.use_temporal_smoothing = use_temporal_smoothing
        self.cbf_history = []
        self.max_history = 5
        
        # Advanced CBF network with better architecture
        self.cbf_net = AdvancedCBFNet(
            gnn_msg_dim=256,  # Larger message dimension
            gnn_out_dim=256,
            gnn_layers=4,     # More layers for better representation
            head_sizes=(512, 256, 128),  # Deeper MLP head
            use_residual=True,
            use_attention=True
        )
    
    def get_cbf_and_gradients_with_history(
        self, 
        params, 
        drone_state: DroneState, 
        point_cloud: jnp.ndarray,
        temporal_weight: float = 0.1
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Compute CBF with temporal smoothing for stability
        
        Args:
            params: Network parameters
            drone_state: Current drone state
            point_cloud: LiDAR point cloud
            temporal_weight: Weight for temporal smoothing
            
        Returns:
            cbf_value: Temporally smoothed CBF value
            cbf_gradients: CBF gradients w.r.t. drone position
        """
        # Get current CBF value
        current_cbf, current_grad = self.get_cbf_and_gradients(
            params, drone_state, point_cloud
        )
        
        if not self.use_temporal_smoothing or len(self.cbf_history) == 0:
            # No temporal smoothing or first computation
            smoothed_cbf = current_cbf
            smoothed_grad = current_grad
        else:
            # Apply exponential moving average for temporal consistency
            prev_cbf = self.cbf_history[-1]['cbf_value']
            prev_grad = self.cbf_history[-1]['cbf_grad']
            
            smoothed_cbf = (1 - temporal_weight) * prev_cbf + temporal_weight * current_cbf
            smoothed_grad = (1 - temporal_weight) * prev_grad + temporal_weight * current_grad
        
        # Update history
        self.cbf_history.append({
            'cbf_value': current_cbf,
            'cbf_grad': current_grad
        })
        
        # Limit history size
        if len(self.cbf_history) > self.max_history:
            self.cbf_history.pop(0)
        
        return smoothed_cbf, smoothed_grad

class AdvancedCBFNet(nn.Module):
    """Advanced CBF network with enhanced architecture from GCBF+ analysis"""
    
    gnn_msg_dim: int = 256
    gnn_out_dim: int = 256
    gnn_layers: int = 4
    head_sizes: Tuple[int, ...] = (512, 256, 128)
    use_residual: bool = True
    use_attention: bool = True
    dropout_rate: float = 0.1
    
    @nn.compact
    def __call__(self, graph: jraph.GraphsTuple, node_types: jnp.ndarray, training: bool = False) -> jnp.ndarray:
        # Enhanced GNN with residual connections
        gnn = EnhancedGNN(
            msg_dim=self.gnn_msg_dim,
            out_dim=self.gnn_out_dim,
            n_layers=self.gnn_layers,
            use_residual=self.use_residual,
            use_attention=self.use_attention,
            dropout_rate=self.dropout_rate if training else 0.0
        )
        
        node_embeddings = gnn(graph, training=training)  # (n_nodes, gnn_out_dim)
        
        # Extract ego drone embedding (always at index 0)
        ego_embedding = node_embeddings[0:1, :]
        
        # Enhanced MLP head with residual connections and batch normalization
        x = ego_embedding
        
        for i, features in enumerate(self.head_sizes):
            residual = x
            
            # Dense layer
            x = nn.Dense(features, kernel_init=nn.initializers.xavier_uniform())(x)
            
            # Batch normalization for better training stability (only when training)
            if training:
                x = nn.BatchNorm(use_running_average=False)(x)
            
            # Activation
            x = nn.swish(x)  # Swish activation for better gradient flow
            
            # Dropout for regularization (only when training and rate > 0)
            if self.dropout_rate > 0 and training:
                x = nn.Dropout(self.dropout_rate, deterministic=not training)(x)
            
            # Residual connection if dimensions match
            if self.use_residual and residual.shape[-1] == features:
                x = x + residual * 0.5  # Scaled residual connection
        
        # Final CBF prediction layer
        cbf_raw = nn.Dense(1, kernel_init=nn.initializers.xavier_uniform())(x)
        
        # Use tanh activation to ensure bounded CBF values [-1, 1]
        cbf_value = nn.tanh(cbf_raw)
        
        return cbf_value.squeeze()  # Return scalar

class EnhancedGNN(nn.Module):
    """Enhanced GNN with advanced features from GCBF+ architecture"""
    
    msg_dim: int = 256
    out_dim: int = 256
    n_layers: int = 4
    use_residual: bool = True
    use_attention: bool = True
    dropout_rate: float = 0.1
    
    @nn.compact
    def __call__(self, graph: jraph.GraphsTuple, training: bool = False) -> jnp.ndarray:
        current_graph = graph
        
        for i in range(self.n_layers):
            # Progressive dimension scaling
            if i == self.n_layers - 1:
                layer_out_dim = self.out_dim
            else:
                layer_out_dim = self.msg_dim
            
            # Enhanced GNN layer with attention and residual connections
            layer = EnhancedGNNLayer(
                msg_dim=self.msg_dim,
                out_dim=layer_out_dim,
                use_attention=self.use_attention,
                dropout_rate=self.dropout_rate if training else 0.0
            )
            
            # Apply layer
            new_graph = layer(current_graph, training=training)
            
            # Residual connection for node features
            if (self.use_residual and 
                current_graph.nodes.shape[-1] == new_graph.nodes.shape[-1]):
                new_graph = new_graph._replace(
                    nodes=new_graph.nodes + current_graph.nodes * 0.5
                )
            
            current_graph = new_graph
        
        return current_graph.nodes

class EnhancedGNNLayer(nn.Module):
    """Enhanced GNN layer with attention mechanism from GCBF+ analysis"""
    
    msg_dim: int
    out_dim: int
    use_attention: bool = True
    dropout_rate: float = 0.1
    hid_size_msg: Tuple[int, ...] = (512, 256)
    hid_size_aggr: Tuple[int, ...] = (256, 128)
    hid_size_update: Tuple[int, ...] = (512, 256)
    
    @nn.compact
    def __call__(self, graph: jraph.GraphsTuple, training: bool = False) -> jraph.GraphsTuple:
        def enhanced_message_fn(edge_feats, sender_feats, receiver_feats):
            """Enhanced message function with better feature processing"""
            # Concatenate all features
            feats = jnp.concatenate([edge_feats, sender_feats, receiver_feats], axis=-1)
            
            # Multi-layer message processing
            x = feats
            for i, size in enumerate(self.hid_size_msg):
                x = nn.Dense(size, kernel_init=nn.initializers.xavier_uniform())(x)
                x = nn.swish(x)  # Better activation function
                
                if self.dropout_rate > 0 and training:
                    x = nn.Dropout(self.dropout_rate, deterministic=not training)(x)
            
            # Final message dimension
            messages = nn.Dense(self.msg_dim, kernel_init=nn.initializers.xavier_uniform())(x)
            return messages
        
        def enhanced_aggregate_fn(msgs, segment_ids, num_segments):
            """Enhanced aggregation with learned attention weights"""
            if self.use_attention:
                # Attention-based aggregation (from GCBF+ implementation)
                gate_feats = MLP(self.hid_size_aggr, act=nn.swish)(msgs)
                attention_scores = nn.Dense(1, kernel_init=nn.initializers.xavier_uniform())(gate_feats).squeeze(-1)
                
                # Apply softmax attention
                attn_weights = jraph.segment_softmax(
                    attention_scores, segment_ids=segment_ids, num_segments=num_segments
                )
                
                # Weighted aggregation
                aggr_msg = jraph.segment_sum(
                    attn_weights[:, None] * msgs, 
                    segment_ids=segment_ids, 
                    num_segments=num_segments
                )
            else:
                # Simple mean aggregation
                aggr_msg = jraph.segment_mean(msgs, segment_ids, num_segments)
            
            return aggr_msg
        
        def enhanced_update_fn(node_feats, aggr_msgs):
            """Enhanced node update function"""
            # Combine node features with aggregated messages
            feats = jnp.concatenate([node_feats, aggr_msgs], axis=-1)
            
            # Multi-layer update processing
            x = feats
            for i, size in enumerate(self.hid_size_update):
                x = nn.Dense(size, kernel_init=nn.initializers.xavier_uniform())(x)
                x = nn.swish(x)
                
                if self.dropout_rate > 0 and training:
                    x = nn.Dropout(self.dropout_rate, deterministic=not training)(x)
            
            # Final output dimension
            updated_features = nn.Dense(self.out_dim, kernel_init=nn.initializers.xavier_uniform())(x)
            return updated_features
        
        # Apply enhanced message passing
        updated_nodes = jraph.GraphNetwork(
            update_node_fn=lambda nodes, sent_msgs, received_msgs, globals_: enhanced_update_fn(nodes, received_msgs),
            update_edge_fn=None,  # Keep edges unchanged
            update_global_fn=None,  # No global features
            aggregate_edges_for_nodes_fn=enhanced_aggregate_fn,
            aggregate_nodes_for_globals_fn=None,
            aggregate_edges_for_globals_fn=None,
        )(
            graph._replace(
                edges=enhanced_message_fn(graph.edges, 
                                         graph.nodes[graph.senders], 
                                         graph.nodes[graph.receivers])
            )
        ).nodes
        
        return graph._replace(nodes=updated_nodes)

# =============================================================================
# TESTING AND VALIDATION
# =============================================================================

def test_pointcloud_to_graph():
    """Test point cloud to graph conversion"""
    # Create test data
    drone_state = DroneState(
        position=jnp.array([0.0, 0.0, 1.0]),
        velocity=jnp.array([1.0, 0.0, 0.0]),
        orientation=jnp.eye(3),
        angular_velocity=jnp.zeros(3)
    )
    
    # Simple obstacle point cloud
    point_cloud = jnp.array([
        [1.0, 0.0, 0.0],  # Front obstacle
        [0.0, 1.0, 0.0],  # Right obstacle  
        [-1.0, 0.0, 0.0], # Rear obstacle
        [0.0, -1.0, 0.0], # Left obstacle
        [0.0, 0.0, 1.0],  # Top obstacle
    ])
    
    config = GraphConfig()
    graph, node_types = pointcloud_to_graph(drone_state, point_cloud, config)
    
    print(f"Number of nodes: {graph.n_node}")
    print(f"Number of edges: {graph.n_edge}")
    print(f"Node features shape: {graph.nodes.shape}")
    print(f"Edge features shape: {graph.edges.shape}")
    print(f"Node types: {node_types}")
    
    return graph, node_types

def test_advanced_perception_module():
    """Test advanced perception module with temporal consistency"""
    print("Testing Advanced Perception Module...")
    
    # Create advanced perception module
    config = GraphConfig(k_neighbors=10, max_range=8.0)
    perception = AdvancedPerceptionModule(config, use_temporal_smoothing=True)
    
    # Initialize network
    key = jax.random.PRNGKey(42)
    init_key, dropout_key, test_key = jax.random.split(key, 3)
    
    drone_state = DroneState(
        position=jnp.array([0.0, 0.0, 1.0]),
        velocity=jnp.array([1.0, 0.5, 0.0]),
        orientation=jnp.eye(3),
        angular_velocity=jnp.zeros(3)
    )
    
    # Generate test point cloud
    point_cloud = jax.random.normal(test_key, (25, 3)) * 3.0
    graph, node_types = pointcloud_to_graph(drone_state, point_cloud, config)
    
    # Initialize network parameters with dropout support
    params = perception.cbf_net.init(
        {'params': init_key, 'dropout': dropout_key}, 
        graph, node_types, training=True
    )
    
    # Test CBF computation with temporal consistency
    for i in range(3):
        # Create a test CBF function that doesn't use dropout for inference
        def test_cbf_fn(state):
            test_graph, test_node_types = pointcloud_to_graph(state, point_cloud, config)
            return perception.cbf_net.apply(
                params, test_graph, test_node_types, 
                training=False, rngs={'dropout': dropout_key}
            )
        
        # Compute CBF and gradients
        cbf_value = test_cbf_fn(drone_state)
        cbf_grad = jax.grad(lambda state: test_cbf_fn(state).sum())(drone_state)
        
        # Update temporal history manually for testing
        perception.cbf_history.append({
            'cbf_value': cbf_value,
            'cbf_grad': cbf_grad
        })
        
        print(f"  Step {i+1}: CBF={cbf_value:.4f}, Grad_norm={jnp.linalg.norm(cbf_grad.position):.4f}")
        
        # Slightly modify drone state for next iteration
        drone_state = drone_state._replace(
            position=drone_state.position + jnp.array([0.1, 0.0, 0.0])
        )
    
    print("âœ… Advanced Perception Module Test: PASSED")
    return True

if __name__ == "__main__":
    # Run basic test
    print("Testing point cloud to graph conversion...")
    test_graph, test_node_types = test_pointcloud_to_graph()
    print("Test completed!")