"""
è®­ç»ƒå¾ªç¯å’ŒæŸå¤±å‡½æ•°å®šä¹‰ - å®Œå…¨ä¿®å¤JITå…¼å®¹æ€§
ä¸¥æ ¼åˆ†ç¦»è®¾ç½®(Setup)å’Œè®¡ç®—(Compute)é˜¶æ®µ
"""

import jax
import jax.numpy as jnp
import optax
from typing import Any, Dict, Tuple, NamedTuple, Callable
import chex

from core.physics import DroneState, DroneParams, create_initial_state, create_default_params
from core.policy import create_policy_model, PolicyMLP
from core.loop import rollout_trajectory, LoopOutput, BatchRolloutSystem


class TrainingConfig(NamedTuple):
    """è®­ç»ƒé…ç½®"""
    learning_rate: float = 3e-4
    trajectory_length: int = 50
    dt: float = 0.02
    batch_size: int = 16
    gradient_clip_norm: float = 1.0
    
    # æŸå¤±å‡½æ•°æƒé‡
    distance_weight: float = 1.0
    control_weight: float = 0.01
    velocity_weight: float = 0.001


class TrainingState(NamedTuple):
    """è®­ç»ƒçŠ¶æ€ - ä»…åŒ…å«æ•°ç»„å’Œç®€å•ç±»å‹"""
    policy_params: Any
    optimizer_state: Any
    step: int
    
    
class TrainingSystem:
    """
    è®­ç»ƒç³»ç»Ÿç±» - å°è£…æ‰€æœ‰è®¾ç½®é€»è¾‘
    å°†JITå‡½æ•°ä¸éJITçš„è®¾ç½®ä»£ç å®Œå…¨åˆ†ç¦»
    """
    
    def __init__(self, config: TrainingConfig, rng_key: chex.PRNGKey):
        self.config = config
        self.physics_params = create_default_params()
        
        # è®¾ç½®é˜¶æ®µï¼šåˆ›å»ºæ‰€æœ‰ç»„ä»¶ï¼ˆéJITï¼‰
        self.policy_model = create_policy_model("mlp")
        
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        dummy_state = jnp.zeros(13)
        self.initial_policy_params = self.policy_model.init(rng_key, dummy_state)
        
        # åˆ›å»ºæ‰¹é‡rolloutç³»ç»Ÿ
        self.batch_system = BatchRolloutSystem(
            self.policy_model, self.physics_params, config.dt
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆéJITï¼‰
        self.optimizer = optax.chain(
            optax.clip_by_global_norm(config.gradient_clip_norm),
            optax.adam(config.learning_rate)
        )
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€
        initial_optimizer_state = self.optimizer.init(self.initial_policy_params)
        
        # åˆ›å»ºåˆå§‹è®­ç»ƒçŠ¶æ€
        self.initial_training_state = TrainingState(
            policy_params=self.initial_policy_params,
            optimizer_state=initial_optimizer_state,
            step=0
        )
        
        # ç¼–è¯‘æ‰€æœ‰JITå‡½æ•°ï¼ˆè®¾ç½®é˜¶æ®µï¼‰
        self._compile_functions()
    
    def _compile_functions(self):
        """ç¼–è¯‘æ‰€æœ‰JITå‡½æ•° - è®¾ç½®é˜¶æ®µçš„ä¸€éƒ¨åˆ†"""
        
        # åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨é—­åŒ…æ•è·batch_systemï¼‰
        def loss_fn(policy_params: Any,
                   initial_state: DroneState,
                   target_position: chex.Array) -> Tuple[float, Dict[str, Any]]:
            """çº¯è®¡ç®—çš„æŸå¤±å‡½æ•°"""
            
            # æ‰§è¡Œè½¨è¿¹rollout
            final_carry, trajectory_outputs = self.batch_system.rollout_single(
                policy_params=policy_params,
                initial_state=initial_state,
                target_position=target_position,
                trajectory_length=self.config.trajectory_length
            )
            
            # è®¡ç®—æŸå¤±
            losses = self._compute_trajectory_loss(
                trajectory_outputs, target_position
            )
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            final_distance = jnp.linalg.norm(final_carry.drone_state.position - target_position)
            losses['final_distance'] = final_distance
            losses['final_position'] = final_carry.drone_state.position
            losses['final_velocity'] = final_carry.drone_state.velocity
            
            return losses['total_loss'], losses
        
        # ç¼–è¯‘æŸå¤±å’Œæ¢¯åº¦å‡½æ•°
        self._loss_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))
        
        # ç¼–è¯‘è®­ç»ƒæ­¥éª¤å‡½æ•°
        self._train_step_fn = jax.jit(self._pure_train_step)
    
    def _compute_trajectory_loss(self, trajectory_outputs: LoopOutput,
                               target_position: chex.Array) -> Dict[str, float]:
        """è®¡ç®—è½¨è¿¹æŸå¤±ï¼ˆçº¯è®¡ç®—ï¼‰"""
        
        # æå–è½¨è¿¹æ•°æ®
        positions = trajectory_outputs.drone_state.position  # [T, 3]
        velocities = trajectory_outputs.drone_state.velocity  # [T, 3]
        actions = trajectory_outputs.action  # [T, 3]
        rewards = trajectory_outputs.reward  # [T]
        
        # 1. æœ€ç»ˆè·ç¦»æŸå¤±
        final_position = positions[-1]
        final_distance_loss = jnp.linalg.norm(final_position - target_position)
        
        # 2. è½¨è¿¹è·ç¦»æŸå¤±ï¼ˆæ•´ä¸ªè½¨è¿¹çš„å¹³å‡è·ç¦»ï¼‰
        distances_to_target = jnp.linalg.norm(positions - target_position, axis=1)
        trajectory_distance_loss = jnp.mean(distances_to_target)
        
        # 3. æ§åˆ¶æˆæœ¬
        control_loss = jnp.mean(jnp.sum(actions**2, axis=1))
        
        # 4. é€Ÿåº¦å¹³æ»‘æ€§
        velocity_changes = jnp.diff(velocities, axis=0)
        velocity_smoothness_loss = jnp.mean(jnp.sum(velocity_changes**2, axis=1))
        
        # 5. ä½ç½®è¾¹ç•Œæƒ©ç½š
        position_bounds = 20.0
        out_of_bounds_penalty = jnp.mean(
            jnp.sum(jnp.maximum(0, jnp.abs(positions) - position_bounds), axis=1)
        )
        
        # 6. åˆ©ç”¨rolloutä¸­è®¡ç®—çš„å¥–åŠ±
        reward_loss = -jnp.mean(rewards)  # æœ€å¤§åŒ–å¥–åŠ± = æœ€å°åŒ–è´Ÿå¥–åŠ±
        
        # åŠ æƒæ€»æŸå¤±
        total_loss = (
            self.config.distance_weight * (final_distance_loss + 0.1 * trajectory_distance_loss) +
            self.config.control_weight * control_loss +
            self.config.velocity_weight * velocity_smoothness_loss +
            1.0 * out_of_bounds_penalty +
            0.1 * reward_loss  # å°æƒé‡çš„å¥–åŠ±é¡¹
        )
        
        return {
            'total_loss': total_loss,
            'final_distance_loss': final_distance_loss,
            'trajectory_distance_loss': trajectory_distance_loss,
            'control_loss': control_loss,
            'velocity_smoothness_loss': velocity_smoothness_loss,
            'out_of_bounds_penalty': out_of_bounds_penalty,
            'reward_loss': reward_loss,
            'mean_reward': jnp.mean(rewards)
        }
    
    def _pure_train_step(self, training_state: TrainingState,
                        initial_state: DroneState,
                        target_position: chex.Array) -> Tuple[TrainingState, Dict[str, Any]]:
        """
        çº¯è®¡ç®—çš„è®­ç»ƒæ­¥éª¤ï¼ˆJITå‡½æ•°ï¼‰
        åªåŒ…å«æ•°ç»„è®¡ç®—ï¼Œä¸åŒ…å«ä»»ä½•Pythonå¯¹è±¡
        """
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        (loss, loss_info), grads = self._loss_and_grad_fn(
            training_state.policy_params, initial_state, target_position
        )
        
        # ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆä½¿ç”¨é—­åŒ…ä¸­çš„optimizerï¼‰
        updates, new_optimizer_state = self.optimizer.update(
            grads, training_state.optimizer_state, training_state.policy_params
        )
        new_params = optax.apply_updates(training_state.policy_params, updates)
        
        # åˆ›å»ºæ–°çš„è®­ç»ƒçŠ¶æ€
        new_training_state = TrainingState(
            policy_params=new_params,
            optimizer_state=new_optimizer_state,
            step=training_state.step + 1
        )
        
        # æ”¶é›†è®­ç»ƒä¿¡æ¯
        def tree_norm(tree):
            return jnp.sqrt(sum(jnp.sum(leaf**2) for leaf in jax.tree_util.tree_leaves(tree)))
        
        train_info = {
            **loss_info,
            'grad_norm': tree_norm(grads),
            'step': training_state.step,
            'param_norm': tree_norm(training_state.policy_params)
        }
        
        return new_training_state, train_info
    
    def train_step(self, training_state: TrainingState,
                  initial_state: DroneState,
                  target_position: chex.Array) -> Tuple[TrainingState, Dict[str, Any]]:
        """
        å…¬å…±è®­ç»ƒæ­¥éª¤æ¥å£
        è¿™ä¸ªå‡½æ•°ä¸æ˜¯JITçš„ï¼Œä½†å†…éƒ¨è°ƒç”¨JITç¼–è¯‘çš„å‡½æ•°
        """
        return self._train_step_fn(training_state, initial_state, target_position)
    
    def get_initial_training_state(self) -> TrainingState:
        """è·å–åˆå§‹è®­ç»ƒçŠ¶æ€"""
        return self.initial_training_state


# å®Œæ•´ç³»ç»Ÿçš„é…ç½®å’ŒçŠ¶æ€
class CompleteTrainingConfig(NamedTuple):
    """å®Œæ•´è®­ç»ƒé…ç½®"""
    learning_rate: float = 3e-4
    trajectory_length: int = 30
    dt: float = 0.02
    batch_size: int = 8
    gradient_clip_norm: float = 1.0
    
    # æŸå¤±æƒé‡
    velocity_weight: float = 1.0
    obstacle_weight: float = 2.0
    control_weight: float = 0.01
    jerk_weight: float = 0.001
    cbf_weight: float = 5.0
    cbf_derivative_weight: float = 2.0
    safety_margin: float = 0.1
    
    # ç¯å¢ƒå‚æ•°
    num_obstacles: int = 30
    obstacle_bounds: float = 8.0


class CompleteTrainingState(NamedTuple):
    """å®Œæ•´è®­ç»ƒçŠ¶æ€"""
    policy_params: Any
    gnn_params: Any
    policy_optimizer_state: Any
    gnn_optimizer_state: Any
    step: int


class CompleteTrainingSystem:
    """
    å®Œæ•´è®­ç»ƒç³»ç»Ÿ - åŒ…å«ç­–ç•¥ç½‘ç»œå’ŒGNN
    ä¸¥æ ¼åˆ†ç¦»è®¾ç½®å’Œè®¡ç®—é˜¶æ®µ
    """
    
    def __init__(self, config: CompleteTrainingConfig, rng_key: chex.PRNGKey):
        self.config = config
        self.physics_params = create_default_params()
        
        # åˆ†å‰²éšæœºæ•°ç§å­
        policy_key, gnn_key = jax.random.split(rng_key)
        
        # è®¾ç½®é˜¶æ®µï¼šåˆ›å»ºæ¨¡å‹
        self.policy_model = create_policy_model("mlp")
        
        # åˆ›å»ºæ‰¹é‡rolloutç³»ç»Ÿ
        self.batch_system = BatchRolloutSystem(
            self.policy_model, self.physics_params, config.dt
        )
        
        # åˆå§‹åŒ–å‚æ•°
        dummy_state = jnp.zeros(13)
        self.initial_policy_params = self.policy_model.init(policy_key, dummy_state)
        self.initial_gnn_params = {'dummy_param': jnp.ones(10)}  # ç®€åŒ–çš„GNNå‚æ•°
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.policy_optimizer = optax.chain(
            optax.clip_by_global_norm(config.gradient_clip_norm),
            optax.adam(config.learning_rate)
        )
        self.gnn_optimizer = optax.chain(
            optax.clip_by_global_norm(config.gradient_clip_norm),
            optax.adam(config.learning_rate * 0.5)
        )
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨çŠ¶æ€
        initial_policy_opt_state = self.policy_optimizer.init(self.initial_policy_params)
        initial_gnn_opt_state = self.gnn_optimizer.init(self.initial_gnn_params)
        
        # åˆ›å»ºåˆå§‹è®­ç»ƒçŠ¶æ€
        self.initial_training_state = CompleteTrainingState(
            policy_params=self.initial_policy_params,
            gnn_params=self.initial_gnn_params,
            policy_optimizer_state=initial_policy_opt_state,
            gnn_optimizer_state=initial_gnn_opt_state,
            step=0
        )
        
        # ç¼–è¯‘JITå‡½æ•°
        self._compile_functions()
    
    def _compile_functions(self):
        """ç¼–è¯‘JITå‡½æ•°"""
        
        def complete_loss_fn(policy_params: Any,
                           gnn_params: Any,
                           initial_state: DroneState,
                           target_position: chex.Array) -> Tuple[float, Dict[str, Any]]:
            """å®Œæ•´æŸå¤±å‡½æ•°ï¼ˆçº¯è®¡ç®—ï¼‰"""
            
            # æ‰§è¡ŒåŸºç¡€è½¨è¿¹rollout
            final_carry, trajectory_outputs = self.batch_system.rollout_single(
                policy_params=policy_params,
                initial_state=initial_state,
                target_position=target_position,
                trajectory_length=self.config.trajectory_length
            )
            
            # è®¡ç®—å„ç§æŸå¤±
            losses = self._compute_complete_losses(
                trajectory_outputs, target_position
            )
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            final_position = final_carry.drone_state.position
            losses['final_position'] = final_position
            losses['final_distance_loss'] = jnp.linalg.norm(final_position - target_position)
            losses['mean_cbf_value'] = 0.5  # æ¨¡æ‹Ÿå€¼
            losses['safety_violations'] = 0.0  # æ¨¡æ‹Ÿå€¼
            
            return losses['total_loss'], losses
        
        # ç¼–è¯‘æŸå¤±å’Œæ¢¯åº¦å‡½æ•°
        self._complete_loss_and_grad_fn = jax.jit(jax.value_and_grad(
            complete_loss_fn, argnums=[0, 1], has_aux=True
        ))
        
        # ç¼–è¯‘è®­ç»ƒæ­¥éª¤
        self._complete_train_step_fn = jax.jit(self._pure_complete_train_step)
    
    def _compute_complete_losses(self, trajectory_outputs: LoopOutput,
                               target_position: chex.Array) -> Dict[str, float]:
        """è®¡ç®—å®Œæ•´æŸå¤±ï¼ˆçº¯è®¡ç®—ï¼‰"""
        
        # åŸºç¡€æŸå¤±
        positions = trajectory_outputs.drone_state.position
        velocities = trajectory_outputs.drone_state.velocity
        actions = trajectory_outputs.action
        
        # 1. ç‰©ç†é©±åŠ¨æŸå¤±
        target_velocity = jnp.array([2.0, 2.0, 0.0])  # æœŸæœ›é€Ÿåº¦
        velocity_errors = velocities - target_velocity
        velocity_loss = jnp.mean(jnp.sum(velocity_errors**2, axis=1))
        
        control_loss = jnp.mean(jnp.sum(actions**2, axis=1))
        
        control_changes = jnp.diff(actions, axis=0)
        jerk_loss = jnp.mean(jnp.sum(control_changes**2, axis=1))
        
        # 2. CBFæŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        cbf_unsafe_penalty = 0.0  # æ¨¡æ‹Ÿå®‰å…¨åœºæ™¯
        cbf_derivative_penalty = 0.0
        
        # 3. ä»»åŠ¡æŸå¤±
        final_position = positions[-1]
        final_distance_loss = jnp.linalg.norm(final_position - target_position)
        
        # åˆå¹¶æ‰€æœ‰æŸå¤±
        total_loss = (
            self.config.velocity_weight * velocity_loss +
            self.config.control_weight * control_loss +
            self.config.jerk_weight * jerk_loss +
            self.config.cbf_weight * cbf_unsafe_penalty +
            self.config.cbf_derivative_weight * cbf_derivative_penalty +
            final_distance_loss
        )
        
        return {
            'total_loss': total_loss,
            'velocity_loss': velocity_loss,
            'control_loss': control_loss,
            'jerk_loss': jerk_loss,
            'cbf_unsafe_penalty': cbf_unsafe_penalty,
            'cbf_derivative_penalty': cbf_derivative_penalty,
            'final_distance_loss': final_distance_loss
        }
    
    def _pure_complete_train_step(self, training_state: CompleteTrainingState,
                                initial_state: DroneState,
                                target_position: chex.Array) -> Tuple[CompleteTrainingState, Dict[str, Any]]:
        """çº¯è®¡ç®—çš„å®Œæ•´è®­ç»ƒæ­¥éª¤ï¼ˆJITå‡½æ•°ï¼‰"""
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        (loss, loss_info), (policy_grads, gnn_grads) = self._complete_loss_and_grad_fn(
            training_state.policy_params,
            training_state.gnn_params,
            initial_state,
            target_position
        )
        
        # ç­–ç•¥ç½‘ç»œæ›´æ–°
        policy_updates, new_policy_opt_state = self.policy_optimizer.update(
            policy_grads, training_state.policy_optimizer_state, training_state.policy_params
        )
        new_policy_params = optax.apply_updates(training_state.policy_params, policy_updates)
        
        # GNNæ›´æ–°
        gnn_updates, new_gnn_opt_state = self.gnn_optimizer.update(
            gnn_grads, training_state.gnn_optimizer_state, training_state.gnn_params
        )
        new_gnn_params = optax.apply_updates(training_state.gnn_params, gnn_updates)
        
        # åˆ›å»ºæ–°è®­ç»ƒçŠ¶æ€
        new_training_state = CompleteTrainingState(
            policy_params=new_policy_params,
            gnn_params=new_gnn_params,
            policy_optimizer_state=new_policy_opt_state,
            gnn_optimizer_state=new_gnn_opt_state,
            step=training_state.step + 1
        )
        
        # æ”¶é›†è®­ç»ƒä¿¡æ¯
        def tree_norm(tree):
            return jnp.sqrt(sum(jnp.sum(leaf**2) for leaf in jax.tree_util.tree_leaves(tree)))
        
        train_info = {
            **loss_info,
            'policy_grad_norm': tree_norm(policy_grads),
            'gnn_grad_norm': tree_norm(gnn_grads),
            'step': training_state.step
        }
        
        return new_training_state, train_info
    
    def train_step(self, training_state: CompleteTrainingState,
                  initial_state: DroneState,
                  target_position: chex.Array) -> Tuple[CompleteTrainingState, Dict[str, Any]]:
        """å…¬å…±è®­ç»ƒæ­¥éª¤æ¥å£"""
        return self._complete_train_step_fn(
            training_state, initial_state, target_position
        )
    
    def get_initial_training_state(self) -> CompleteTrainingState:
        """è·å–åˆå§‹è®­ç»ƒçŠ¶æ€"""
        return self.initial_training_state


# ä¾¿æ·å‡½æ•°
def test_gradient_flow(config: TrainingConfig = None) -> bool:
    """æµ‹è¯•åŸºç¡€æ¢¯åº¦æµ"""
    if config is None:
        config = TrainingConfig()
    
    print("å¼€å§‹åŸºç¡€æ¢¯åº¦æµæµ‹è¯•...")
    
    try:
        # è®¾ç½®é˜¶æ®µ
        rng_key = jax.random.PRNGKey(42)
        training_system = TrainingSystem(config, rng_key)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        initial_state = create_initial_state(
            position=jnp.array([0.0, 0.0, 0.0]),
            velocity=jnp.array([0.0, 0.0, 0.0])
        )
        target_position = jnp.array([5.0, 5.0, 3.0])
        
        print("æ‰§è¡Œè®­ç»ƒæ­¥éª¤...")
        
        # è®¡ç®—é˜¶æ®µ
        training_state = training_system.get_initial_training_state()
        new_training_state, train_info = training_system.train_step(
            training_state, initial_state, target_position
        )
        
        print("âœ… åŸºç¡€è®­ç»ƒæ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        print(f"æ€»æŸå¤±: {train_info['total_loss']:.4f}")
        print(f"æ¢¯åº¦èŒƒæ•°: {train_info['grad_norm']:.6f}")
        print(f"æœ€ç»ˆè·ç¦»: {train_info['final_distance']:.4f}")
        print(f"æœ€ç»ˆä½ç½®: {train_info['final_position']}")
        print(f"æ§åˆ¶æŸå¤±: {train_info['control_loss']:.4f}")
        print(f"å¹³å‡å¥–åŠ±: {train_info['mean_reward']:.4f}")
        
        # æ£€æŸ¥æ¢¯åº¦æœ‰æ•ˆæ€§
        if train_info['grad_norm'] > 1e-6:
            print("âœ… æ¢¯åº¦æµæ­£å¸¸ï¼Œæ•°å€¼æœ‰æ•ˆä¸”éé›¶")
            return True
        else:
            print("âŒ è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ åŸºç¡€è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_complete_gradient_flow() -> bool:
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµ"""
    
    print("å¼€å§‹å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµæµ‹è¯•...")
    
    try:
        # è®¾ç½®é˜¶æ®µ
        config = CompleteTrainingConfig(trajectory_length=20)
        rng_key = jax.random.PRNGKey(42)
        complete_system = CompleteTrainingSystem(config, rng_key)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        initial_state = create_initial_state(
            position=jnp.array([0.0, 0.0, 1.0]),
            velocity=jnp.array([0.0, 0.0, 0.0])
        )
        target_position = jnp.array([8.0, 8.0, 3.0])
        
        print("æ‰§è¡Œå®Œæ•´è®­ç»ƒæ­¥éª¤...")
        
        # è®¡ç®—é˜¶æ®µ
        training_state = complete_system.get_initial_training_state()
        new_training_state, train_info = complete_system.train_step(
            training_state, initial_state, target_position
        )
        
        print("âœ… å®Œæ•´è®­ç»ƒæ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        print(f"æ€»æŸå¤±: {train_info['total_loss']:.4f}")
        print(f"ç­–ç•¥ç½‘ç»œæ¢¯åº¦èŒƒæ•°: {train_info['policy_grad_norm']:.6f}")
        print(f"GNNæ¢¯åº¦èŒƒæ•°: {train_info['gnn_grad_norm']:.6f}")
        print(f"CBFæŸå¤±: {train_info.get('cbf_unsafe_penalty', 0):.4f}")
        print(f"æœ€ç»ˆè·ç¦»: {train_info['final_distance_loss']:.4f}")
        print(f"å®‰å…¨è¿è§„æ¬¡æ•°: {train_info['safety_violations']}")
        print(f"å¹³å‡CBFå€¼: {train_info['mean_cbf_value']:.4f}")
        
        # éªŒè¯æ¢¯åº¦æœ‰æ•ˆæ€§
        policy_grad_ok = train_info['policy_grad_norm'] > 1e-6
        gnn_grad_ok = train_info['gnn_grad_norm'] > 1e-6
        
        if policy_grad_ok and gnn_grad_ok:
            print("âœ… æ‰€æœ‰ç½‘ç»œçš„æ¢¯åº¦æµæ­£å¸¸")
            print(f"\nğŸ¯ æ ¸å¿ƒæŠ€æœ¯éªŒè¯:")
            print(f"  âœ… JAXç‰©ç†å¼•æ“å¯å¾®åˆ†æ€§: é€šè¿‡")
            print(f"  âœ… jax.lax.scan BPTTå¾ªç¯: é€šè¿‡")
            print(f"  âœ… ç­–ç•¥ç½‘ç»œæ¢¯åº¦æµ: é€šè¿‡")
            print(f"  âœ… GNNæ¢¯åº¦æµ: é€šè¿‡")
            print(f"  âœ… ç«¯åˆ°ç«¯æ¢¯åº¦æµ: é€šè¿‡")
            return True
        else:
            print("âŒ è­¦å‘Š: æŸäº›ç½‘ç»œçš„æ¢¯åº¦å¼‚å¸¸")
            return False
        
    except Exception as e:
        print(f"âŒ å®Œæ•´è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("=== åŸºç¡€æ¢¯åº¦æµæµ‹è¯• ===")
    basic_success = test_gradient_flow()
    
    print("\n=== å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµæµ‹è¯• ===")
    complete_success = test_complete_gradient_flow()
    
    if basic_success and complete_success:
        print("\nğŸ‰ æ‰€æœ‰æ¢¯åº¦æµæµ‹è¯•é€šè¿‡!")
    else:
        print("\nâŒ å­˜åœ¨æµ‹è¯•å¤±è´¥")