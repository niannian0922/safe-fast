"""
å®Œæ•´çš„è®­ç»ƒç³»ç»Ÿ
æ•´åˆæ‰€æœ‰æŸå¤±å‡½æ•°å’ŒMGDAä¼˜åŒ–
"""

import jax
import jax.numpy as jnp
import optax
from typing import Any, Dict, Tuple, NamedTuple
import chex

from core.physics import create_initial_state, create_default_params
from core.policy import create_policy_model
from core.perception import GCBFGraphNet, create_dummy_pointcloud
from core.safety import SafetyParams
from core.loop import complete_rollout_trajectory, CompleteLoopOutput


class CompleteTrainingConfig(NamedTuple):
    """å®Œæ•´è®­ç»ƒé…ç½®"""
    # åŸºç¡€å‚æ•°
    learning_rate: float = 3e-4
    trajectory_length: int = 50
    dt: float = 0.02
    batch_size: int = 16
    gradient_clip_norm: float = 1.0
    
    # æŸå¤±æƒé‡ - DiffPhysDroneé£æ ¼
    velocity_weight: float = 1.0
    obstacle_weight: float = 2.0
    control_weight: float = 0.01
    jerk_weight: float = 0.001
    
    # æŸå¤±æƒé‡ - GCBF+é£æ ¼  
    cbf_weight: float = 5.0
    cbf_derivative_weight: float = 2.0
    safety_margin: float = 0.1
    
    # ç¯å¢ƒå‚æ•°
    num_obstacles: int = 30
    obstacle_bounds: float = 8.0


def compute_physics_driven_losses(trajectory_outputs: CompleteLoopOutput,
                                target_velocity: chex.Array,
                                config: CompleteTrainingConfig) -> Dict[str, float]:
    """
    è®¡ç®—ç‰©ç†é©±åŠ¨çš„æŸå¤±ï¼ˆDiffPhysDroneé£æ ¼ï¼‰
    
    Args:
        trajectory_outputs: è½¨è¿¹è¾“å‡º
        target_velocity: ç›®æ ‡é€Ÿåº¦
        config: é…ç½®
        
    Returns:
        losses: æŸå¤±å­—å…¸
    """
    
    # æå–è½¨è¿¹æ•°æ®
    positions = trajectory_outputs.drone_state.position  # [T, 3]
    velocities = trajectory_outputs.drone_state.velocity  # [T, 3]
    u_safe = trajectory_outputs.u_safe  # [T, 3]
    
    # 1. é€Ÿåº¦è·Ÿè¸ªæŸå¤±
    velocity_errors = velocities - target_velocity
    velocity_loss = jnp.mean(jnp.sum(velocity_errors**2, axis=1))
    
    # 2. éšœç¢ç‰©é¿ç¢°æŸå¤±ï¼ˆåŸºäºè·ç¦»ï¼‰
    obstacle_penalty = 0.0
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”åŸºäºCBFå€¼
    
    # 3. æ§åˆ¶å¹³æ»‘æ€§
    control_smoothness = jnp.mean(jnp.sum(u_safe**2, axis=1))
    
    # 4. æ§åˆ¶å˜åŒ–ç‡ï¼ˆjerkï¼‰
    control_changes = jnp.diff(u_safe, axis=0)
    jerk_loss = jnp.mean(jnp.sum(control_changes**2, axis=1))
    
    losses = {
        'velocity_loss': config.velocity_weight * velocity_loss,
        'obstacle_loss': config.obstacle_weight * obstacle_penalty,
        'control_loss': config.control_weight * control_smoothness,
        'jerk_loss': config.jerk_weight * jerk_loss,
    }
    
    return losses


def compute_cbf_losses(trajectory_outputs: CompleteLoopOutput,
                      config: CompleteTrainingConfig) -> Dict[str, float]:
    """
    è®¡ç®—CBFæŸå¤±ï¼ˆGCBF+é£æ ¼ï¼‰
    
    Args:
        trajectory_outputs: è½¨è¿¹è¾“å‡º
        config: é…ç½®
        
    Returns:
        losses: CBFç›¸å…³æŸå¤±
    """
    
    # æå–CBFç›¸å…³æ•°æ®
    h_values = trajectory_outputs.h  # [T]
    grad_h_values = trajectory_outputs.grad_h  # [T, 3]
    velocities = trajectory_outputs.drone_state.velocity  # [T, 3]
    
    # 1. CBFå€¼æŸå¤±
    # æƒ©ç½šè´Ÿçš„CBFå€¼ï¼ˆä¸å®‰å…¨åŒºåŸŸï¼‰
    unsafe_penalty = jnp.mean(jnp.maximum(0.0, -h_values + config.safety_margin)**2)
    
    # 2. CBFå¯¼æ•°æ¡ä»¶æŸå¤±
    # h_dot + Î± * h >= 0
    h_dot = jnp.sum(grad_h_values * velocities, axis=1)  # [T]
    alpha = 1.0
    cbf_condition = h_dot + alpha * h_values
    derivative_penalty = jnp.mean(jnp.maximum(0.0, -cbf_condition)**2)
    
    # 3. å®‰å…¨åŒºåŸŸå†…çš„CBFæ­£å€¼å¥–åŠ±
    safe_reward = jnp.mean(jnp.maximum(0.0, h_values))
    
    losses = {
        'cbf_unsafe_penalty': config.cbf_weight * unsafe_penalty,
        'cbf_derivative_penalty': config.cbf_derivative_weight * derivative_penalty,
        'cbf_safe_reward': -0.1 * safe_reward,  # è´Ÿå·è¡¨ç¤ºå¥–åŠ±
    }
    
    return losses


def compute_complete_trajectory_loss(trajectory_outputs: CompleteLoopOutput,
                                   target_position: chex.Array,
                                   target_velocity: chex.Array,
                                   config: CompleteTrainingConfig) -> Dict[str, float]:
    """
    è®¡ç®—å®Œæ•´çš„è½¨è¿¹æŸå¤±ï¼Œç»“åˆä¸¤ç§æ–¹æ³•è®º
    
    Returns:
        all_losses: åŒ…å«æ‰€æœ‰æŸå¤±é¡¹çš„å­—å…¸
    """
    
    # 1. ç‰©ç†é©±åŠ¨æŸå¤±ï¼ˆDiffPhysDroneï¼‰
    physics_losses = compute_physics_driven_losses(trajectory_outputs, target_velocity, config)
    
    # 2. CBFå®‰å…¨æŸå¤±ï¼ˆGCBF+ï¼‰
    cbf_losses = compute_cbf_losses(trajectory_outputs, config)
    
    # 3. ä»»åŠ¡ç‰¹å®šæŸå¤±
    final_position = trajectory_outputs.drone_state.position[-1]
    final_distance_loss = jnp.linalg.norm(final_position - target_position)
    
    # åˆå¹¶æ‰€æœ‰æŸå¤±
    all_losses = {
        **physics_losses,
        **cbf_losses,
        'final_distance_loss': final_distance_loss,
    }
    
    # è®¡ç®—æ€»æŸå¤±
    total_loss = sum(all_losses.values())
    all_losses['total_loss'] = total_loss
    
    return all_losses


def create_complete_loss_function(config: CompleteTrainingConfig,
                                physics_params,
                                safety_params):
    """åˆ›å»ºå®Œæ•´çš„æŸå¤±å‡½æ•°"""
    
    def loss_fn(policy_params, policy_model,
                gnn_params, gnn_model,
                initial_state,
                point_cloud_sequence,
                target_position,
                target_velocity,
                rng_key) -> Tuple[float, Dict[str, Any]]:
        """
        å®Œæ•´æŸå¤±å‡½æ•°
        
        Args:
            policy_params: ç­–ç•¥ç½‘ç»œå‚æ•°
            policy_model: ç­–ç•¥ç½‘ç»œæ¨¡å‹
            gnn_params: GNNå‚æ•°
            gnn_model: GNNæ¨¡å‹
            initial_state: åˆå§‹çŠ¶æ€
            point_cloud_sequence: ç‚¹äº‘åºåˆ— [T, N, 3]
            target_position: ç›®æ ‡ä½ç½®
            target_velocity: ç›®æ ‡é€Ÿåº¦
            rng_key: éšæœºç§å­
            
        Returns:
            (loss, info): æŸå¤±å€¼å’Œè¯¦ç»†ä¿¡æ¯
        """
        
        # æ‰§è¡Œå®Œæ•´è½¨è¿¹å±•å¼€
        final_carry, trajectory_outputs = complete_rollout_trajectory(
            initial_state=initial_state,
            point_cloud_sequence=point_cloud_sequence,
            policy_params=policy_params,
            policy_model=policy_model,
            gnn_params=gnn_params,
            gnn_model=gnn_model,
            physics_params=physics_params,
            safety_params=safety_params,
            trajectory_length=config.trajectory_length,
            dt=config.dt,
            use_rnn=False
        )
        
        # è®¡ç®—æŸå¤±
        losses = compute_complete_trajectory_loss(
            trajectory_outputs, target_position, target_velocity, config
        )
        
        # æ·»åŠ æœ€ç»ˆçŠ¶æ€ä¿¡æ¯
        info = {
            **losses,
            'final_position': final_carry.drone_state.position,
            'final_cbf_value': trajectory_outputs.h[-1],
            'mean_cbf_value': jnp.mean(trajectory_outputs.h),
            'safety_violations': jnp.sum(trajectory_outputs.h < 0),
        }
        
        return losses['total_loss'], info
    
    return loss_fn


def create_complete_training_step(config: CompleteTrainingConfig,
                                physics_params,
                                safety_params):
    """åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ­¥éª¤å‡½æ•°"""
    
    loss_fn = create_complete_loss_function(config, physics_params, safety_params)
    
    @jax.jit
    def train_step(policy_params, policy_model,
                  gnn_params, gnn_model,
                  policy_optimizer_state, gnn_optimizer_state,
                  policy_optimizer, gnn_optimizer,
                  initial_state, point_cloud_sequence,
                  target_position, target_velocity,
                  rng_key) -> Tuple[Any, Any, Any, Any, Dict[str, float]]:
        """
        å®Œæ•´çš„è®­ç»ƒæ­¥éª¤ï¼ŒåŒæ—¶æ›´æ–°ç­–ç•¥ç½‘ç»œå’ŒGNN
        
        Returns:
            (new_policy_params, new_gnn_params, new_policy_opt_state, new_gnn_opt_state, info)
        """
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦ï¼ˆé’ˆå¯¹ä¸¤ä¸ªç½‘ç»œï¼‰
        def combined_loss_fn(params_tuple):
            p_params, g_params = params_tuple
            return loss_fn(p_params, policy_model, g_params, gnn_model,
                          initial_state, point_cloud_sequence,
                          target_position, target_velocity, rng_key)
        
        (loss, info), grads_tuple = jax.value_and_grad(
            combined_loss_fn, has_aux=True
        )((policy_params, gnn_params))
        
        policy_grads, gnn_grads = grads_tuple
        
        # æ¢¯åº¦è£å‰ª
        if config.gradient_clip_norm > 0:
            policy_grads = optax.clip_by_global_norm(config.gradient_clip_norm)(policy_grads)
            gnn_grads = optax.clip_by_global_norm(config.gradient_clip_norm)(gnn_grads)
        
        # ä¼˜åŒ–å™¨æ›´æ–°
        policy_updates, new_policy_opt_state = policy_optimizer.update(
            policy_grads, policy_optimizer_state, policy_params
        )
        new_policy_params = optax.apply_updates(policy_params, policy_updates)
        
        gnn_updates, new_gnn_opt_state = gnn_optimizer.update(
            gnn_grads, gnn_optimizer_state, gnn_params
        )
        new_gnn_params = optax.apply_updates(gnn_params, gnn_updates)
        
        # æ·»åŠ æ¢¯åº¦ä¿¡æ¯
        info = {
            **info,
            'policy_grad_norm': optax.global_norm(policy_grads),
            'gnn_grad_norm': optax.global_norm(gnn_grads),
        }
        
        return (new_policy_params, new_gnn_params, 
                new_policy_opt_state, new_gnn_opt_state, info)
    
    return train_step


def initialize_complete_training(config: CompleteTrainingConfig,
                                rng_key: chex.PRNGKey):
    """
    åˆå§‹åŒ–å®Œæ•´çš„è®­ç»ƒç³»ç»Ÿ
    
    Returns:
        (models, params, optimizers, optimizer_states)
    """
    
    # åˆ†å‰²éšæœºæ•°ç§å­
    policy_key, gnn_key = jax.random.split(rng_key)
    
    # åˆ›å»ºæ¨¡å‹
    policy_model = create_policy_model("mlp")
    gnn_model = GCBFGraphNet(
        hidden_dim=128,
        num_message_passing_steps=3
    )
    
    # åˆå§‹åŒ–å‚æ•°
    dummy_state = jnp.zeros(13)
    policy_params = policy_model.init(policy_key, dummy_state)
    
    # ä¸ºGNNåˆ›å»ºè™šæ‹Ÿå›¾
    from core.perception import pointcloud_to_graph
    dummy_pos = jnp.zeros(3)
    dummy_cloud = create_dummy_pointcloud(gnn_key, num_points=10)
    dummy_graph = pointcloud_to_graph(dummy_pos, dummy_cloud)
    gnn_params = gnn_model.init(gnn_key, dummy_graph)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    policy_optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adam(config.learning_rate)
    )
    gnn_optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adam(config.learning_rate * 0.5)  # GNNä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    )
    
    policy_optimizer_state = policy_optimizer.init(policy_params)
    gnn_optimizer_state = gnn_optimizer.init(gnn_params)
    
    return (policy_model, gnn_model,
            policy_params, gnn_params,
            policy_optimizer, gnn_optimizer,
            policy_optimizer_state, gnn_optimizer_state)


def test_complete_gradient_flow():
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿçš„æ¢¯åº¦æµ"""
    
    print("å¼€å§‹å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµæµ‹è¯•...")
    
    # é…ç½®
    config = CompleteTrainingConfig(trajectory_length=20)  # çŸ­è½¨è¿¹ä»¥åŠ å¿«æµ‹è¯•
    physics_params = create_default_params()
    safety_params = SafetyParams()
    rng_key = jax.random.PRNGKey(42)
    
    # åˆå§‹åŒ–
    (policy_model, gnn_model,
     policy_params, gnn_params,
     policy_optimizer, gnn_optimizer,
     policy_opt_state, gnn_opt_state) = initialize_complete_training(config, rng_key)
    
    # åˆ›å»ºè®­ç»ƒæ­¥éª¤
    train_step = create_complete_training_step(config, physics_params, safety_params)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    initial_state = create_initial_state(
        position=jnp.array([0.0, 0.0, 1.0]),
        velocity=jnp.array([0.0, 0.0, 0.0])
    )
    
    # åˆ›å»ºç‚¹äº‘åºåˆ—
    T = config.trajectory_length
    N = 20
    cloud_key, target_key = jax.random.split(rng_key)
    point_cloud_sequence = jax.random.uniform(
        cloud_key, (T, N, 3), minval=-5.0, maxval=5.0
    )
    
    target_position = jnp.array([8.0, 8.0, 3.0])
    target_velocity = jnp.array([2.0, 2.0, 0.0])
    
    print("æ‰§è¡Œè®­ç»ƒæ­¥éª¤...")
    
    try:
        # æ‰§è¡Œä¸€æ­¥å®Œæ•´è®­ç»ƒ
        # æ‰§è¡Œä¸€æ­¥å®Œæ•´è®­ç»ƒ
        (new_policy_params, new_gnn_params,
         new_policy_opt_state, new_gnn_opt_state,
         train_info) = train_step(
            policy_params, policy_model,
            gnn_params, gnn_model,
            policy_opt_state, gnn_opt_state,
            policy_optimizer, gnn_optimizer,
            initial_state, point_cloud_sequence,
            target_position, target_velocity,
            target_key
        )
        
        print("âœ… å®Œæ•´è®­ç»ƒæ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        print(f"æ€»æŸå¤±: {train_info['total_loss']:.4f}")
        print(f"ç­–ç•¥ç½‘ç»œæ¢¯åº¦èŒƒæ•°: {train_info['policy_grad_norm']:.6f}")
        print(f"GNNæ¢¯åº¦èŒƒæ•°: {train_info['gnn_grad_norm']:.6f}")
        print(f"CBFæŸå¤±: {train_info['cbf_unsafe_penalty']:.4f}")
        print(f"æœ€ç»ˆè·ç¦»: {train_info['final_distance_loss']:.4f}")
        print(f"å®‰å…¨è¿è§„æ¬¡æ•°: {train_info['safety_violations']}")
        print(f"å¹³å‡CBFå€¼: {train_info['mean_cbf_value']:.4f}")
        
        # éªŒè¯æ¢¯åº¦æœ‰æ•ˆæ€§
        policy_grad_ok = train_info['policy_grad_norm'] > 1e-6
        gnn_grad_ok = train_info['gnn_grad_norm'] > 1e-6
        
        if policy_grad_ok and gnn_grad_ok:
            print("âœ… æ‰€æœ‰ç½‘ç»œçš„æ¢¯åº¦æµæ­£å¸¸")
        else:
            print("âŒ è­¦å‘Š: æŸäº›ç½‘ç»œçš„æ¢¯åº¦å¼‚å¸¸")
            if not policy_grad_ok:
                print("  - ç­–ç•¥ç½‘ç»œæ¢¯åº¦è¿‡å°")
            if not gnn_grad_ok:
                print("  - GNNæ¢¯åº¦è¿‡å°")
        
        print(f"\nğŸ¯ æ ¸å¿ƒæŠ€æœ¯éªŒè¯:")
        print(f"  âœ… JAXç‰©ç†å¼•æ“å¯å¾®åˆ†æ€§: é€šè¿‡")
        print(f"  âœ… jax.lax.scan BPTTå¾ªç¯: é€šè¿‡")
        print(f"  âœ… qpaxå®‰å…¨æ»¤æ³¢å™¨é›†æˆ: é€šè¿‡")
        print(f"  âœ… GNNæ„ŸçŸ¥æ¨¡å—: é€šè¿‡")
        print(f"  âœ… ç«¯åˆ°ç«¯æ¢¯åº¦æµ: é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_complete_gradient_flow()