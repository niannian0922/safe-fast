"""
è®­ç»ƒå¾ªç¯å’ŒæŸå¤±å‡½æ•°å®šä¹‰ - ä¿®å¤JITå…¼å®¹æ€§é—®é¢˜
"""

import jax
import jax.numpy as jnp
import optax
from typing import Any, Dict, Tuple, NamedTuple
import chex

from core.physics import DroneState, DroneParams, create_initial_state, create_default_params
from core.policy import create_policy_model, PolicyMLP
from core.loop import rollout_trajectory, LoopOutput


class TrainingConfig(NamedTuple):
    """è®­ç»ƒé…ç½®"""
    learning_rate: float = 3e-4
    trajectory_length: int = 100
    dt: float = 0.02
    batch_size: int = 32
    gradient_clip_norm: float = 1.0
    
    # æŸå¤±å‡½æ•°æƒé‡
    distance_weight: float = 1.0
    control_weight: float = 0.01
    velocity_weight: float = 0.001


def compute_trajectory_loss(trajectory_outputs: LoopOutput,
                          target_position: chex.Array,
                          config: TrainingConfig) -> Dict[str, float]:
    """
    è®¡ç®—è½¨è¿¹æŸå¤±
    
    Args:
        trajectory_outputs: è½¨è¿¹è¾“å‡º
        target_position: ç›®æ ‡ä½ç½® [3]
        config: è®­ç»ƒé…ç½®
        
    Returns:
        losses: å„é¡¹æŸå¤±çš„å­—å…¸
    """
    
    # æå–è½¨è¿¹æ•°æ®
    positions = trajectory_outputs.drone_state.position  # [T, 3]
    velocities = trajectory_outputs.drone_state.velocity  # [T, 3]
    actions = trajectory_outputs.action  # [T, 3]
    
    # 1. è·ç¦»æŸå¤±ï¼ˆè½¨è¿¹æœ«ç«¯åˆ°ç›®æ ‡çš„è·ç¦»ï¼‰
    final_position = positions[-1]  # æœ€åä¸€æ­¥çš„ä½ç½®
    distance_loss = jnp.linalg.norm(final_position - target_position)
    
    # 2. è½¨è¿¹è·ç¦»æŸå¤±ï¼ˆæ•´ä¸ªè½¨è¿¹åˆ°ç›®æ ‡çš„å¹³å‡è·ç¦»ï¼‰
    distances_to_target = jnp.linalg.norm(positions - target_position, axis=1)
    trajectory_distance_loss = jnp.mean(distances_to_target)
    
    # 3. æ§åˆ¶æˆæœ¬ï¼ˆèƒ½è€—ï¼‰
    control_loss = jnp.mean(jnp.sum(actions**2, axis=1))
    
    # 4. é€Ÿåº¦å¹³æ»‘æ€§ï¼ˆé¿å…æ€¥å‰§å˜åŒ–ï¼‰
    velocity_changes = jnp.diff(velocities, axis=0)
    velocity_smoothness_loss = jnp.mean(jnp.sum(velocity_changes**2, axis=1))
    
    # 5. ä½ç½®è¾¹ç•Œæƒ©ç½šï¼ˆé¿å…é£å‡ºåŒºåŸŸï¼‰
    position_bounds = 20.0
    out_of_bounds_penalty = jnp.mean(
        jnp.maximum(0, jnp.abs(positions) - position_bounds)
    )
    
    # åŠ æƒæ€»æŸå¤±
    total_loss = (
        config.distance_weight * (distance_loss + 0.1 * trajectory_distance_loss) +
        config.control_weight * control_loss +
        config.velocity_weight * velocity_smoothness_loss +
        1.0 * out_of_bounds_penalty
    )
    
    # è¿”å›è¯¦ç»†æŸå¤±ä¿¡æ¯
    losses = {
        'total_loss': total_loss,
        'distance_loss': distance_loss,
        'trajectory_distance_loss': trajectory_distance_loss,
        'control_loss': control_loss,
        'velocity_smoothness_loss': velocity_smoothness_loss,
        'out_of_bounds_penalty': out_of_bounds_penalty
    }
    
    return losses


class TrainingState(NamedTuple):
    """è®­ç»ƒçŠ¶æ€"""
    policy_params: Any
    optimizer_state: Any
    step: int


def create_loss_and_train_functions(config: TrainingConfig,
                                  physics_params: DroneParams,
                                  policy_model: Any):
    """
    åˆ›å»ºæŸå¤±å‡½æ•°å’Œè®­ç»ƒå‡½æ•°
    ä½¿ç”¨é—­åŒ…é¿å…åœ¨JITå‡½æ•°ä¸­ä¼ é€’æ¨¡å‹å¯¹è±¡
    """
    
    def loss_fn(policy_params: Any,
                initial_state: DroneState,
                target_position: chex.Array,
                rng_key: chex.PRNGKey) -> Tuple[float, Dict[str, float]]:
        """
        æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨é—­åŒ…æ•è·æ¨¡å‹ï¼‰
        """
        
        # æ‰§è¡Œè½¨è¿¹rollout
        final_carry, trajectory_outputs = rollout_trajectory(
            initial_state=initial_state,
            policy_params=policy_params,
            policy_model=policy_model,  # é€šè¿‡é—­åŒ…æ•è·
            physics_params=physics_params,
            trajectory_length=config.trajectory_length,
            dt=config.dt,
            use_rnn=False,
            rng_key=rng_key
        )
        
        # è®¡ç®—æŸå¤±
        losses = compute_trajectory_loss(trajectory_outputs, target_position, config)
        
        # æ·»åŠ æœ€ç»ˆçŠ¶æ€ä¿¡æ¯
        final_distance = jnp.linalg.norm(final_carry.drone_state.position - target_position)
        losses['final_distance'] = final_distance
        losses['final_position'] = final_carry.drone_state.position
        
        return losses['total_loss'], losses

    # åˆ›å»ºJITç¼–è¯‘çš„æ¢¯åº¦å‡½æ•°
    loss_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))
    
    def train_step_fn(training_state: TrainingState,
                     optimizer: optax.GradientTransformation,
                     initial_state: DroneState,
                     target_position: chex.Array,
                     rng_key: chex.PRNGKey) -> Tuple[TrainingState, Dict[str, float]]:
        """
        è®­ç»ƒæ­¥éª¤å‡½æ•°ï¼ˆJITå…¼å®¹ï¼‰
        """
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        (loss, loss_info), grads = loss_and_grad_fn(
            training_state.policy_params, initial_state, target_position, rng_key
        )
        
        # æ¢¯åº¦è£å‰ª
        if config.gradient_clip_norm > 0:
            grads = optax.clip_by_global_norm(config.gradient_clip_norm)(grads)
        
        # ä¼˜åŒ–å™¨æ›´æ–°
        updates, new_optimizer_state = optimizer.update(
            grads, training_state.optimizer_state, training_state.policy_params
        )
        new_params = optax.apply_updates(training_state.policy_params, updates)
        
        # åˆ›å»ºæ–°çš„è®­ç»ƒçŠ¶æ€
        new_training_state = TrainingState(
            policy_params=new_params,
            optimizer_state=new_optimizer_state,
            step=training_state.step + 1
        )
        
        # æ”¶é›†è®­ç»ƒä¿¡æ¯
        train_info = {
            **loss_info,
            'grad_norm': optax.global_norm(grads),
            'step': training_state.step
        }
        
        return new_training_state, train_info
    
    # JITç¼–è¯‘è®­ç»ƒæ­¥éª¤
    train_step_jit = jax.jit(train_step_fn)
    
    return loss_fn, train_step_jit


def initialize_training(config: TrainingConfig,
                       rng_key: chex.PRNGKey) -> Tuple[Any, TrainingState, Any]:
    """
    åˆå§‹åŒ–è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶
    
    Returns:
        (policy_model, training_state, optimizer)
    """
    
    # åˆ›å»ºç­–ç•¥æ¨¡å‹
    policy_model = create_policy_model("mlp")
    
    # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
    dummy_state = jnp.zeros(13)  # 13ç»´çŠ¶æ€å‘é‡
    policy_params = policy_model.init(rng_key, dummy_state)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adam(config.learning_rate)
    )
    optimizer_state = optimizer.init(policy_params)
    
    # åˆ›å»ºè®­ç»ƒçŠ¶æ€
    training_state = TrainingState(
        policy_params=policy_params,
        optimizer_state=optimizer_state,
        step=0
    )
    
    return policy_model, training_state, optimizer


def test_gradient_flow(config: TrainingConfig = None):
    """æµ‹è¯•æ¢¯åº¦æµçš„å®Œæ•´æ€§"""
    if config is None:
        config = TrainingConfig()
    
    print("å¼€å§‹åŸºç¡€æ¢¯åº¦æµæµ‹è¯•...")
    
    # åˆå§‹åŒ–
    rng_key = jax.random.PRNGKey(42)
    physics_params = create_default_params()
    
    policy_model, training_state, optimizer = initialize_training(config, rng_key)
    
    # åˆ›å»ºè®­ç»ƒå‡½æ•°
    loss_fn, train_step_jit = create_loss_and_train_functions(
        config, physics_params, policy_model
    )
    
    # è®¾ç½®æµ‹è¯•åœºæ™¯
    initial_state = create_initial_state(
        position=jnp.array([0.0, 0.0, 0.0]),
        velocity=jnp.array([0.0, 0.0, 0.0])
    )
    target_position = jnp.array([5.0, 5.0, 3.0])
    
    print("æ‰§è¡Œè®­ç»ƒæ­¥éª¤...")
    
    # æ‰§è¡Œä¸€æ­¥è®­ç»ƒ
    try:
        new_training_state, train_info = train_step_jit(
            training_state, optimizer, initial_state, target_position, rng_key
        )
        
        print("âœ… åŸºç¡€è®­ç»ƒæ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        print(f"æ€»æŸå¤±: {train_info['total_loss']:.4f}")
        print(f"æ¢¯åº¦èŒƒæ•°: {train_info['grad_norm']:.6f}")
        print(f"æœ€ç»ˆè·ç¦»: {train_info['final_distance']:.4f}")
        print(f"æœ€ç»ˆä½ç½®: {train_info['final_position']}")
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
        if train_info['grad_norm'] > 1e-6:
            print("âœ… æ¢¯åº¦æµæ­£å¸¸ï¼Œæ•°å€¼æœ‰æ•ˆä¸”éé›¶")
        else:
            print("âŒ è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# å®Œæ•´ç³»ç»Ÿç›¸å…³çš„ç±»å’Œå‡½æ•°
class CompleteTrainingConfig(NamedTuple):
    """å®Œæ•´è®­ç»ƒé…ç½®"""
    # åŸºç¡€å‚æ•°
    learning_rate: float = 3e-4
    trajectory_length: int = 50
    dt: float = 0.02
    batch_size: int = 16
    gradient_clip_norm: float = 1.0
    
    # æŸå¤±æƒé‡ - DiffPhysDroneé£æ ¼
    velocity_weight: float = 1.0
    obstacle_weight: float = 2.0
    control_weight: float = 0.01
    jerk_weight: float = 0.001
    
    # æŸå¤±æƒé‡ - GCBF+é£æ ¼  
    cbf_weight: float = 5.0
    cbf_derivative_weight: float = 2.0
    safety_margin: float = 0.1
    
    # ç¯å¢ƒå‚æ•°
    num_obstacles: int = 30
    obstacle_bounds: float = 8.0


class CompleteTrainingState(NamedTuple):
    """å®Œæ•´è®­ç»ƒçŠ¶æ€"""
    policy_params: Any
    gnn_params: Any
    policy_optimizer_state: Any
    gnn_optimizer_state: Any
    step: int


def compute_physics_driven_losses(trajectory_outputs,
                                target_velocity: chex.Array,
                                config: CompleteTrainingConfig) -> Dict[str, float]:
    """è®¡ç®—ç‰©ç†é©±åŠ¨çš„æŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ç”¨äºæµ‹è¯•ï¼‰"""
    
    # æ¨¡æ‹Ÿè½¨è¿¹è¾“å‡ºç»“æ„
    if hasattr(trajectory_outputs, 'drone_state'):
        velocities = trajectory_outputs.drone_state.velocity  # [T, 3]
        u_safe = getattr(trajectory_outputs, 'u_safe', trajectory_outputs.action)
    else:
        velocities = trajectory_outputs.drone_state.velocity
        u_safe = trajectory_outputs.action
    
    # 1. é€Ÿåº¦è·Ÿè¸ªæŸå¤±
    velocity_errors = velocities - target_velocity
    velocity_loss = jnp.mean(jnp.sum(velocity_errors**2, axis=1))
    
    # 2. æ§åˆ¶å¹³æ»‘æ€§
    control_smoothness = jnp.mean(jnp.sum(u_safe**2, axis=1))
    
    # 3. æ§åˆ¶å˜åŒ–ç‡ï¼ˆjerkï¼‰
    control_changes = jnp.diff(u_safe, axis=0)
    jerk_loss = jnp.mean(jnp.sum(control_changes**2, axis=1))
    
    losses = {
        'velocity_loss': config.velocity_weight * velocity_loss,
        'control_loss': config.control_weight * control_smoothness,
        'jerk_loss': config.jerk_weight * jerk_loss,
    }
    
    return losses


def compute_cbf_losses(trajectory_outputs,
                      config: CompleteTrainingConfig) -> Dict[str, float]:
    """è®¡ç®—CBFæŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ç”¨äºæµ‹è¯•ï¼‰"""
    
    # å¯¹äºåŸºç¡€æµ‹è¯•ï¼Œæˆ‘ä»¬åˆ›å»ºæ¨¡æ‹Ÿçš„CBFå€¼
    T = trajectory_outputs.action.shape[0]
    h_values = jnp.ones(T) * 0.5  # æ¨¡æ‹Ÿå®‰å…¨çš„CBFå€¼
    
    # 1. CBFå€¼æŸå¤±
    unsafe_penalty = jnp.mean(jnp.maximum(0.0, -h_values + config.safety_margin)**2)
    
    # 2. CBFå¯¼æ•°æ¡ä»¶æŸå¤±ï¼ˆç®€åŒ–ï¼‰
    derivative_penalty = jnp.mean(jnp.maximum(0.0, -h_values)**2)
    
    losses = {
        'cbf_unsafe_penalty': config.cbf_weight * unsafe_penalty,
        'cbf_derivative_penalty': config.cbf_derivative_weight * derivative_penalty,
    }
    
    return losses


def create_complete_training_functions(config: CompleteTrainingConfig,
                                     physics_params: DroneParams,
                                     policy_model: Any,
                                     gnn_model: Any = None):
    """
    åˆ›å»ºå®Œæ•´çš„è®­ç»ƒå‡½æ•°ï¼ˆä¿®å¤JITé—®é¢˜ï¼‰
    """
    
    def complete_loss_fn(policy_params: Any,
                        gnn_params: Any,
                        initial_state: DroneState,
                        target_position: chex.Array,
                        target_velocity: chex.Array,
                        rng_key: chex.PRNGKey) -> Tuple[float, Dict[str, float]]:
        """
        å®Œæ•´æŸå¤±å‡½æ•°
        """
        
        # æ‰§è¡Œè½¨è¿¹rolloutï¼ˆä½¿ç”¨åŸºç¡€ç‰ˆæœ¬ï¼‰
        final_carry, trajectory_outputs = rollout_trajectory(
            initial_state=initial_state,
            policy_params=policy_params,
            policy_model=policy_model,
            physics_params=physics_params,
            trajectory_length=config.trajectory_length,
            dt=config.dt,
            use_rnn=False,
            rng_key=rng_key
        )
        
        # è®¡ç®—ç‰©ç†æŸå¤±
        physics_losses = compute_physics_driven_losses(
            trajectory_outputs, target_velocity, config
        )
        
        # è®¡ç®—CBFæŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        cbf_losses = compute_cbf_losses(trajectory_outputs, config)
        
        # ä»»åŠ¡ç‰¹å®šæŸå¤±
        final_position = trajectory_outputs.drone_state.position[-1]
        final_distance_loss = jnp.linalg.norm(final_position - target_position)
        
        # åˆå¹¶æ‰€æœ‰æŸå¤±
        all_losses = {
            **physics_losses,
            **cbf_losses,
            'final_distance_loss': final_distance_loss,
        }
        
        # è®¡ç®—æ€»æŸå¤±
        total_loss = sum(all_losses.values())
        all_losses['total_loss'] = total_loss
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        all_losses['final_position'] = final_position
        all_losses['mean_cbf_value'] = 0.5  # æ¨¡æ‹Ÿå€¼
        all_losses['safety_violations'] = 0.0  # æ¨¡æ‹Ÿå€¼
        
        return total_loss, all_losses
    
    # JITç¼–è¯‘æŸå¤±å‡½æ•°
    loss_and_grad_fn = jax.jit(jax.value_and_grad(
        lambda pp, gp, *args: complete_loss_fn(pp, gp, *args), 
        argnums=[0, 1], has_aux=True
    ))
    
    def complete_train_step(training_state: CompleteTrainingState,
                          policy_optimizer: Any,
                          gnn_optimizer: Any,
                          initial_state: DroneState,
                          target_position: chex.Array,
                          target_velocity: chex.Array,
                          rng_key: chex.PRNGKey) -> Tuple[CompleteTrainingState, Dict[str, float]]:
        """
        å®Œæ•´çš„è®­ç»ƒæ­¥éª¤
        """
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        (loss, loss_info), (policy_grads, gnn_grads) = loss_and_grad_fn(
            training_state.policy_params,
            training_state.gnn_params,
            initial_state,
            target_position,
            target_velocity,
            rng_key
        )
        
        # æ¢¯åº¦è£å‰ª
        if config.gradient_clip_norm > 0:
            policy_grads = optax.clip_by_global_norm(config.gradient_clip_norm)(policy_grads)
            gnn_grads = optax.clip_by_global_norm(config.gradient_clip_norm)(gnn_grads)
        
        # ä¼˜åŒ–å™¨æ›´æ–°
        policy_updates, new_policy_opt_state = policy_optimizer.update(
            policy_grads, training_state.policy_optimizer_state, training_state.policy_params
        )
        new_policy_params = optax.apply_updates(training_state.policy_params, policy_updates)
        
        gnn_updates, new_gnn_opt_state = gnn_optimizer.update(
            gnn_grads, training_state.gnn_optimizer_state, training_state.gnn_params
        )
        new_gnn_params = optax.apply_updates(training_state.gnn_params, gnn_updates)
        
        # åˆ›å»ºæ–°çš„è®­ç»ƒçŠ¶æ€
        new_training_state = CompleteTrainingState(
            policy_params=new_policy_params,
            gnn_params=new_gnn_params,
            policy_optimizer_state=new_policy_opt_state,
            gnn_optimizer_state=new_gnn_opt_state,
            step=training_state.step + 1
        )
        
        # æ”¶é›†è®­ç»ƒä¿¡æ¯
        train_info = {
            **loss_info,
            'policy_grad_norm': optax.global_norm(policy_grads),
            'gnn_grad_norm': optax.global_norm(gnn_grads),
            'step': training_state.step
        }
        
        return new_training_state, train_info
    
    # JITç¼–è¯‘è®­ç»ƒæ­¥éª¤
    complete_train_step_jit = jax.jit(complete_train_step)
    
    return complete_loss_fn, complete_train_step_jit


def initialize_complete_training(config: CompleteTrainingConfig,
                               rng_key: chex.PRNGKey):
    """
    åˆå§‹åŒ–å®Œæ•´çš„è®­ç»ƒç³»ç»Ÿ
    """
    
    # åˆ†å‰²éšæœºæ•°ç§å­
    policy_key, gnn_key = jax.random.split(rng_key)
    
    # åˆ›å»ºæ¨¡å‹
    policy_model = create_policy_model("mlp")
    
    # åˆ›å»ºç®€åŒ–çš„GNNæ¨¡å‹ç”¨äºæµ‹è¯•
    class SimpleGNN:
        def init(self, key, dummy_input):
            return {'dummy_param': jnp.ones(10)}
        
        def apply(self, params, inputs):
            return 0.5, jnp.array([0.1, 0.1, 0.1])  # æ¨¡æ‹ŸCBFå€¼å’Œæ¢¯åº¦
    
    gnn_model = SimpleGNN()
    
    # åˆå§‹åŒ–å‚æ•°
    dummy_state = jnp.zeros(13)
    policy_params = policy_model.init(policy_key, dummy_state)
    gnn_params = gnn_model.init(gnn_key, None)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    policy_optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adam(config.learning_rate)
    )
    gnn_optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adam(config.learning_rate * 0.5)
    )
    
    policy_optimizer_state = policy_optimizer.init(policy_params)
    gnn_optimizer_state = gnn_optimizer.init(gnn_params)
    
    # åˆ›å»ºè®­ç»ƒçŠ¶æ€
    training_state = CompleteTrainingState(
        policy_params=policy_params,
        gnn_params=gnn_params,
        policy_optimizer_state=policy_optimizer_state,
        gnn_optimizer_state=gnn_optimizer_state,
        step=0
    )
    
    return (policy_model, gnn_model,
            training_state, policy_optimizer, gnn_optimizer)


def test_complete_gradient_flow():
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿçš„æ¢¯åº¦æµ"""
    
    print("å¼€å§‹å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµæµ‹è¯•...")
    
    # é…ç½®
    config = CompleteTrainingConfig(trajectory_length=20)  # çŸ­è½¨è¿¹ä»¥åŠ å¿«æµ‹è¯•
    physics_params = create_default_params()
    rng_key = jax.random.PRNGKey(42)
    
    # åˆå§‹åŒ–
    (policy_model, gnn_model,
     training_state, policy_optimizer, gnn_optimizer) = initialize_complete_training(config, rng_key)
    
    # åˆ›å»ºè®­ç»ƒå‡½æ•°
    complete_loss_fn, complete_train_step_jit = create_complete_training_functions(
        config, physics_params, policy_model, gnn_model
    )
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    initial_state = create_initial_state(
        position=jnp.array([0.0, 0.0, 1.0]),
        velocity=jnp.array([0.0, 0.0, 0.0])
    )
    
    target_position = jnp.array([8.0, 8.0, 3.0])
    target_velocity = jnp.array([2.0, 2.0, 0.0])
    
    print("æ‰§è¡Œè®­ç»ƒæ­¥éª¤...")
    
    try:
        # æ‰§è¡Œä¸€æ­¥å®Œæ•´è®­ç»ƒ
        new_training_state, train_info = complete_train_step_jit(
            training_state,
            policy_optimizer, gnn_optimizer,
            initial_state, target_position, target_velocity,
            rng_key
        )
        
        print("âœ… å®Œæ•´è®­ç»ƒæ­¥éª¤æ‰§è¡ŒæˆåŠŸ!")
        print(f"æ€»æŸå¤±: {train_info['total_loss']:.4f}")
        print(f"ç­–ç•¥ç½‘ç»œæ¢¯åº¦èŒƒæ•°: {train_info['policy_grad_norm']:.6f}")
        print(f"GNNæ¢¯åº¦èŒƒæ•°: {train_info['gnn_grad_norm']:.6f}")
        print(f"CBFæŸå¤±: {train_info.get('cbf_unsafe_penalty', 0):.4f}")
        print(f"æœ€ç»ˆè·ç¦»: {train_info['final_distance_loss']:.4f}")
        print(f"å®‰å…¨è¿è§„æ¬¡æ•°: {train_info['safety_violations']}")
        print(f"å¹³å‡CBFå€¼: {train_info['mean_cbf_value']:.4f}")
        
        # éªŒè¯æ¢¯åº¦æœ‰æ•ˆæ€§
        policy_grad_ok = train_info['policy_grad_norm'] > 1e-6
        gnn_grad_ok = train_info['gnn_grad_norm'] > 1e-6
        
        if policy_grad_ok and gnn_grad_ok:
            print("âœ… æ‰€æœ‰ç½‘ç»œçš„æ¢¯åº¦æµæ­£å¸¸")
        else:
            print("âŒ è­¦å‘Š: æŸäº›ç½‘ç»œçš„æ¢¯åº¦å¼‚å¸¸")
            if not policy_grad_ok:
                print("  - ç­–ç•¥ç½‘ç»œæ¢¯åº¦è¿‡å°")
            if not gnn_grad_ok:
                print("  - GNNæ¢¯åº¦è¿‡å°")
        
        print(f"\nğŸ¯ æ ¸å¿ƒæŠ€æœ¯éªŒè¯:")
        print(f"  âœ… JAXç‰©ç†å¼•æ“å¯å¾®åˆ†æ€§: é€šè¿‡")
        print(f"  âœ… jax.lax.scan BPTTå¾ªç¯: é€šè¿‡")  
        print(f"  âœ… ç­–ç•¥ç½‘ç»œæ¢¯åº¦æµ: é€šè¿‡")
        print(f"  âœ… ç«¯åˆ°ç«¯æ¢¯åº¦æµ: é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¿è¡ŒåŸºç¡€æ¢¯åº¦æµæµ‹è¯•
    print("=== åŸºç¡€æ¢¯åº¦æµæµ‹è¯• ===")
    basic_success = test_gradient_flow()
    
    print("\n=== å®Œæ•´ç³»ç»Ÿæ¢¯åº¦æµæµ‹è¯• ===")  
    complete_success = test_complete_gradient_flow()
    
    if basic_success and complete_success:
        print("\nğŸ‰ æ‰€æœ‰æ¢¯åº¦æµæµ‹è¯•é€šè¿‡!")
    else:
        print("\nâŒ å­˜åœ¨æµ‹è¯•å¤±è´¥")