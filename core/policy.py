"""
å®‰å…¨æ•æ·é£è¡Œç³»ç»Ÿçš„ç­–ç•¥ç½‘ç»œã€‚

æœ¬æ¨¡å—å®ç°ç¥ç»ç½‘ç»œç­–ç•¥ï¼Œç»“åˆä»¥ä¸‹ç ”ç©¶çš„è§è§£ï¼š
1. GCBF+ : ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œçš„åˆ†å¸ƒå¼å®‰å…¨æ§åˆ¶
2. DiffPhysDrone : ç«¯åˆ°ç«¯åŸºäºè§†è§‰çš„é£è¡Œæ§åˆ¶

ç­–ç•¥æ¶æ„æ”¯æŒå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œ
å…·æœ‰ç”¨äºæ—¶é—´ä¸€è‡´æ€§å’Œå®‰å…¨æ„è¯†çš„å¾ªç¯è®°å¿†ã€‚
"""

import jax
import jax.numpy as jnp
from jax import random
from typing import NamedTuple, Optional, Tuple, Callable, Any
import chex
from flax import linen as nn
from flax import struct
import optax


# =============================================================================
# ç­–ç•¥çŠ¶æ€è¡¨ç¤º
# =============================================================================

@struct.dataclass
class PolicyState:
    """å¸¦è®°å¿†çš„ç­–ç•¥ç½‘ç»œçš„çŠ¶æ€è¡¨ç¤ºã€‚"""
    rnn_state: chex.Array  # å¾ªç¯ç½‘ç»œçš„éšè—çŠ¶æ€
    step_count: int  # è¯¾ç¨‹å­¦ä¹ çš„å½“å‰æ­¥éª¤è®¡æ•°å™¨
    action_history: chex.Array  # ç”¨äºå¹³æ»‘æ€§çš„å…ˆå‰åŠ¨ä½œå†å²


@struct.dataclass 
class PolicyParams:
    """ç­–ç•¥ç½‘ç»œé…ç½®çš„å‚æ•°ã€‚"""
    # ç½‘ç»œæ¶æ„
    hidden_dims: Tuple[int, ...] = (256, 256)
    rnn_hidden_size: int = 256
    activation: str = "relu"
    use_rnn: bool = True
    
    # æ§åˆ¶çº¦æŸ
    max_thrust: float = 0.8
    thrust_smoothing: float = 0.95  # æŒ‡æ•°å¹³æ»‘å› å­
    
    # å®‰å…¨é›†æˆ
    enable_cbf_integration: bool = True
    safety_margin: float = 0.1
    
    # è®­ç»ƒè¶…å‚æ•°
    action_penalty_coef: float = 0.01
    smoothness_penalty_coef: float = 0.001


# =============================================================================
# åŸºç¡€ç­–ç•¥ç½‘ç»œ
# =============================================================================

class MLPBlock(nn.Module):
    """å…·æœ‰å¯é…ç½®æ¿€æ´»å‡½æ•°çš„å¤šå±‚æ„ŸçŸ¥å™¨å—ã€‚"""
    
    features: int
    activation: str = "relu"
    use_bias: bool = True
    dropout_rate: float = 0.0
    
    def setup(self):
        self.dense = nn.Dense(self.features, use_bias=self.use_bias)
        if self.dropout_rate > 0:
            self.dropout = nn.Dropout(self.dropout_rate)
        else:
            self.dropout = None
            
    def __call__(self, x: chex.Array, training: bool = False) -> chex.Array:
        x = self.dense(x)
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°
        if self.activation == "relu":
            x = nn.relu(x)
        elif self.activation == "tanh":
            x = nn.tanh(x)
        elif self.activation == "swish":
            x = nn.swish(x)
        elif self.activation == "gelu":
            x = nn.gelu(x)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {self.activation}")
            
        # å¦‚æœæŒ‡å®šåˆ™åº”ç”¨dropout
        if self.dropout is not None:
            x = self.dropout(x, deterministic=not training)
            
        return x


class PolicyNetworkMLP(nn.Module):
    """
    å•æ™ºèƒ½ä½“æ§åˆ¶çš„åŸºç¡€MLPç­–ç•¥ç½‘ç»œã€‚
    
    åŸºäºDiffPhysDroneçš„è½»é‡çº§æ¶æ„ï¼Œä½†é€šè¿‡JAX/Flaxå®ç°å¢å¼ºï¼Œ
    å¹¶æ”¹å–„äº†æ•°å€¼ç¨³å®šæ€§ã€‚
    """
    
    params: PolicyParams
    output_dim: int = 3  # 3Dæ¨åŠ›å‘½ä»¤
    
    def setup(self):
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åˆ›å»ºMLPå±‚ï¼ˆå…¼å®¹Flaxï¼‰
        self.layers = [
            MLPBlock(
                features=features,
                activation=self.params.activation,
                dropout_rate=0.1 if i < len(self.params.hidden_dims) - 1 else 0.0
            )
            for i, features in enumerate(self.params.hidden_dims)
        ]
        
        # å¸¦tanhæ¿€æ´»çš„è¾“å‡ºå±‚ç”¨äºæœ‰ç•Œæ§åˆ¶
        self.output_layer = nn.Dense(self.output_dim)
    
    def __call__(
        self, 
        observations: chex.Array,  # [batch_size, obs_dim] 
        training: bool = False
    ) -> chex.Array:
        """
        é€šè¿‡MLPç­–ç•¥çš„å‰å‘ä¼ é€’ã€‚
        
        å‚æ•°ï¼š
            observations: è¾“å…¥è§‚æµ‹å€¼
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
            
        è¿”å›ï¼š
            [-1, 1]èŒƒå›´å†…çš„æ§åˆ¶å‘½ä»¤
        """
        x = observations
        
        # é€šè¿‡éšè—å±‚å‰å‘ä¼ é€’
        for layer in self.layers:
            x = layer(x, training=training)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        # åº”ç”¨tanhè·å¾—æœ‰ç•Œè¾“å‡º
        control_output = nn.tanh(x)
        
        return control_output


# å¾ªç¯ç­–ç•¥ç½‘ç»œï¼ˆå—DiffPhysDroneå¯å‘ï¼‰å·ç§¯+å¾ªç¯æ··åˆæ¶æ„

class PolicyNetworkRNN(nn.Module):
    """
    ç”¨äºæ—¶é—´ä¸€è‡´æ€§çš„å¾ªç¯ç­–ç•¥ç½‘ç»œã€‚
    
    å®ç°DiffPhysDroneçš„CRNNæ¶æ„çš„å…³é”®è§è§£ï¼š
    - ç”¨äºæ—¶é—´è®°å¿†å’Œè§„åˆ’ä¸€è‡´æ€§çš„GRU
    - ç”¨äºå®æ—¶éƒ¨ç½²çš„è½»é‡çº§è®¾è®¡
    - é›†æˆåŠ¨ä½œå¹³æ»‘
    """
    
    params: PolicyParams
    output_dim: int = 3
    
    def setup(self):
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼çš„ç‰¹å¾æå–å±‚ï¼ˆå…¼å®¹Flaxï¼‰
        self.feature_layers = [
            MLPBlock(features=features, activation=self.params.activation)
            for features in self.params.hidden_dims[:-1]  # é™¤æœ€åä¸€ä¸ªå¤–çš„æ‰€æœ‰
        ]
        
        # ä¸éœ€è¦è®¾ç½®RNNå±‚ï¼Œæˆ‘ä»¬åœ¨__call__ä¸­ç›´æ¥ä½¿ç”¨GRUCell
        
        # è¾“å‡ºæŠ•å½±
        final_hidden_dim = self.params.hidden_dims[-1] if self.params.hidden_dims else self.params.rnn_hidden_size
        self.output_projection = nn.Dense(final_hidden_dim)
        self.control_head = nn.Dense(self.output_dim)
        
        # åŠ¨ä½œå†å²é›†æˆä»¥è·å¾—å¹³æ»‘æ€§
        self.action_history_proj = nn.Dense(self.params.rnn_hidden_size // 4)
    
    def __call__(
        self,
        observations: chex.Array,  # [batch_size, seq_len, obs_dim]
        rnn_state: chex.Array,     # [batch_size, rnn_hidden_size] 
        action_history: Optional[chex.Array] = None,  # [batch_size, history_len, 3]
        training: bool = False
    ) -> Tuple[chex.Array, chex.Array]:
        """
        é€šè¿‡RNNç­–ç•¥çš„å‰å‘ä¼ é€’ã€‚
        
        å‚æ•°ï¼š
            observations: è¾“å…¥è§‚æµ‹åºåˆ—
            rnn_state: ä¸Šä¸€ä¸ªRNNéšè—çŠ¶æ€
            action_history: ç”¨äºå¹³æ»‘çš„å…ˆå‰åŠ¨ä½œå†å²
            training: è®­ç»ƒæ¨¡å¼æ ‡å¿—
            
        è¿”å›ï¼š
            (control_commands, new_rnn_state)
        """
        batch_size = observations.shape[0]
        
        # ç‰¹å¾æå–:è¿›è¡Œåˆæ­¥çš„ç‰¹å¾æå–ã€‚è¿™ä¸€æ­¥å°†åŸå§‹çš„ã€å¯èƒ½ç»´åº¦å¾ˆé«˜çš„è§‚æµ‹æ•°æ®ï¼Œè½¬æ¢æˆäº†æ›´æŠ½è±¡ã€ä¿¡æ¯å¯†åº¦æ›´é«˜çš„ç‰¹å¾å‘é‡ x
        x = observations
        for layer in self.feature_layers:
            x = layer(x, training=training)
        
        # å¤„ç†åŠ¨ä½œå†å²ä»¥è·å¾—å¹³æ»‘æ€§ï¼ˆDiffPhysDroneè§è§£ï¼‰
        if action_history is not None:
            action_features = self.action_history_proj(
                action_history.reshape(batch_size, -1)
            )
            # ä¸å½“å‰ç‰¹å¾ç»„åˆ
            x = jnp.concatenate([x, action_features], axis=-1)
        
        # RNNå¤„ç† - åœ¨æ—¶é—´ç»´åº¦ä¸Šæ‰«æ
        rnn_cell = nn.GRUCell(features=self.params.rnn_hidden_size)#å®ä¾‹åŒ–ä¸€ä¸ª GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰ã€‚GRU æ˜¯ä¸€ç§æ¯”åŸºç¡€ RNN æ›´å…ˆè¿›çš„å¾ªç¯å•å…ƒï¼Œå®ƒå†…éƒ¨æœ‰â€œæ›´æ–°é—¨â€å’Œâ€œé‡ç½®é—¨â€ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ï¼Œå¹¶ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
        new_rnn_state, rnn_output = rnn_cell(rnn_state, x)#GRU å•å…ƒå†…éƒ¨è¿›è¡Œå¤æ‚çš„é—¨æ§è®¡ç®—ï¼Œèåˆæ–°æ—§ä¿¡æ¯ã€‚
        #æ˜¯ GRU å¯¹å½“å‰æƒ…å†µçš„ä¸€ä¸ªé«˜åº¦æµ“ç¼©çš„æ€»ç»“ï¼Œä½†å®ƒè¿˜ä¸æ˜¯æœ€ç»ˆçš„æ§åˆ¶æŒ‡ä»¤
        # è¾“å‡ºæŠ•å½±
        x = self.output_projection(rnn_output)
        x = nn.relu(x)
        
        # å¸¦æœ‰ç•Œè¾“å‡ºçš„æ§åˆ¶å¤´
        control_output = self.control_head(x)#ä»£ç å°†å®ƒé€šè¿‡å¦å¤–ä¸¤å±‚ MLP ,å°†å…¶â€œè§£ç â€æˆä¸€ä¸ª 3 ç»´çš„æ¨åŠ›å‘é‡ control_outputã€‚
        control_output = nn.tanh(control_output)  # ç»‘å®šåˆ°[-1, 1]
        
        return control_output, new_rnn_state


# =============================================================================
# ç­–ç•¥å·¥å‚å’Œå®ç”¨ç¨‹åº
# =============================================================================

def create_policy_network(
    params: PolicyParams,
    network_type: str = "mlp",
    output_dim: int = 3
) -> nn.Module:
    """
    åˆ›å»ºç­–ç•¥ç½‘ç»œçš„å·¥å‚å‡½æ•°ã€‚
    
    å‚æ•°ï¼š
        params: ç­–ç•¥å‚æ•°
        network_type: ç½‘ç»œç±»å‹ï¼ˆ"mlp" æˆ– "rnn"ï¼‰
        output_dim: è¾“å‡ºç»´åº¦
        
    è¿”å›ï¼š
        ç­–ç•¥ç½‘ç»œå®ä¾‹
    """
    if network_type == "mlp":
        return PolicyNetworkMLP(params=params, output_dim=output_dim)
    elif network_type == "rnn":
        return PolicyNetworkRNN(params=params, output_dim=output_dim)
    else:
        raise ValueError(f"æœªçŸ¥çš„ç½‘ç»œç±»å‹: {network_type}")


def init_policy_state(
    policy_params: PolicyParams,
    rng_key: chex.PRNGKey,
    batch_size: int = 1
) -> PolicyState:
    """åˆå§‹åŒ–ç­–ç•¥çŠ¶æ€ã€‚"""
    rnn_state = jnp.zeros((batch_size, policy_params.rnn_hidden_size))
    action_history = jnp.zeros((batch_size, 3, 3))  # æœ€å3ä¸ªåŠ¨ä½œ
    
    return PolicyState(
        rnn_state=rnn_state,
        step_count=0,
        action_history=action_history
    )


def apply_control_constraints(
    raw_control: chex.Array,
    params: PolicyParams,
    previous_action: Optional[chex.Array] = None
) -> chex.Array:
    """
    åº”ç”¨æ§åˆ¶çº¦æŸå’Œå¹³æ»‘ã€‚
    
    å®ç°æ¥è‡ªDiffPhysDroneçš„æ§åˆ¶å¤„ç†ï¼š
    - æ¨åŠ›å¹…åº¦çº¦æŸ
    - ç”¨äºç¨³å®šæ€§çš„æ—¶é—´å¹³æ»‘
    """
    # ç¼©æ”¾åˆ°å®é™…æ¨åŠ›èŒƒå›´
    control_output = raw_control * params.max_thrust
    
    # å¦‚æœæœ‰å¯ç”¨çš„ä¸Šä¸€ä¸ªåŠ¨ä½œï¼Œåº”ç”¨æŒ‡æ•°å¹³æ»‘
    if previous_action is not None:
        control_output = (
            params.thrust_smoothing * previous_action + 
            (1.0 - params.thrust_smoothing) * control_output
        )
    
    # å¼ºåˆ¶ç¡¬çº¦æŸ
    control_output = jnp.clip(control_output, -params.max_thrust, params.max_thrust)
    
    return control_output


# =============================================================================
# ç­–ç•¥è¯„ä¼°å’Œå®ç”¨ç¨‹åº
# =============================================================================

@jax.jit
def evaluate_policy_mlp(
    policy: nn.Module,
    params: chex.Array,
    observations: chex.Array,
    training: bool = False
) -> chex.Array:
    """MLPçš„JITç¼–è¯‘ç­–ç•¥è¯„ä¼°ã€‚"""
    return policy.apply(params, observations, training=training)


@jax.jit 
def evaluate_policy_rnn(
    policy: nn.Module,
    params: chex.Array,
    observations: chex.Array,
    rnn_state: chex.Array,
    action_history: Optional[chex.Array] = None,
    training: bool = False
) -> Tuple[chex.Array, chex.Array]:
    """RNNçš„JITç¼–è¯‘ç­–ç•¥è¯„ä¼°ã€‚"""
    return policy.apply(
        params, observations, rnn_state, action_history, training=training
    )


def compute_policy_loss_components(
    predicted_actions: chex.Array,
    target_actions: chex.Array,
    action_history: chex.Array,
    params: PolicyParams
) -> Tuple[chex.Array, dict]:
    """
    éµå¾ªDiffPhysDroneæ–¹æ³•è®¡ç®—ç­–ç•¥æŸå¤±ç»„ä»¶ã€‚
    
    å‚æ•°ï¼š
        predicted_actions: ç½‘ç»œè¾“å‡ºåŠ¨ä½œ
        target_actions: ç›®æ ‡åŠ¨ä½œï¼ˆæ¥è‡ªQPæ±‚è§£å™¨ï¼‰
        action_history: ç”¨äºå¹³æ»‘æ€§çš„å…ˆå‰åŠ¨ä½œ
        params: ç­–ç•¥å‚æ•°
        
    è¿”å›ï¼š
        (total_loss, loss_dict)
    """
    # åŠ¨ä½œè·Ÿè¸ªæŸå¤±ï¼ˆä¸»è¦ç›®æ ‡ï¼‰
    action_loss = jnp.mean((predicted_actions - target_actions) ** 2)#jnp.mean è®¡ç®—äº†æ‰€æœ‰è¿™äº›å¹³æ–¹è¯¯å·®çš„å¹³å‡å€¼ã€‚è¿™å°±æ˜¯æ ‡å‡†çš„å‡æ–¹è¯¯å·® (Mean Squared Error, MSE)ï¼Œæ˜¯å›å½’ä»»åŠ¡ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ã€‚
    
    # åŠ¨ä½œå¹…åº¦æƒ©ç½šï¼ˆèƒ½é‡æ•ˆç‡ï¼‰
    magnitude_loss = jnp.mean(jnp.sum(predicted_actions ** 2, axis=-1))
    #å°†æ¨åŠ›å‘é‡ [tx, ty, tz] çš„æ¯ä¸ªåˆ†é‡å¹³æ–¹ï¼Œå¾—åˆ° [tx^2, ty^2, tz^2]ã€‚
    #jnp.sum(..., axis=-1)ï¼šæ²¿ç€æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå³ xyz åˆ†é‡ï¼‰æ±‚å’Œï¼Œå¾—åˆ° tx^2 + ty^2 + tz^2ã€‚è¿™æ­£æ˜¯å‘é‡æ¨¡é•¿ï¼ˆè·ç¦»åŸç‚¹çš„è·ç¦»ï¼‰çš„å¹³æ–¹ã€‚
    #jnp.mean(...)ï¼šè®¡ç®—æ‰¹å¤„ç†ä¸­æ‰€æœ‰åŠ¨ä½œæ¨¡é•¿å¹³æ–¹çš„å¹³å‡å€¼ã€‚
    
    # å¹³æ»‘æ€§æƒ©ç½šï¼ˆåŸºäºåŠ¨ä½œå¯¼æ•°ï¼‰
    if action_history.shape[-2] > 1:  # è‡³å°‘éœ€è¦2ä¸ªå†å²æ­¥éª¤
        action_derivatives = jnp.diff(action_history, axis=-2)#jnp.diff å‡½æ•°è®¡ç®—äº†ä¸€ä¸ªæ•°ç»„ä¸­æ²¿æŒ‡å®šè½´çš„ N é˜¶å·®åˆ†
        smoothness_loss = jnp.mean(jnp.sum(action_derivatives ** 2, axis=-1))
    else:
        smoothness_loss = 0.0
    
    # ç»„åˆæŸå¤±
    total_loss = (
        action_loss + 
        params.action_penalty_coef * magnitude_loss +
        params.smoothness_penalty_coef * smoothness_loss
    )
    
    loss_dict = {
        "action_loss": action_loss,
        "magnitude_loss": magnitude_loss, 
        "smoothness_loss": smoothness_loss,
        "total_loss": total_loss
    }
    
    return total_loss, loss_dict


# =============================================================================
# æµ‹è¯•å’ŒéªŒè¯
# =============================================================================

def validate_policy_implementation():
    """éªŒè¯ç­–ç•¥ç½‘ç»œå®ç°ã€‚"""
    print("ğŸ§ª éªŒè¯ç­–ç•¥ç½‘ç»œå®ç°...")
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    params = PolicyParams(
        hidden_dims=(128, 64),
        rnn_hidden_size=128,
        use_rnn=True
    )
    
    # æµ‹è¯•MLPç­–ç•¥
    mlp_policy = create_policy_network(params, "mlp")
    
    # åˆå§‹åŒ–å‚æ•°
    key = random.PRNGKey(42)
    dummy_obs = jnp.ones((4, 10))  # æ‰¹é‡4ï¼Œè§‚æµ‹ç»´åº¦10
    
    mlp_params = mlp_policy.init(key, dummy_obs)
    mlp_output = mlp_policy.apply(mlp_params, dummy_obs)
    
    print(f"âœ… MLPç­–ç•¥: è¾“å…¥ {dummy_obs.shape} -> è¾“å‡º {mlp_output.shape}")
    assert mlp_output.shape == (4, 3), f"æœŸæœ›(4, 3)ï¼Œå¾—åˆ°{mlp_output.shape}"
    
    # æµ‹è¯•RNNç­–ç•¥
    rnn_policy = create_policy_network(params, "rnn")
    rnn_state = jnp.zeros((4, params.rnn_hidden_size))
    
    rnn_params = rnn_policy.init(key, dummy_obs, rnn_state)
    rnn_output, new_rnn_state = rnn_policy.apply(rnn_params, dummy_obs, rnn_state)
    
    print(f"âœ… RNNç­–ç•¥: è¾“å…¥ {dummy_obs.shape} -> è¾“å‡º {rnn_output.shape}")
    assert rnn_output.shape == (4, 3), f"æœŸæœ›(4, 3)ï¼Œå¾—åˆ°{rnn_output.shape}"
    assert new_rnn_state.shape == rnn_state.shape, "RNNçŠ¶æ€å½¢çŠ¶ä¸åŒ¹é…"
    
    # æµ‹è¯•JITç¼–è¯‘
    jit_mlp = jax.jit(mlp_policy.apply)
    jit_output = jit_mlp(mlp_params, dummy_obs)
    
    print(f"âœ… JITç¼–è¯‘: MLPç­–ç•¥ç¼–è¯‘æˆåŠŸ")
    assert jnp.allclose(mlp_output, jit_output), "JITè¾“å‡ºä¸åŒ¹é…"
    
    # æµ‹è¯•æ§åˆ¶çº¦æŸ
    raw_control = jnp.array([[0.8, -0.6, 1.2], [-0.5, 0.9, -0.3]])
    prev_action = jnp.array([[0.1, -0.1, 0.2], [-0.2, 0.3, -0.1]])
    
    constrained_control = apply_control_constraints(raw_control, params, prev_action)
    print(f"âœ… æ§åˆ¶çº¦æŸ: åº”ç”¨æˆåŠŸ")
    
    # éªŒè¯è¾¹ç•Œ
    assert jnp.all(jnp.abs(constrained_control) <= params.max_thrust), "æ§åˆ¶è¾¹ç•Œè¢«è¿å"
    
    print("ğŸ‰ ç­–ç•¥ç½‘ç»œéªŒè¯: æ‰€æœ‰æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    validate_policy_implementation()