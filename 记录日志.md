2025.10.1
整个团队先进行了讨论:
“我们对当前研发状态的观察与反思

近期完成的具体事项

### 1. 清理旧资产，重建训练基线
- 删除所有 legacy 训练脚本和结果目录，仅保留当前管线所需文件。
- 在安全层关闭的情况下重新训练效率阶段（`train_safe_policy.py --disable-safety`），loss 自 3.17e4 下降到 1.70e3，确认 BPTT 与策略梯度链路正常（结果见 `outputs/efficiency_test`）。

### 2. 安全层基础：解析 CBF 回退
- 在 `core/perception.py` 中实现软最小距离的解析 CBF `_analytic_cbf_statistics`，当神经网络未训练或数值不稳定时提供有限值/梯度/海森矩阵。
- 启用解析 CBF 后的安全训练（无噪声）loss ≈ 5.7e3、约束违背 ≈ 1e-9（见 `outputs/safety_cbf_pretrained`）。

### 3. CBF 神经网络预训练（持续迭代中）
- 初始版 `tools/pretrain_cbf.py`（批处理实现）依据解析 CBF 构建监督数据，并用 Optax Adam 训练 Flax 网络：
  - 数据集 256×100 步 → loss ≈ 0.30；
  - 数据集 1024×2000 步 → loss ≈ 0.09~0.23（最终快照 `outputs/cbf_pretrained.pkl`）。
- 支持 `--init-params` 续训，可逐步扩大数据集或延长训练时间。

### 4. 含噪声课程的安全训练实验
- 引入噪声/课程 CLI 参数（`--stage-steps`, `--noise-levels`, `--disable-curriculum`）。
- 在噪声课程 `(100,100,100)` / `noise=(0.0,0.02,0.05)` 下训练 300 步（`outputs/safety_noise_cbf`）：loss ≈ 3.16e3、约束违背 ≈ 0.28。
- 提供多组调参实验（例如 `(150,100,50)`）作为后续对比。

### 5. 工具及日志完善
- `tools/stage_summary.py`：可扫描任意输出目录并生成 JSON 指标（loss、约束违背、梯度范数等）。
- `conversation_log.txt`：已记录每阶段任务与结论，便于追踪。


当前能力与限制

### 已实现能力
1. **效率阶段**：纯策略训练与 BPTT 链路稳定可复现（`outputs/efficiency_test`）。
2. **安全层基础**：解析 CBF + qpax + Optax 训练可运行，并可记录约束违背指标。
3. **CBF 预训练**：支持消费解析 CBF 监督、加载预训练参数进行端到端训练。
4. **噪声课程**：可配置三阶段噪声/权重并保存训练历史，用于比较不同策略。
5. **指标汇总**：工具脚本能自动输出 JSON 指标，便于脚本化分析。
6. **跨设备验证**：代码已在 macOS CPU 环境跑通；依赖纯 JAX/Flax/Optax，若团队成员在 Windows/Linux 上安装相同版本依赖，可直接复现训练；预训练参数和训练日志已记录在 `outputs/` 下，方便团队成员从 GitHub 拉取后加载。

### 仍存限制
1. **效率阶段成功率仍低**：当前 efficiency 示例仅 50 步，并未统计成功率；需要扩展训练步数、调节损失权重或课程，达到“几乎完美”后再用于后续阶段。
2. **神经 CBF 梯度稳定性**：在噪声较大的训练中约束违背仍较高，说明预训练数据或网络尚未充分学习解析 CBF 的结构。
3. **预训练效率**：工具脚本仍为批处理实现；大规模数据上耗时较长，未来需进一步向量化/JIT 以适配更大训练。
4. **噪声化主训练**：噪声课程的安全权衡仍需系统调参（噪声幅度、权重、loss 配比）。
5. **自动化流程**：效率→安全→噪声全流程尚未脚本化，需要额外的 wrapper 脚本以便批处理与 CI。
6. **环境差异**：Mac CPU 上的训练可作为“可行性验证”；团队成员在 Windows/GPU 上重新训练需确保环境同步（JAX、Flax 等版本，` requirements.txt`），并注意随机种子、浮点差异可能少许影响收敛速度。



核心文件职责概览

| 文件路径 | 主要功能 |
|----------|-----------|
| `train_safe_policy.py` | 统一训练入口，负责读取 `configs/default_config.py`、构建政策/安全/物理模块、执行 Optax 训练（支持噪声课程与解析/神经 CBF）。 |
| `core/loop.py` | JAX 原生 BPTT 循环，串联感知 → 策略 → CBF/QP → 物理引擎，支持噪声扰动与安全开关。 |
| `core/perception.py` | 构建图神经网络 CBF；当网络不稳定时使用软最小距离解析 CBF。 |
| `core/safety.py` | 带松弛变量的 CBF-QP（qpax），失败时优雅回退到名义控制，保持可微性。 |
| `tools/stage_summary.py` | 对训练输出目录（Pickle）生成 JSON/文本摘要，汇总 loss、约束违背等指标。 |
| `tools/pretrain_cbf.py` | 使用解析 CBF 监督预训练神经 CBF 网络（当前为批处理版本，支持 `--init-params` 续训）。 |
| `configs/default_config.py` | 统一配置：噪声课程、损失权重、物理参数等。 |
| `conversation_log.txt` | 记录每阶段主要决策和产出，可视作操作日志。 |
| `outputs/` 目录 | 保存各阶段训练结果（Pickle + 日志），便于团队成员加载复现；例如 `efficiency_test/`、`safety_noise_cbf/` 等。 |



下一步计划

近期
1. **效率阶段长程训练**：设置更长 episodes/horizon（例如 >1000），调节损失权重（目标 vs. 控制惩罚），统计成功率（可增加成功率计算脚本）。训练到“成功率接近 100%”后保存策略参数，作为安全阶段初始状态。
2. **完善 CBF 预训练**：
   - 将 `tools/pretrain_cbf.py` 改造成 vectorized/JIT 实现，减少 Python 循环；
   - 扩大数据集、迭代次数，使噪声环境下解析 loss 较低（目前 0.28 仍偏高）。
3. **噪声权重扫描**：
   - 编写脚本批量运行不同 `stage_steps`、`noise_levels`、`safety_weight` 组合；
   - 使用 `tools/stage_summary.py` 汇总指标，确定“噪声条件下约束违背 < 0.1” 的权重区间。

中期
1. **自动化训练流程**：封装效率 → 安全 → 噪声训练为统一命令（可接受配置），便于复现实验与 CI 集成。
2. **神经 CBF FFN 优化**：在解析表示稳固后，尝试学习 CBF 的梯度信息或引入更丰富的监督（如边界样本）。
3. **混合噪声与安全正则**：结合课程学习、约束权重退火或新正则手段，进一步降低噪声环境下的约束违背。
4. **跨平台验证**：补充在 Windows/GPU 环境的安装指导和示例脚本，确保团队成员可在更强硬件上无缝重训。

长期
1. **更大规模训练**：多噪声场景、多课程配置、完整策略+CBF 联合训练（换多 GPU/cluster）。
2. **神经 CBF 完全替换解析模型**：在预训练和安全联合训练稳定后，尝试完全依赖神经 CBF，评估解析回退的必要性。
3. **自动化报告与可视化**：构建脚本自动生成 Markdown/HTML 报告（结合训练曲线、约束指标），支持一键复现与审阅。”


这是我们以上讨论的内容,接着我们开始执行计划:


• 训练

  - 效率长程训练： 采用 --episodes 1500 --horizon 180 --disable-safety --disable-curriculum --noise-levels 0.0，输出目录 outputs/efficiency_long_run_zero_noise，训练损失收敛至约 9.0e2。
  - 历史表现： 训练早期存在较大末端误差（历史成功率≈0.83），属迭代探索阶段的数据，不影响最终策略质量。

  评估

  - 评估配置： 基于最终策略参数，使用与训练一致的物理与观测设定，在 200 个随机初始状态、无噪声条件下离线滚动。
  - 结果指标： 成功率 100%，末端距离均值 1.47 cm、P95 1.57 cm、最大值 1.59 cm，最终速度均值 6.5 mm/s。评估摘要存于 outputs/efficiency_long_run_zero_noise/eval_summary.json。

  交付物

  - 训练记录： outputs/efficiency_long_run_zero_noise/training_results.pkl（含策略参数、训练历史）。
  - 策略快照： outputs/efficiency_long_run_zero_noise/policy_params.pkl（供后续安全阶段载入）。
  - 评估摘要： outputs/efficiency_long_run_zero_noise/eval_summary.json（集中记录成功率与距离指标）。

  后续规划

  - 载入策略： 在安全阶段训练前，从 policy_params.pkl 读取参数以确保起点具备 ≥98% 的效率成功率。
  - 进一步验证：在噪声或不同目标位置下复用评估脚本扩展测试集。



20205.10.1下午

代码改动

  - train_safe_policy.py：新增 --policy-params 参数，main 支持加载外部策略权重。这样在开始安全阶段时可直接用效率训练得到的策略作为初始值。

效率策略资产

  - 策略快照：outputs/efficiency_long_run_zero_noise/policy_params.pkl
  - 训练记录：outputs/efficiency_long_run_zero_noise/training_results.pkl
  - 零噪声评估摘要（200 次 rollout）：outputs/efficiency_long_run_zero_noise/eval_summary.json

扩展评估

  - 使用上述策略，针对不同噪声（0.0/0.02/0.05）与目标位置（原始目标 + 两个偏移目标）共 9 组设置，每组 100 次 rollout。
  - 结果写入 outputs/efficiency_long_run_zero_noise/eval_extended.json。主要结论：
      - 原目标：噪声 0.0/0.02 成功率 100%，噪声 0.05 成功率 99%，末端距离均值 ≤ 4 cm。
      - 偏移目标：在未重新训练的情况下成功率几乎为 0，表明当前策略针对单一目标高度专用，后续若需多目标性能需继续扩展训练数据或采用目标条件输入。

下一步规划

  - 安全阶段调用 train_safe_policy.py --policy-params outputs/efficiency_long_run_zero_noise/policy_params.pkl ... 载入效率策略。
  - 若要提升多目标泛化，可在效率训练阶段引入目标条件或多目标采样，并重新评估扩展指标。



2025.10.2

• 训练执行

  - 安全阶段训练：运行 python3 train_safe_policy.py --episodes 1000 --horizon 180 --policy-params outputs/efficiency_long_run_zero_noise/policy_params.pkl --output-dir outputs/
    safety_stage_from_efficiency，保留安全层（解析 CBF 回退）。阶段损失稳定于 1.2e3，约束违背均值接近 0。训练资产存于 outputs/safety_stage_from_efficiency/training_results.pkl。

  评估结果

  - Stage summary：tools/stage_summary.py outputs/safety_stage_from_efficiency --json 记录末步损失 ≈1.21e3、约束违背 ≈4e-9。
  - 安全评估（200 次 rollout，各噪声水平）见 outputs/safety_stage_from_efficiency/eval_summary.json：
      - noise=0.00：成功率 78.5%，平均末端距离 0.83 m，表明在无观测噪声下易触发大范围松弛，成功率逊于含噪配置。
      - noise=0.02：成功率 98%，平均距离 6.1 cm，松弛激活率约 0.94，松弛均值 1.2e-3。
      - noise=0.05：成功率 92.5%，平均距离 7.3 cm，松弛激活率约 0.95，松弛均值 9.0e-4。

  资产

  - 策略权重：训练结果中的 params_policy（可用 --policy-params 载入）。
  - 评估文件：outputs/safety_stage_from_efficiency/eval_summary.json 汇总成功率、末距、松弛指标。

  后续规划

  - 提升零噪声稳健性：当前在 noise=0.0 时成功率偏低，考虑检视解析 CBF 的指标及松弛权重，或在训练中加入额外正则（如减小松弛上限、增加效率权重）。
  - CBF 网络训练：尽快替换纯解析回退，加载高质量神经 CBF，以降低高噪声条件下松弛使用率并提升整体成功率。需要时可继续评估不同目标或多阶段课程的效果。


  
  
  2025.10.2下午

  Zero 噪声失败诊断

  - 解析训练日志 (outputs/safety_stage_from_efficiency/training_results.pkl) 显示松弛激活率在后半程保持 ≥0.93，说明策略高度依赖松弛；零噪声评估（200 次 rollout）成功率仅 78.5%，平均末距
    0.83 m。
  - 最大松弛值虽小（平均 1e-3），但高激活率使得策略在安全层下动作被大幅抑制，导致多次无法抵达目标。

  尝试的调整

  - 将 config.safety.max_relaxation 从原值调整为 0.3，并把 relaxation_penalty 提升至 300，同时在训练命令中使用 --relaxation-weight 5.0 --stage-steps 400,300,300（延长零噪声阶段）。
  - 新模型训练后（outputs/safety_stage_low_relax），噪声 0.02/0.05 成功率提升到 99%/99.5%，但零噪声成功率仍仅 75.5%，表明单纯收紧松弛尚未解决问题。

  资产与结果

  - outputs/safety_stage_low_relax/training_results.pkl：最新安全训练记录。
  - outputs/safety_stage_low_relax/eval_summary.json：0/0.02/0.05 噪声评估（200 rollout）。
  - 旧版本结果保留在 outputs/safety_stage_from_efficiency/eval_summary.json 以供对比。

  后续规划

  1. 恢复较大的松弛上限（避免训练早期被限制），改而显著提高训练期对约束违背的惩罚（--relaxation-weight、--solver-weight）并加入明确的 slack 触发惩罚统计，避免过多依赖松弛。
  2. 验证解析 CBF 的可行域：通过统计 cbf_min 与真实障碍距离，确认软最小距离模型是否过于保守；必要时调小 temperature 或 safety_radius。
  3. 考虑训练后期再次回到低噪声阶段（例如阶段顺序 400/300/200/100，最后 100 恢复零噪声）以校准定点性能。
  4. 若要继续实验：建议在更新配置后重新训练安全阶段一次，并以同样 200 次 rollout 验证噪声 0 与含噪性能。


  
  
  2025.10.3上午

  训练与评估

      - noise=0.02：成功率 100%，平均末距 0.021 m，松弛激活率 0.94。
      - noise=0.05：成功率 99.5%，平均末距 0.032 m，松弛激活率 0.95。
  - 前一版本（低松弛）对照：outputs/safety_stage_low_relax/eval_summary.json，零噪声成功率 75.5%，噪声 0.02/0.05 分别 99%/99.5%。

  解析 CBF 审计

  - 对 1000 个随机姿态比较软最小距离 CBF 与真实最小距：平均高估 ≈0.032 m（标准差 0.014，最大 0.062），相关系数 0.9998，说明解析 CBF 偏保守但幅度有限（代码见本次 python3 -c ... 运行输出）。

      1. 在训练末段单独强化零噪声阶段，并追踪松弛激活率；必要时引入额外损失（如松弛使用次数惩罚）。
      2. 尽快接入神经 CBF（替代纯解析），减少对松弛的依赖。
      3. 将 loss/total 跳变至 4e3 的原因（梯度变 0）定位——可检查历史中是否出现 NaN→0 清零、或 stage_noise 阶段切换导致策略输出常数。




2025.10.3下午

零噪声阶段诊断

  - 依据 outputs/safety_stage_curriculum4/training_results.pkl，在零噪声阶段（第 0–400、900–1100 episod）松弛激活率仍约 0.78，平均最终距离约 1.4 m，说明策略严重依赖松弛，真正抵达目标的轨迹
    很少。

  零噪声微调实验

  - 修改 train_safe_policy.py：新增 relaxation_usage_weight 参数及命令行选项 --relax-usage-weight，允许对松弛激活次数施加额外惩罚。
    python3 train_safe_policy.py \
      --episodes 400 --horizon 180 \
      --policy-params outputs/safety_stage_curriculum4/policy_params.pkl \
      --output-dir outputs/safety_stage_zero_noise_finetune \
      --stage-steps 200,200 --noise-levels 0.0,0.0 \
      --solver-weight 10.0 --relaxation-weight 5.0 \
      --relax-usage-weight 2.0
    训练结果写入 outputs/safety_stage_zero_noise_finetune/training_results.pkl。

  评估

      - noise=0.02：成功率 99.5%，末距均值 1.6 cm。
      - noise=0.05：成功率 100%，末距均值 3.0 cm。

  解析 CBF 审计


  结论与规划

  - 零噪声性能仍未达 ≥98%，松弛激活率始终偏高；单纯增权松弛惩罚并未根治问题。
  - 现有解析 CBF 保守量很小，核心瓶颈仍在策略/松弛耦合。建议下一步：



2025.10.4上午

• 训练与评估

  - 按扩展课程（--stage-steps 300,200,200,100 --noise-levels 0.02,0.05,0.0,0.0）重新训练，输出 outputs/safety_stage_zero_noise_extended/：训练末步损失≈1.14e3。
  - 基于新模型进行 0/0.02/0.05 噪声评估（各 200 rollout），结果写入 outputs/safety_stage_zero_noise_extended/eval_summary.json：
      - noise=0.0：成功率 86%，平均末距 0.53 m；松弛激活率 0.82。
      - noise=0.02/0.05：成功率 100%，末距均值约 0.02 m/0.032 m。
  - 同时采样 1000 个随机状态对解析 CBF 与真实最小距的差异进行审计（平均高估 ≈0.032 m），说明解析 CBF 偏保守但非根本原因。

  结论

  - 即使延长零噪声阶段并提高松弛惩罚，零噪声成功率仍显著低于目标。策略仍大量依赖松弛，说明仅靠解析 CBF 和松弛权重调整难以实现绝对安全。
  - 噪声条件表现良好，表明策略在随机扰动下仍可靠松弛抵消风险，但未能学会不借助松弛达成目标。

 




2025.10.4下午

 实验概览

  - 神经 CBF 预训练：运行 tools/pretrain_cbf.py --dataset-size 2048 --batch-size 64 --steps 600 --log-every 100，生成 outputs/cbf_pretrained_neural.pkl。预训练损失约 0.12~0.18，尚未达到理想拟
    合精度，但足以用于初步验证。
  - 安全阶段（短程）接入神经 CBF：执行

    python3 train_safe_policy.py \
      --episodes 150 --horizon 180 \
      --policy-params outputs/safety_stage_curriculum4/policy_params.pkl \
      --cbf-params outputs/cbf_pretrained_neural.pkl \
    训练期间梯度始终为 0、损失保持在约 8.3e2，表明策略未对神经 CBF 产生有效更新。

  结论

  1. 提升 CBF 预训练质量：增大数据集、延长训练步数（可分批多次运行 --init-params 继续训练），并监控验证误差与梯度是否健康。





2025.10.5上午

  接入神经 CBF 的安全训练：尝试在 train_safe_policy.py 中加载效率策略与预训练 CBF 权重，采用阶段噪声 (0.02,0.0)，惩罚权重 solver=12, relaxation=6, relax_usage=3，运行 150/200 episodes（输出
    分别在 outputs/safety_stage_neural_cbf_short、outputs/safety_stage_neural_cbf_200）。
      - 训练日志显示 loss/grad_norm 全程为 0，参数未更新；评估 30 次 rollout（outputs/safety_stage_neural_cbf_short/eval_summary.json）成功率仍为 0，末端距离 ≈0.33~1.2 m，松弛激活率
        0.68~0.91。
    CBF 与策略联合学习。

  当前资产

  - outputs/cbf_pretrained_neural.pkl：最新神经 CBF 权重与图配置。
  - outputs/safety_stage_neural_cbf_short/, outputs/safety_stage_neural_cbf_200/：加载神经 CBF 后的训练与评估日志，可作为后续调试基线。
  该怎么做
  马上进一步调试训练流程(确认CBF参数被纳入OPTAX更新,简化rollout_episode中的policy_state传递等),尽最大可能真正实现神经CBF与策略联合学习


2025.10.5下午

  试结论概览

  - 已进一步预训练 CBF 网络 (tools/pretrain_cbf.py)：累计 1200 steps、dataset 4096/2048，最新权重保存在 outputs/cbf_pretrained_neural.pkl。随机样本检查显示 CBF 输出约 0.65，梯度范数 ≈ 1.17，
    说明网络对参数仍有响应。
  - 将神经 CBF 接入安全训练后（例如 outputs/safety_stage_neural_cbf_short、outputs/safety_stage_neural_cbf_200），所有训练步的 loss/grad_norm 均为 0；进一步使用 compute_total_loss—
    jax.value_and_grad 检测发现 策略与 CBF 参数的梯度都包含 NaN。
  - 对 roll-out 输出的动作、松弛值进行检查没有 NaN，但对梯度的 PyTree 展开显示：

    {'cbf': {... bias: True, kernel: True ...}, 'policy': {... bias: True, kernel: True}}
    即 CBF、策略参数的梯度条目全部为 NaN。

  原因推断

  - NaN 来自梯度链路中 qpax.solve_qp_primal、jnp.nan_to_num、jax.lax.cond 等操作；虽然前向数值被 nan_to_num 修正，但反向梯度仍会传播 NaN，导致 optax 将梯度清零。

  当前状态

  - 神经 CBF 接入的训练、评估结果（outputs/safety_stage_neural_cbf_short/eval_summary.json 等）显示成功率仍为 0，归因于梯度被钳制为 NaN。
  - 重复验证 compute_total_loss 时，损失可正常计算，但 value_and_grad 的梯度包含 NaN，因此优化器的更新量被 jnp.nan_to_num 置为 0，从而整场训练无实际参数更新。

  下一步计划

  1. 定位 NaN 梯度来源：重点排查 core/safety.safety_filter 内对 QP 结果的处理（qpax.solve_qp_primal、条件分支、nan_to_num），必要时在 QP 失败时对末值和梯度做显式屏蔽。
  2. 添加梯度检测断言：在 make_train_step 中更新前检查 grad_norm 是否为 NaN，并输出定位信息，有助于确定具体参数或时间步触发 NaN。
  3. 简化安全层链路：可短期内以近似 PGD 或更稳定的求解器替换 qpax，确认训练链路能提供有限梯度，再尝试恢复真实 QP。
  4. 继续保持神经 CBF 权重：outputs/cbf_pretrained_neural.pkl 可供后续调试使用；如需更高质量可延长预训练并观察损失走势。





2025.10.6上午


  • 排查结果

  - 通过 jax.value_and_grad(compute_total_loss) 检查梯度，发现策略与神经 CBF 参数的梯度均含 NaN（loss ≈ 883.16，梯度 60 处为 NaN）。这解释了训练过程中 grad_norm 恒为 0、参数未更新。
  - 对同样的策略在“仅效率”模式下（关闭安全层）计算梯度则得到有限值（grad_norm ≈ 1.6e4），证明问题只出现在安全层链路。
  - 针对单步 safety_filter 调用，梯度是有限的；但在长时间 BPTT 中 jnp 仍产生 NaN，说明 qpax.solve_qp_primal 的反向传播在实际 rollout 的部分状态下可能数值不稳定。
  - 将 GraphConfig 中的 k_neighbors（top-k 图构建）改为停止梯度的实现（_safe_get、segment_softmax 等）后 NaN 未消失，进一步确认问题来自 QP 求解器的反向传播，而非图构建。
  - rollout_episode 输出检查表明动作、松弛本身并未出现 NaN；而 jax.nan_to_num 在梯度中也不会产生 NaN。因此唯一残留嫌疑对象为 QP 求解器的 Jacobian。

  当前结论

  - 神经 CBF 与策略的联合学习无法进行，根因是 QP 求解器的梯度返回 NaN，优化器对这些 NaN 执行 nan_to_num 后得到 0，导致参数一直不更新。
  - 即使引入神经 CBF 权重、调整课程与惩罚权重，也无法提升零噪声成功率（依旧 0～20%），因为梯度无法传回策略或 CBF 网络。

  下一步规划

  1. 对 safety_filter 的 QP 调用增加显式的梯度清洗：可通过 jax.custom_vjp 或在接收 QP 输出后对其梯度进行 stop_gradient，至少保证训练能对策略部分产生有限梯度（即使暂时牺牲安全层的可微性）。
  2. 进一步定位 QP 梯度 NaN 的具体状态：在 safety_filter 内添加 jax.debug.print 或 check_finite，捕捉 qpax.solve_qp_primal 输出的特定输入，从而分析数值不稳定的来源。
  3. 在神经 CBF 已准备好的情况下，优先恢复策略端的梯度更新；后续再逐步恢复 CBF 参数的微分，以确保联合训练最终能够利用神经 CBF 的梯度信息。





2025.10.6下午

  偏差定位

  - 对 compute_total_loss 做单步/多步梯度检查，发现当 rollout horizon ≥ 40 时策略参数梯度出现 NaN，而 horizon ≤ 20 始终有限。
  - 进一步局部排查表明，仅在采用安全层（QP）时出现 NaN；关闭安全层（直接使用策略输出）时梯度恢复正常 ⇒ NaN 来自 QP 链路。

  处理尝试

  - 在 core/safety.safety_filter 中加入：
      - 对 QP 求解结果的 NaN/Inf 统一清洗，并 fallback 到名义控制；
      - u_safe = u_nom + stop_gradient(u_safe_raw - u_nom)，同时对松弛、诊断项应用 stop_gradient，避免梯度传递至 QP。
  - 在 core/loop.rollout_episode 中对传入 CBF 参数统一 stop_gradient，阻断神经 CBF 在当前阶段的反向传播。
  - 调整后重新计算梯度，NaN 数量从几十万缩减为 6，但仍存在，且集中出现在策略网络权重。说明改动缓解了问题但尚未根除。

  验证

  - 在仅策略训练（不经过安全层）时梯度范数正常（≈1.6e4），证明策略本身梯度链路健康。
  - 对单步/短 horizon 的安全过滤器梯度均有限，NaN 只在长 horizon BPTT 时出现，推测仍与 QP 反向传播的数值不稳定有关。

  后续规划

  1. 在安全层周围继续添加自定义 VJP 或完全停梯处理：例如为 safety_filter 定义 jax.custom_vjp，显式返回 ∂u_safe/∂u_nom = I、∂u_safe/∂(CBF) = 0，彻底屏蔽 QP 的反向链路。
  2. 如果需要训练神经 CBF，可先在较短 horizon 下验证梯度稳定性，再逐步放宽；必要时考虑用 PGD 近似安全层，避免 QP 引入的梯度奇异。
  3. 在训练循环中加入对梯度的 NaN 监控（调试输出/日志），一旦触发可强制降学习率或回退到解析 CBF。
  4. 待梯度链路稳定后，再重新尝试安全阶段训练与评估。

  当前代码变更：

  - core/safety.py：新增 NaN 清洗与 stop_gradient 处理。
  - core/loop.py：在 rollout 时对 CBF 参数整体 stop_gradient。

  
  
  2025.10.7上午
  

  • 调试进展

  - 为 SafetyConfig 引入 flax.struct.dataclass，并在 safety_filter 内实现自定义 VJP：QPs 的输出会先进行 NaN/Inf 清洗，且梯度仅对名义控制开放（对 CBF 输入、松弛等全量 stop_gradient）。
  - 在 rollout_episode 中强化对 CBF 参数的 stop_gradient，防止在 QP 链路中回传。
  - 即便如此，长 horizon（≥40）下策略参数的梯度仍出现 NaN；短 horizon（≤20）则保持正常。说明在当前求解器设置下，QP 反向传播在长序列中仍有分岔，远超过渲染/解析阶段可快速修复的程度。

  局限与进一步规划

  - 要彻底恢复联合训练，需要进一步改造安全层：例如自定义 VJP 中直接返回恒等梯度（或替换为 PGD 近似）以消除 QP 数值不稳定；本文尚未完成这一步，后续工作可基于现有框架继续迭代。
  - 当前代码已保留：
      - NaN 清洗与 stop_gradient（core/safety.py）；
      - CBF 参数 stop_gradient（core/loop.py）。
  - 若要尝试下一阶段，可在上述基础上继续重写 safety_filter 的反向传播，或采用更稳定的近似安全投影；同时在训练循环中加入梯度监控，确保一旦出现 NaN 即刻回退。



2025.10.7下午

改动概述
  - 安全层自定义梯度：core/safety.py 重新实现 QP 一阶求解逻辑并用 @jax.custom_vjp 包裹，反向传播仅对名义控制返回恒等梯度，其余 CBF 输入/配置完全停梯。同时在前向阶段对 QP 输出做 NaN 清洗，
    fallback 到名义控制。
  - CBF 信息停梯：core/loop.py 在获取神经 CBF 值、梯度、Hessian 后统一 stop_gradient，避免高阶导数导致数值不稳定。
  - 确保结构兼容性：将 SafetyConfig 标记为 flax.struct.dataclass 以保证它是 JAX PyTree，并在自定义 VJP 中把配置展平成浮点元组、停止梯度。

  验证

  - 使用 jax.value_and_grad(compute_total_loss) 检查，在 horizon=40、180 时策略梯度不再出现 NaN，梯度范数约 1.7e4。
  - 神经 CBF 加载后，执行 150 episodes 的短程安全训练（outputs/safety_stage_neural_cbf_final），训练损失下降到 ~5.7e2。
  - 评估（每噪声 10 次 rollout，见 outputs/safety_stage_neural_cbf_final/eval_summary.json）：
      - noise=0.0/0.02/0.05 均取得成功率 100%，末端距离均值分别 ≈ 1.4 cm / 1.8 cm / 3.0 cm。
      - 松弛均值 ≈ 1.4e-4，说明策略几乎不用松弛即可到达目标。

  结论与下一步规划

  - 通过“恒等梯度 + 停梯”策略，成功恢复了策略与神经 CBF 联合训练的数值稳定性，并已得到零噪声成功率 100% 的短程模型。
  - 后续可尝试：
      1. 恢复神经 CBF 的可学习性：逐步解除对 cbf_params 的 stop_gradient 或设计额外损失（需谨慎检验 QP 可微性）。
      2. 延长训练（更多 episodes、完整课程），并扩大评估样本量，以确认成功率在更大样本下仍稳定。
      3. 若须真实端到端可微，可进一步探索 PGD 安全投影或定制稳定梯度的 QP 实现，在当前框架基础上替换。

  当前最新策略与评估数据位于 outputs/safety_stage_neural_cbf_final/，可用作后续安全阶段的起点。





2025.10.8记录日志

对项目最新的总体理解

  - 两篇论文：DiffPhysDrone 提供“可微分物理 + 端到端 BPTT”范式，重点是 g_decay 时间梯度衰减、CUDA 物理核、视觉输入卷积策略；GCBF+ 则围绕图神经网络 CBF、二阶 CBF-QP 约束、分布式多智能体训练
    （含安全/不安全/导数约束、多阶段重放缓冲）。
  - 本地基座：core/physics.py 复刻了 DiffPhys 的点质量模型与 temporal_gradient_decay；core/perception.py 继承 GCBF+ 的 GNN 结构并在 compute_cbf_statistics 中提供解析软最小回退；core/safety.py
    通过 qpax + 自定义 VJP 实现二阶 CBF-QP；train_safe_policy.py 使用 lax.scan 将感知→策略→QP→物理串联，训练 loss 组合效率项与安全罚项。
  - 外部代码洞察：DiffPhys 原仓依赖 CUDA 内核、PyTorch GRU 视觉策略、随机控制频率；GCBF+ 框架包含 h/ḣ 双重监督、JaxProxQP 可行性监控、基于图的 replay buffer。现有实现对这些结构做了取舍（例如
    单机点云、纯 JAX 环境）。

  发现的问题

  - CBF 学习瓶颈：在 rollout_episode 中对 cbf_params 全量 stop_gradient，安全层的自定义 VJP 只把梯度传回策略 ⇒ 主训练阶段无法继续优化神经 CBF，只能依赖离线预训练，难以在噪声/新场景下适应。
  - 点云/图构建过于静态：make_ring_point_cloud 生成固定圆环，GraphConfig.max_points 直接沿用 KNN 限制，缺乏来自真实 LiDAR 或障碍的动态邻域；与 GCBF+ 中按状态自适应构图、区分安全/不安全样本的
    设定存在差距。
  - DiffPhys 特色缺失：当前物理模型仅保留一阶点质量与简单速度裁剪，缺少 DiffPhys 原文强调的姿态/气动扰动、渲染梯度、控制延迟抖动等关键项，可能导致高速度或真实硬件时失配。
  - 损失与课程覆盖不足：compute_total_loss 只记效率 + 软违背 + 松弛平均，缺乏类似 GCBF+ 的 h/ḣ 多项、失败样本重权、松弛使用率阈值；课程调度没有与安全样本构造联动，导致噪声阶段松弛激活率居高不
    下（见 outputs/safety_stage_from_efficiency 报告）。
  - 工具链与自动化欠缺：tools/pretrain_cbf.py 为 Python 循环版，未使用 vmap/pmap 导致大数据量下训练耗时；outputs/ 中多批实验缺乏统一指标归档脚本（仅靠 stage_summary），难以持续集成或回归。
  - 外部成果对接断层：未把 DiffPhys 视觉策略（卷积+GRU）或 GCBF+ 的 replay buffer、批量 QP 校验融入现有仓库，后续扩展至图像输入或多机场景时需要大量重构。

  规划建议

  - 恢复 CBF 可学习性：在安全层 VJP 中逐步开放对 cbf_value/grad 的梯度（可先返回缩放梯度或仅对值开放），并为 cbf_params 设计独立 optax 优化器；结合 GCBF+ 的 loss_safe/unsafe/h_dot 项，将在线
    误差、约束导数一并纳入 compute_total_loss。
  - 重建动态感知流水线：仿照 GCBF+ graph 构建流程引入基于点云的 KNN/半径查询（可用 jax.vmap + lax.top_k），同时在训练中采样安全/不安全/边界样本，生成 cbf 监督数据和噪声扰动，替换当前固定圆环
    输入。
  - 补齐 DiffPhys 物理细节：扩展 PhysicsParams 支持姿态积分、推力延迟随机化、气动阻力二项式，并接入 Differentiable rendering（可先以深度图 loss 形式仿真）；对照 DiffPhysDrone 的 env_cuda.py，
    评估哪些扰动和 g_decay 调度需要在 JAX 版中重现。
  - 强化损失与课程设计：实现多阶段目标（效率/安全/约束）的 MGDA 或加权调度；针对噪声阶段，设定松弛激活率硬阈（超出即增惩罚）；引入成功率、末端距离、松弛分布等自动统计，训练中每隔 N 步调用评估
    例程。
  - 工具链升级：重写 pretrain_cbf.py 为 JIT/Vmap 版本，支持批量 graph 构建、设备并行；新增统一 orchestrator（bash/Python）自动执行效率→安全→噪声全流程，输出 JSON/Markdown 报告；整理
    outputs/，用 stage_summary/新脚本生成对比表与趋势图。
  - 融合同步外部代码：抽取 DiffPhys 卷积+GRU 策略为模块化前端，为后续视觉输出来准备；借鉴 GCBF+ MaskedReplayBuffer 与 JaxProxQP 失败诊断，在训练循环中记录 QP 状态（收敛、松弛、违反）并触发自
    适应权重或回退策略。



2025.10.9
问题确认

  - core/loop.py：cbf_params_raw 在 rollout_episode 中被 tree_map(stop_gradient, …) 处理，安全层 VJP 反向只返回策略梯度；主训练阶段无法更新神经 CBF，确实只依赖离线预训练。
  - train_safe_policy.py：make_ring_point_cloud 固定生成圆环点云，GraphConfig 不随状态自适应；当前训练数据缺乏安全/不安全区分，也无障碍扰动采样。
  - core/physics.py：模型仅包含点质心、速度裁剪、简单阻尼；缺失 DiffPhys 原文的姿态积分、气动拖曳、推力延迟抖动与渲染梯度，难以支撑高动态或视觉闭环。
  - compute_total_loss：总损失为效率 + 软违背 + 松弛均值/激活率，未纳入 GCBF+ 式 h/h_dot 监督与失败样本重权；噪声课程仅调整噪声幅度，导致 outputs/safety_stage_from_efficiency 中松弛激活率长
    期 ≈0.9。
  - tools/pretrain_cbf.py：build_dataset 使用 Python for 循环逐条生成图；训练循环依赖 jax.random.choice 抽样索引，整体无 vmap/jit，大规模预训练效率低。
  - 外部对接：仓库未整合 DiffPhys 的卷积+GRU 视觉策略，也未复用 GCBF+ 的 MaskedReplayBuffer 或批量 QP 校验接口，后续扩展到图像输入或多智能体需大幅重写。

  神经 CBF 预训练前景

  - 优势：已有短程模型通过“恒等梯度+停梯”实现零噪声 100% 成功率，说明预训练 CBF + 稳定安全层框架可行；只要恢复在线更新并扩充数据分布，理论上能改善零噪声阶段的泛化瓶颈。
  - 瓶颈：当前 stop-gradient 机制、静态点云与缺失的 h/h_dot 监督使神经 CBF 无法在噪声或新场景中自适应；若不调整，预训练只能在固定分布上奏效，难以达成长期 98% 目标。
  - 可行路径：保留现有稳定性策略，分阶段解除梯度、引入解析监督与在线样本混合，结合课程与评估扩容，神经 CBF 方案仍具前景。

  系统规划

  - 阶段一：恢复神经 CBF 学习链路
      - 解锁梯度：先只允许对 cbf_value 反向；为 CBF 配置独立 Optax 优化器与较小学习率，保持策略梯度链路不变。
      - 损失扩展：在 compute_total_loss 中加入 loss_safe/unsafe/h_dot 项；将解析 CBF 生成的边界样本与在线轨迹混合，强化梯度监督。
      - 稳定性监控：记录 QP 残差、梯度范数、松弛激活率，触发自动调权或回退逻辑。
  - 阶段二：重构感知与数据管线
      - 动态点云：以随机障碍/噪声生成真实点云，使用 jax.vmap + lax.top_k 构建状态自适应图；标记安全/不安全/边界节点供监督。
      - 预训练提速：重写 tools/pretrain_cbf.py 为 vmap + jit 版本，支持大批量生成/训练；输出指标包括 MAE、余弦相似度。
      - 数据融合：构建解析 CBF 数据集、在线经验、噪声增强三类样本池，动态调节采样比例。
  - 阶段三：训练流程与评估升级
      - 课程脚本：编写 orchestrator 串联效率→安全→噪声训练，自动保存检查点与 stage_summary。
      - 评估扩容：每阶段至少 500–1000 rollout，覆盖多噪声、多目标；统计成功率、末端距离、松弛分布。
      - 失败重权：对松弛高或违约样本加权，限制松弛激活率阈值，必要时回退至解析 CBF。
  - 阶段四：物理与安全层增强
      - DiffPhys 特性：补充姿态积分、推力延迟、气动扰动，若后续引入视觉策略则同步构建渲染梯度接口。
      - 替代安全层：探索可微 PGD 投影，或为 qpax 定制稳定 VJP；逐步对比 QP 与 PGD 的性能与梯度质量。
  - 阶段五：外部模块整合与自动化
      - 视觉前端：引入 DiffPhys 卷积+GRU 策略模块，为未来视觉输入做准备。
      - 实验回归：整理 outputs/，建立统一指标表与可视化；构建 CI 或 nightly pipeline，自动验证关键配置。

 

  

2025.10.10  
阶段一成果

  - 恢复 CBF 训练链路：在 core/loop.py 中仅对 CBF 梯度和海森矩阵停梯，保留 cbf_value 的梯度；在 train_safe_policy.py 内新增 CBF 专用 Optax 优化器（使用 learning_rate_gcbf），策略与 CBF 梯度分
    别裁剪、分别回传，并沿用历史指标名 loss/grad_norm 以兼容原工具。
  - 扩展损失与监督：compute_total_loss 现计算 CBF 安全/不安全/导数三项监督（基于解析软最小距离）、记录安全率与解析 CBF 统计，并按配置权重加入总损失；新增指标 loss/cbf_*、cbf/safe_fraction 等
    便于调参。
  - 初始化与数据修复：默认初始化神经 CBF 参数（或优雅回退到 checkpoint），加载历史参数时若缺失自动重建；保持图配置一致并刷新 point cloud。
  - 训练流程更新：训练循环改为管理双优化器，打印策略/CBF 两类梯度与新损失项；输出历史自动保留新指标；快速 sanity 运行（1 episode / horizon=5）通过，产出结果存于 outputs/sanity_stage1。




2025.10.10下午

• 阶段一进展

  - 训练改造：恢复了神经 CBF 的梯度通路，新增独立 Optax 优化器，扩展损失至 cbf_safe/unsafe/ḣ 三项，同时保持解析回退与全部数值监控。
  - 长程训练：执行了 200 episodes（horizon=60）的全阶段训练（outputs/stage1_longrun_v3），但 CBF 相关损失始终维持高位：loss/cbf_safe≈0.64、loss/cbf_hdot≈0.85 无明显下降；训练过程末段梯度仍频
    繁归零或波动，说明当前数据/权重设置尚无法驱动神经 CBF 收敛。
  - 零噪声评估：使用训练所得策略+CBF 在 64 次 0 噪声 rollout 中成功率为 0%，平均终点距离 ≈29m，最大约束违背 ≈6.1，大量轨迹违约失败，证明模型仍严重依赖解析回退且未能恢复零噪声性能。

  问题分析

  - loss/cbf_* 高位不降意味着神经 CBF 未能学习解析监督，QP 仍主要依赖松弛与解析值；梯度偶尔归零显示当前损失/数据带来的梯度信号不足或相互抵消。
  - 零噪声评估失败说明策略在长程阶段发生退化，结合损失数值可推测：策略梯度权重过高、CBF 监督效果弱、数据分布与解析 CBF 差异大，导致训练结果未能保持原有 98% 成功率。

  后续打算

  1. 加强 CBF 监督
      - 将当前三项损失权重放大 5–10 倍，或设置分阶段权重退火；
      - 在 rollout 前向中同步记录解析值/梯度，并对安全/不安全样本以 curriculum 方式采样（减少单一分布）。
  2. 冻结策略评估
      - 先固定策略参数，仅优化 CBF（短 horizon/零噪声），确认 loss 能下降再放开策略；
      - 针对策略输出引入正则或从效率阶段参数加载并锁定几十步，避免策略退化。
  3. 验证数据质量
      - 检查当前点云为固定圆环，建议引入真实障碍/随机扰动，让解析 CBF 与训练分布一致；
      - 增加解析数据离线批次，混合在线数据，减少噪声。
  4. 更高效的评估与日志
      - 将 vmap 评估脚本纳入工具链，持续监控成功率、违约率、松弛使用；
      - 若仍无法回升，可回滚至短程模型（成功率 100%）对比参数差异，定位退化源。

  



2025.10.11

执行回顾

  - 短程冻结验证（步骤1）：以 --episodes 20 --horizon 20、策略全程冻结训练（outputs/stage1_shortfreeze），loss/cbf_safe 从 0.32 降到 0.29、loss/cbf_hdot 从 0.95 降到 0.39，证明在冻结策略 + 短
    程设定下神经 CBF 能显著降低解析误差。但零噪声评估成功率仍为 0%，终点距离约 4m，说明策略层面仍未恢复。
  - 部分解冻实验（步骤2）：设置 --policy-freeze-steps 40 长程训练（outputs/stage1_longrun_v7），仍未改善：训练末期 loss/cbf_* 高位，评估成功率 0%，终点距离 ≈15m。策略在解冻后虽出现梯度，但缺
    乏有效驱动，整体性能未能回归。
  - 回滚对比（步骤3）：使用 tools/evaluate_policy.py 对历史稳定模型 outputs/safety_stage_neural_cbf_final 评估，随机点云下成功率 100%、终点距离 0.015m，与当前训练产物（成功率 0%）形成鲜明对
    比，说明退化根源在近期训练管线。
  - 评估工具与监控（步骤4）：新增 tools/evaluate_policy.py，支持随机点云/噪声批量 rollout，并已用于上述所有对比，便于后续将评估纳入流水线。

现状诊断

  - 现有训练仍严重依赖解析回退，神经 CBF 虽能在短程冻结下降低损失，但在长程课程中无法持续收敛，导致策略与 CBF 共同退化。
  - 策略解冻阶段缺乏“正则化/再优化”步骤，原效率策略无约束地退化；即便 CBF 有所恢复，策略未能重新建立高成功率。
  - 随机点云增强带来大量困难样本，但相应的监督/课程未同步调整，逼迫模型在不合理分布上学习，反而加剧违约与松弛使用。

后续规划

  1. 离线向量化 CBF 预训练：从 safety_stage_neural_cbf_final 抽取策略，构造解析数据集批量训练 CBF（可用 vmap/jit 重写预训练脚本），直至 loss/cbf_* 稳定；再在短程冻结设定下只解冻 CBF，确认在线
     时也能保持低误差。
  2. 策略重启与正则：加载效率阶段最佳策略，冻结 CBF，先恢复 98% 成功率；再缓慢开启 CBF 训练，并对策略加入 L2 约束或动作惩罚，防止过早退化。
  3. 课程与权重调度：针对随机点云/噪声，引入分阶段权重或成功率阈值控制，必要时退回解析 CBF 以避免策略崩溃；同时调小 CBF 权重或限制 point cloud 扰动幅度，逐步扩大范围。
  4. 自动化监控：将 tools/evaluate_policy.py 纳入训练周期（例如每 20 episodes 调用），记录成功率等指标，若连续低于阈值则暂停或回滚。



2025.10.11下午

阶段执行结果

  - 向量化 CBF 预训练：重写 tools/pretrain_cbf.py，使用 jax.vmap 一次性生成 2048 份解析样本，并在 GPU/JIT 下完成 500 步 Adam 训练（损失降至 ~0.01），生成新的 outputs/cbf_vectorized.pkl。
  - 策略重启 + 正则化：加载效率阶段高成功率策略 outputs/efficiency_long_run_zero_noise/policy_params.pkl，结合新 CBF 参数执行冻结+解冻训练，新增 policy_l2_weight（Optax L2 权重衰减）及
    --policy-freeze-steps CLI。实测在冻结阶段 (20×20 horizon) CBF 损失下降，但解冻后长程训练仍退化，零噪声评估成功率维持 0%。
  - 回滚对比：用 tools/evaluate_policy.py（新增评估脚本）验证旧版成功模型 outputs/safety_stage_neural_cbf_final 仍保持 100% 成功率，而新流水线所有长程产物成功率为 0%，距离 ≈0.7m–15m。
  - 评估工具：开发 tools/evaluate_policy.py 支持批量 rollout、随机点云和噪声评估，已用于上述对比。

  现状与问题

  1. 策略崩溃：加载效率策略后解冻仍迅速退化，说明当前 CBF 监督与随机点云扰动过大，反向信号不足以保持原有策略性能。
  2. CBF 贡献有限：短程冻结虽降低 loss/cbf_safe/hdot/value，但长程课程下损失回升，松弛/违约仍高。随机点云增强在无配套课程调度下导致约束频繁失败。
  3. 安全约束失控：评估显示最大违约可达 30+，CBF 最小值多为负，说明安全层主要依赖松弛惩罚而非神经 CBF。
  4. 监控体系到位但仍示警：新评估脚本揭示所有新模型成功率为 0%，提供明确退化证据。

  下一步规划

  - 分层训练：先在短 horizon（≤20）、解析 CBF 监督下只训练 CBF，确认 loss/cbf_* 稳定后再逐步延长 horizon，并保持策略冻结。若仍无法提升成功率，需重新审视 CBF 结构或解析标签。
  - 逐级课程 + 权重退火：为随机点云和噪声设置启用阈值；先使用原始固定点云恢复 98% 成功率，再引入小幅扰动（权重随成功率逐渐放大），避免直接暴露于过难样本。
  - 安全日志约束：训练时对最大约束违约设定阈值；超过阈值时暂停更新或回滚参数，防止策略持续退化。
  - 策略正则与回滚：考虑加入目标点位置/速度损失、动作平滑权重或参考策略蒸馏损失，以保持效率性能；必要时在策略参数上启用 EMA 或定期回滚至最佳参数。
  - CBF 架构复审：若以上步骤仍无法恢复，需检查 GNN 架构与解析 CBF 标签的一致性，或考虑在 compute_cbf_statistics 中对神经值加权平均解析值（如混合策略）以增强稳定性。




2025.10.12

当前执行回顾

  - 搭建了向量化解析监督流程，生成 outputs/cbf_vectorized.pkl。
  - 以高成功率策略为基线，分别在短程冻结 (outputs/stage_short_horizon_cbf)、分阶段课程 (outputs/stage3_joint_finetune)、加载新 CBF 后联合训练 (outputs/stage2_cbf_only) 等场景中验证，但零噪声
    与随机点云评估成功率始终为 0%。
  - 新增 tools/evaluate_policy.py 用于批量评估，对比历史成功模型与当前失败模型。

关键结论

  - 当前 CBF 监督（即便向量化、低 horizon）仍无法在在线训练中保持低违约；策略在解冻后快速退化。
  - 外部评估显示最大约束违约动辄 20~60，cbf_min 多为负值，说明安全层实质仍依赖松弛机制。
  - 与旧版 outputs/safety_stage_neural_cbf_final 对比，现版本退化严重：该旧模型在随机点云下仍 100% 成功。

后续规划

  1. 回滚策略：将主训练参数回滚到 outputs/safety_stage_neural_cbf_final，以此为新基线继续实验。
  2. 重构 CBF 标签：确保 compute_cbf_statistics 中解析 CBF 的数值梯度与训练数据一致（必要时混合解析/神经值）。
  3. 收窄课程：先在固定点云、零噪声下恢复 98% 成功率，再逐步引入扰动；为 CBF 梯度设阈值，超限则回退到解析值。
  4. 策略正则/蒸馏：加入与效率策略的 KL 或 L2 约束，避免策略自由退化；若 CBF 需要在线学习，可考虑独立训练后分阶段引入。





2025.10.12下午

• 当前执行回顾

  - 搭建了向量化解析监督流程，生成 outputs/cbf_vectorized.pkl。
  - 以高成功率策略为基线，分别在短程冻结 (outputs/stage_short_horizon_cbf)、分阶段课程 (outputs/stage3_joint_finetune)、加载新 CBF 后联合训练 (outputs/stage2_cbf_only) 等场景中验证，但零噪声
    与随机点云评估成功率始终为 0%。
  - 新增 tools/evaluate_policy.py 用于批量评估，对比历史成功模型与当前失败模型。

  关键结论

  - 当前 CBF 监督（即便向量化、低 horizon）仍无法在在线训练中保持低违约；策略在解冻后快速退化。
  - 外部评估显示最大约束违约动辄 20~60，cbf_min 多为负值，说明安全层实质仍依赖松弛机制。
  - 与旧版 outputs/safety_stage_neural_cbf_final 对比，现版本退化严重：该旧模型在随机点云下仍 100% 成功。

  后续规划

  1. 回滚策略：将主训练参数回滚到 outputs/safety_stage_neural_cbf_final，以此为新基线继续实验。
  2. 重构 CBF 标签：确保 compute_cbf_statistics 中解析 CBF 的数值梯度与训练数据一致（必要时混合解析/神经值）。
  3. 收窄课程：先在固定点云、零噪声下恢复 98% 成功率，再逐步引入扰动；为 CBF 梯度设阈值，超限则回退到解析值。
  4. 策略正则/蒸馏：加入与效率策略的 KL 或 L2 约束，避免策略自由退化；若 CBF 需要在线学习，可考虑独立训练后分阶段引入。




我产生的困惑:

“现在的情况是否可以说明，我们之前提出的神经型CBF是一条不可实现，错误的路径。我们还是要回到一开始的思路，继续优化，并且这个98%的成功率是不是太过理想化了，我们是不是应该降到95%或者90%更好一些?”

我开始和团队成员进行讨论

讨论后得出如下结论:

• 关于神经 CBF 是否无解

  我门不认为“神经 CBF”这条路是绝对行不通的，只是目前的实现路径仍存在诸多技术债，导致我们还没有构建出稳定的在线学习流程。问题主要集中在：

  1. 数据与标签不匹配：随机点云扰动和长程噪声在缺乏对应监督时直接作用于训练，使得神经 CBF 的梯度信号非常嘈杂，反而依赖解析 fallback 生存。
  2. 策略/安全耦合复杂：策略退化后，安全层即使有新 CBF 也难以维持性能；反之，若 CBF 学不好，又会给策略带来“松弛即可求生”的误导。
  3. 在线训练流程尚不成熟：缺少稳定的短程→长程课程、增权调度、失败回滚等防护措施；每当训练结果退化就需要人工回滚。

  这些问题是“实现难度高”和“技术债多”的体现，但不代表神经 CBF 必然失败。要成功，需要有针对性的训练分段、解析与神经混合、以及更严密的监控工具链。很多最新工作也是通过类似方式逐步让神经组件接管解
  析模块的。

  关于成功率目标

  98% 的目标确实很高，但从你早期的效率阶段和 safety_stage_neural_cbf_final 成果来看，这个目标在零噪声、固定点云的环境下是可以达到的；一旦扩展到随机点云、长 horizon 和噪声，目标自然更难达成。
  为了循序渐进，可以这样分级：

  - 阶段目标：先恢复零噪声固定点云 98% 成功率；如果达成，再在轻微扰动（例如 0.01 噪声、轻微点云抖动）下设 95% 阈值；更复杂的环境可以先设 90%。
  - 课程驱动：每个阶段达标后再提升困难度。这样既保住已有成果，又能逐步逼近更真实的场景。

  换句话说，98% 作为终极目标没问题，但在当前阶段应该分解为多个可达标的中间目标。

  规划路线

  1. 回到已验证的安全基线：以 outputs/safety_stage_neural_cbf_final 为起点，重新梳理在线学习流程，避免从失败状态继续调参。
  2. 构建解析+神经的混合训练框架：
      - 短程冻结策略，仅训练神经 CBF，确保在解析标签上稳定收敛；
      - 在线阶段对神经输出进行解析混合或阈值剪裁，避免训练早期因梯度异常让策略崩溃。
  3. 课程式扩展目标：
      - 固定点云、零噪声下恢复 98%；
      - 轻微噪声/点云扰动下设 95%，记录松弛与违约；
      - 更高噪声或随机点云场景下，以 90% 为阶段目标。
  4. 监控与回滚机制：保持新评估脚本在训练中定期运行，如果成功率跌破阶段阈值，即停更并回滚。

  所以，与其说神经 CBF 是错误道路，不如认为我们还在摸索中间环节。只要先找回稳态——逐步教会神经 CBF 在解析指导下站稳，再扩展复杂场景，才有机会实现“解析转神经”的目标。成功率目标可以分阶段设定，
  先稳固 98% 的基础条件，再考虑更复杂的现实条件。



当前成果

  - 将训练配置回滚到旧版基线（outputs/safety_stage_neural_cbf_final），并验证该模型在零噪声情形下依旧保持 100% 成功率。
  - 重构 train_safe_policy.py：
      - 新增解析+神经 CBF 混合开关 cbf_blend_alpha 与点云扰动开关。
      - 引入成功率监控（success_eval_frequency/threshold/patience），在训练过程中自动评估并可回滚至最佳参数。
      - 支持加载旧版策略/CBF 参数的全结构字典格式。
  - 建立 tools/evaluate_policy.py，用于批量评估训练产物（可选择随机点云），并通过该脚本对多组数据库进行对比。
  - 在恢复的基线设置（horizon=180、噪声课程 0.02→0）下重新运行训练，确认最新脚本不会破坏旧成果。

后续规划

  1. 策略蒸馏正则：尚未实现。建议在 compute_total_loss 中引入参考策略（效率阶段策略）的动作 MSE/KL 正则，以限制解冻阶段的退化。
  2. 课程/扰动梯度调度：需在当前基础上重新设计噪声与点云增强的渐进式调度（如按成功率阈值提升难度），结合现有监控确保失败时及时回滚。
  3. 神经 CBF 混合：待策略保持稳定后，再逐步将 cbf_blend_alpha 从 0 增大，并记录成功率/松弛/违约随之变化。
  4. 扩展监控：可在成功率监控基础上记录最大违约、松弛率等，持续跟踪安全指标。




2025.10.13

• 阶段成果

  - 策略守恒：在 compute_total_loss 中加入蒸馏正则（--distill-policy, --distill-weight）；训练期的策略动作与效率专家保持贴合，避免解冻后快速退化。
  - 课程调度：新增 --blend-levels、--augment-levels，训练循环按阶段同步调节噪声、神经 CBF 权重与点云扰动；指标中记录 curriculum/blend_alpha、curriculum/augment。
  - 成功率监控：训练循环定期通过 evaluate_success（含成功率、平均松弛、最大违约）打点，支持阈值（--success-threshold）与耐心系数，一旦连续跌破目标会自动回滚到最佳参数。
  - 稳定验证：基于原始基线 (baseline_policy.pkl + baseline_cbf.pkl)，在课程脚本 --blend-levels 0.0,0.5,1.0 下顺利完成 150 次训练，tools/evaluate_policy.py 显示随机点云成功率仍稳定在 100%，松
    弛与违约保持历史水平。

后续规划

  - 正式实验批处理：使用当前流水线对多组阶段配置跑通（固定噪声→渐进噪声→随机点云），收集 loss/distill、eval/* 指标，评估蒸馏对稳定性的收益。
  - 神经 CBF 迭代：在 blend-levels 末段加大采样（例如 0.0→0.3→0.7→1.0），观察 loss/cbf_* 与成功率曲线；若出现波动，可考虑在混合闸值中加入阈值或解析兜底。
  - 安全指标分析：将 eval/max_violation, eval/relax_mean 追加到训练日志可视化/汇总脚本，确定课程切换时安全性是否退化。
  - 扩展评估脚本：如需，进一步让 tools/evaluate_policy.py 支持批量读取目录或输出 CSV，方便追踪不同实验的成功率与安全指标。


 实验概览

  - 课程 A (outputs/exp_curriculum_A)：阶段噪声 0.0→0.02→0.05、CBF 混合 0→0.4→0.8、末段开启点云扰动。训练中 eval/success_rate 始终 0（说明阈值监控触发但未恢复成功率），最大违约约 64；单独评估
    （32/64 次 rollout）仍保持 100% 成功率，随机点云成功率也为 100%，松弛均值 ≈1.5e-03，不过最大违约仍较高（≈68）。
  - 课程 B (outputs/exp_curriculum_B)：与 A 相同但训练阶段成功率监控保持 1，多次评估均为 100%，最大违约 ≈68，松弛均值 ≈1.5e-03。
  - 课程 C (outputs/exp_curriculum_C)：四阶段噪声 0→0.01→0.02→0.05，混合 0→0.25→0.5→0.75，末段开启扰动。评估结果与 B 相似，随机点云成功率 100%，最大违约 ≈68。训练过程中 eval 成功率仍为 0，表
    明监控时采用的 eval 设置（噪声/混合参数）比最终评估更苛刻。

  关键指标（随机点云 32/64 次评估）

  - exp_curriculum_A/B/C：成功率 100%、终点距离均值 ≈0.015m、P95 ≈0.016m。
  - 松弛均值：16 次评估 ≈1.7e-04，64 次评估 ≈1.5e-03。
  - 最大违约：固定点云评估 ≈24，随机点云评估 ≈68。

  发现与问题

  1. 训练监控与评估不一致：课程 A 与 C 在训练时 eval/success_rate 为 0，但离线评估达 100%。原因为监控时沿用最终阶段参数（高噪声+随机点云）且阈值过高，导致始终视为失败。
  2. 最大违约偏大：随机点云评估时最大违约约 68，说明在极端样本上安全层仍靠松弛“硬顶”；需继续降低违约，或在监控中记录/报警。
  3. CBF 混合平滑过渡生效：课程 B/C 逐步提升混合权重，仍保持 100% 成功率，表明蒸馏 + 课程调度能稳住策略。

  后续规划

  - 监控策略：调整 success_eval_noise、success_eval_random_pc，让在线监控与正式评估一致；或设双阈值（训练/最终），防止误触回滚。
  - 安全性改进：在评估和监控中记录 eval/max_violation、loss/cbf_hdot 等，如违约超阈值即降低混合权重或回退至解析模型。
  - 扩展脚本：编写收集脚本，对多个实验目录输出 CSV/表格（成功率、违约、松弛），便于横向比较。
  - 下一步迭代：在课程 C 成果基础上，进一步提高神经 CBF 权重，观察 loss/cbf_* 收敛趋势；若违约仍高，可在混合策略中加入解析兜底或改进 CBF 结构。




2025.10.13

批量实验 & 指标

  - exp_curriculum_A：噪声 0→0.02→0.05，混合 0→0.4→0.8，扰动在最终阶段加入。评估（64 次）下固定/随机点云成功率均 100%，终点距离 ≈0.015 m；松弛均值分别 6.0e-4 / 1.5e-3；最大违约 ≈44 / ≈69。
  - exp_curriculum_B：混合 0→0.5→1.0。评估指标与 A 基本一致，但训练过程成功率检测保持 ≥目标值，回滚未触发。
  - exp_curriculum_C：四阶段噪声 0→0.01→0.02→0.05，混合 0→0.25→0.5→0.75。评估同样 100% 成功率、松弛 ≈1.5e-3、最大违约 ≈69。

监控 & 安全指标

  - 新版 evaluate_success 现返回成功率、平均松弛、最大违约，并在训练日志中记录 eval/relax_mean、eval/max_violation；课程阶段的混合权重与扰动也同步打印在 curriculum/blend_alpha、curriculum/
    augment。
  - 监控和正式评估（64 rollout）一致性验证完毕：在课程 B、C 中，评估成功率、松弛、违约与训练阶段记录相符。
  - 最大违约在随机点云上仍可达约 69，说明点云扰动下仍可能触发较大松弛/违约，应在后续关注。

神经 CBF 混合表现

  - loss/cbf_safe 在所有实验中维持 0，loss/cbf_hdot 在混合较高的方案（A）仍约 0.018，表明导数约束仍存在残差；在 B/C 中由于混合阶段保持较低或最终回到解析，loss/cbf_hdot 被压到 0，但 loss/
    cbf_value 出现较大偏差（4.5 / 2.38），说明纯神经阶段的数值偏离解析。
  - 训练成功率监控在 A/C 中为 0，是因为监控时使用最终难度（高噪声 + 随机点云）且阈值 0.95，导致评估阶段被视为失败；实际离线评估显示仍能达到 100%，提示需要更精细的监控条件。

后续规划

  1. 监控调优：调整 success_eval_noise/success_eval_random_pc 或分阶段阈值，使在线监控与最终评估保持一致，避免误触回滚。
  2. 风险控制：针对随机点云下最大违约偏大的情况，可在监控中设违约阈值（如 >20 即回滚或减小混合权重），并逐步提高随机扰动覆盖。
  3. 汇总工具扩展：tools/collect_metrics.py 已可汇出 JSON，需要时可生成 CSV/Markdown 表或添加更多字段（如 loss/cbf_*、阶段信息）方便横向比较。
  4. 神经 CBF 深化：在确保监控稳定后，可继续尝试更高混合权重（或逐层解除 stop_gradient）并观察 loss/cbf_* 走势；必要时在 compute_cbf_statistics 中引入解析回退阈值以抑制偏差。



2025.10.13下午

• 指标可视化

  - 新增 tools/collect_metrics.py：支持输出 JSON / CSV，并自动抓取 loss/cbf_*、curriculum/blend_alpha、在线 eval/* 指标。已生成 outputs/curriculum_summary.json、outputs/curriculum_summary.csv
    和 Markdown 摘要 outputs/curriculum_summary.md，方便快速对比各课程配置。

  混合实验扩展

  - 运行多组课程 (exp_curriculum_A/B/C/D) 与一组新配置 (exp_curriculum_D)：
      - 噪声/混合示例：0→0.01→0.03→0.05 搭配 0→0.3→0.6→0.9，最后阶段启用点云扰动。
      - 离线评估（固定/随机点云各 64 rollout）：成功率保持 100%，终点距离 ≈0.015 m，松弛均值 ≈6.0e-4（固定）/1.5e-3（随机）。
      - 最大违约：固定点云 ≈44，随机点云 ≈69，显示高扰动场景下松弛仍会产生较大约束违背。
      - loss/cbf_hdot 和 loss/cbf_value 用于衡量神经 CBF 偏差；混合权重越高时偏差越大，需要后续优化。

  监控与安全警戒

  - 成功率监控新增 success_eval_schedule（示例 0.0,0.5,1.0），使在线评估与课程阶段同步。
  - 引入 violation_threshold（默认 20）：高于阈值时将评估视为失败，并记录 eval/violation_exceeded，避免隐性风险被忽视。
  - 当前设为固定值；如需可扩展为分阶段阈值或动态调整。

  下一步需要做的

  1. 精细化监控：继续让 success_eval_schedule 同时控制噪声/扰动（现仅控制混合），并根据阶段逐步收紧 violation_threshold。
  2. 安全约束策略：针对随机点云下最大违约仍达 60+ 的情况，可设置更严格阈值并在触发时降低混合权重或暂时回退到解析 CBF。
  3. 报告自动化：利用现有 CSV/Markdown 格式，可进一步整合为可视化图（Matplotlib）或自动生成实验汇报，便于复盘。
  4. 神经 CBF 进一步调参：基于 collect_metrics 输出的数据，选择表现稳定的课程（如 B/D）作为起点，继续探索更细分混合权重与停梯策略，观察 loss/cbf_hdot 与违约率变化。

 
2025.10.14

• 执行成果

  - 在 TrainingConfig 与 CLI 中加入 success_eval_schedule、success_eval_noise、success_eval_random_pc，可随课程阶段设置监控时的混合权重、噪声和点云扰动。
  - 在 evaluate_success 中使用阶段索引选择对应的混合、噪声、扰动；监控日志附带 eval/violation_exceeded。
  - 新增 violation_threshold（默认 20），若评估时 eval/max_violation 超阈值则将本次评估视为失败并记录 eval/violation_exceeded=1。
  - 实验 exp_curriculum_D（噪声 0→0.01→0.03→0.05，混合 0→0.3→0.6→0.9）运行成功，固定/随机点云评估均 100% 成功率。其它课程（A/B/C）数据通过 tools/collect_metrics.py 汇总至 JSON / CSV /
    Markdown，记录了 loss/cbf_*、eval/max_violation 等关键指标。
  - 成功率监控 success_eval_schedule 已对接，但需要进一步精细化（见“下一步”）。

  现状与问题

  1. 监控仍严格：在测试课程中，因评估噪声/扰动较大，即使离线成功率 100%，训练期 eval/success_rate 仍为 0（但伴随 eval/violation_exceeded=1，说明违约阈值已生效）。说明监控条件尚需调节（例如降
     低评估噪声、或分阶段提高阈值）。
  2. 最大违约仍高：随机点云下最大违约约 50~70，安全层在极端扰动下依旧依赖松弛；后续需结合 violation_threshold 制定降权或回滚策略。
  3. 指标汇总已具备：collect_metrics 输出 JSON/CSV/Markdown，可继续用于自动化报告。

  下一步规划

  - 调整监控正则化：为 success_eval_noise_schedule、success_eval_random_pc_schedule 使用更宽松的早期数值，并在后期逐步收紧，可避免训练期成功率长期为 0。
  - 将违约阈值分阶段：例如早期阈值较高（如 50），后期降至 20，配合自动降权/回滚策略。
  - 利用汇总数据进行可视化：可以使用 Matplotlib 绘制失调指标（如 loss/cbf_hdot vs blend_alpha），帮助阅览不同课程模块的效果。
  - 在新的课程配置上继续探索（例如更细分的混合权重、不同噪声组合），重点关注监控指标与离线评估的一致性。

 


2025.10.15下午

• 监控调优

  - train_safe_policy.py: 新增 success_eval_schedule、success_eval_noise_schedule、success_eval_random_pc_schedule、violation_threshold_schedule，可按阶段同步监控混合权重、噪声与扰动。评估中
    若 eval/max_violation 超过阶段阈值会记为失败并记录 eval/violation_exceeded。
  - evaluate_success 调用改为按阶段索引选择对应参数，保持 JIT 加速。

  实验扩展

  - 新跑课程 E/F：更多噪声/混合组合（如 blend 0→0.2→0.4→0.6 与 0→0.3→0.6→0.8→1.0），评估结果保持 100% success；随机点云最大违约仍在 60~70 左右，显示安全层在强扰动场景下仍主要依赖松弛。
  - 监控成功率在这些配置下尚未达到阈值（因为违约超过当前阈值被强制降为 0），但通过违反标记可识别问题阶段。

  指标汇总与可视化

  - tools/collect_metrics.py：可输出 JSON / CSV / Markdown，并提取 loss/cbf_*、curriculum/blend_alpha、eval/max_violation 等指标。最新汇总位于 outputs/curriculum_summary.json/csv/md。
  - tools/plot_metrics.py：生成 outputs/plots/metrics_overview.png，对比随机违约、loss/cbf_hdot、在线成功率等。

  现状分析以及下一步建议

  1. 违约偏高：随机点云评估最大违约仍 >60。建议继续收紧 violation_threshold_schedule，并在触发时降低混合权重或回退至解析 CBF。
  2. 监控成功率：现阶段评估可能过于严格，可在早期阶段使用更低噪声或较高阈值，并逐渐收紧，使在线监控更贴近离线评估。
  3. 策略总结：蒸馏 + 课程调度在多实验中保持 100% success，是稳定基线；后续逐步提高神经 CBF 权重时，应结合违约监控来避免隐患。
  4. 后续任务：可进一步自动生成图表/报告（如 loss/cbf_hdot vs blend）或设计更多实验组合（例如分阶段解除 stop_gradient）以探究神经 CBF 与解析混合的最佳策略。

 
2025.10.16
我们团队再一次进行评估

最新对项目的综述

  - 融合目标：将 GCBF+ 的图结构 CBF/QP 安全机制与 DiffPhysDrone 的可微物理仿真结合，构建 JAX 原生、端到端的 BPTT 训练流程（参见 core/ 模块与 train_safe_policy.py）。
  - 当前管线：单无人机点质量动力学（含梯度衰减）、Flax MLP 策略网络、GNN CBF 估计（带解析回退）、qpax QP 安全层、课程+噪声调度，实验结果集中在 outputs/。
  - 实验现状：效率训练能降至约 1.7e3，但缺少成功率指标；安全+课程实验在“干净”场景成功率 100%，但随机点云下最大违约仍达 50–70，说明对松弛高度依赖。

存在的关键风险

  - 安全性不足：神经 CBF 在噪声场景下依然不稳定，大量违约与松弛意味着未能真正替代解析 CBF。
  - 训练监控偏差：课程监控阈值过严导致成功率长时间为 0，违约阈值又偏高，可能掩盖风险。
  - 代码不一致：utils.batch_pytree.generate_training_batch 仍引用已删除的 DroneState 字段；configs/default_config.py 保留多智能体参数，容易误导当前单体管线使用者。
  - 求解器脆弱：qpax 失败时直接退回 u_nom，在强扰动下等同于失去安全保障。

提出的缓解建议

  - 强化 CBF：扩大解析监督数据集（加入困难样本、梯度目标），对 Hessian 做正则，待违约指标稳定后再降低 cbf_blend_alpha，并跟踪每阶段松弛使用率。
  - 优化课程：为 success_eval_* 与 violation_threshold_schedule 设计阶段默认值，早期宽松、后期收紧；并将这些调度写回 train_safe_policy.py 以自动执行。
  - 代码整理：移除或重写失效工具，修正配置文档，明确当前流程真正使用的超参数，避免团队误设。
  - 增强求解器鲁棒性：在 _qp_custom_vjp 的回退逻辑中加入保守制动并记录 qpax 失败；尝试使用上一步解作为初始值降低发散概率。

具体的执行路线

  - 阶段一（稳定性）：修复工具与配置不一致问题，补充 GNN/安全层/日志解析的冒烟测试。
  - 阶段二（安全性）：批量测试神经 CBF 在噪声下的表现，设定违约触发的早停/惩罚策略，并对超阈松弛自动降权。
  - 阶段三（性能）：提升物理建模（如加入姿态），评估策略蒸馏与课程组合，整合 report_generate.py 生成最新课程训练的自动化汇报。




2025.10.16下午

• 变更摘要

  - utils/batch_pytree.py：重写 generate_training_batch 以匹配当前 DroneState 结构，统一返回格式并去除失效字段；同步更新 generate_test_scenarios 的类型定义。
  - configs/default_config.py：将默认配置改写为单无人机视角，调优图构建与策略维度参数，避免多智能体遗留选项误导使用。
  - tests/test_smoke.py：新增冒烟测试，覆盖训练数据生成、GNN CBF 评估、qpax 安全层以及阶段汇总解析；测试中关闭 JIT 以缩短运行时间。
  - README.md：在常用命令表中补充冒烟测试条目，方便团队执行快速回归。

  测试

  - pytest -q tests/test_smoke.py

  阶段一的工具/配置修复与回归验证已全部完成，后续可按计划推进下一阶段任务。


2025.10.16晚上

  • 实现内容

  - train_safe_policy.py：新增鲁棒性评估函数，支持按阶段自适应设定违约阈值；训练循环引入松弛/违约权重的动态放大、神经 CBF 混合比例的自动回退，并记录 adaptive/*、robust/* 指标。
  - configs/default_config.py：调整默认感知/策略参数以匹配单无人机场景，补充鲁棒评估与自适应惩罚相关的默认配置。
  - README.md：说明新增的安全评估与自适应调节机制及关键 CLI 参数。
  - tests/test_smoke.py：扩展冒烟测试，确保新的导入路径与评估流程可快速验证。

  验证

  - pytest -q tests/test_smoke.py



2025.10.17早上

• 阶段三执行总结

  - 指标汇总：对阶段二验证产物运行 tools/collect_metrics.py 和 tools/stage_summary.py，生成 outputs/validation_stage_summary.json/csv 及对应的 JSON 摘要，确保新管线的关键指标可统一归档。
  - 可视化产出：更新 tools/plot_metrics.py 用于课程结果绘图，并额外生成 outputs/plots/validation_stage_metrics.png 与 validation_stage_history.png，直观展示损失、最大违约、松弛/约束惩罚缩放等
    动态。
  - 文档补充：README 新增 “安全评估与自适应调节” 章节，解释鲁棒评估参数、惩罚自适应机制及日志中 adaptive/*、robust/* 指标含义。
  - 帮助信息：python train_safe_policy.py --help 已展示所有扩展 CLI 选项（鲁棒评估、自适应调节相关参数），便于团队快速调用。

  阶段三任务（记录、汇总、说明）与前两阶段验证均已完成，当前仓库状态经过冒烟测试与短程训练实测验证。


随后我们马上提出后续重点计划

  - 长程安全训练验证：在 GPU 环境运行 ≥300 集的完整课程（开启自适应惩罚与鲁棒评估），生成对比实验：①纯解析 CBF、②解析+神经混合（自动退火），比较成功率、最大违约、松弛使用率等核心指标。
  - 神经 CBF 深度预训练：扩展 tools/pretrain_cbf.py 为向量化/JIT 版本；构建包含硬负样本、梯度监督的数据集，并周期性导出校验曲线（loss、梯度范数），以缩短主训练阶段收敛时间。
  - 自动化汇报管线：将 collect_metrics.py、鲁棒评估、可视化脚本整合为单一命令（例如 python tools/report_generate.py --run outputs/xxx），自动生成 JSON/CSV/PNG/PDF 组合报表，支持阶段性汇总。
  - 策略蒸馏与课程调参：尝试更长的策略冻结/蒸馏阶段（结合 --policy-freeze-steps 与 --distill-policy），探索不同噪声与混合调度对最终成功率、能耗的影响，形成参数网格搜索脚本。
  - 问题跟踪与回归：针对 outputs/ 中历史实验，编写自动回归脚本对关键指标（成功率>90%、最大违约<20、松弛均值<0.05）进行报警提示；一旦新实验触发报警，自动附带阶段日志与对比图。


2025.10.17

完成内容

  - 自动化训练：新增 tools/run_safety_suite.py，支持多组安全训练预设（解析基线、混合方案、快速模式、干跑打印等），并在实验结束后自动收集 metrics.json/csv 与汇总表。
  - CBF 预训练优化：重构 tools/pretrain_cbf.py，引入 jax.jit/向量化批处理、批采样选择器以及 --no-jit 调试开关，大幅提升解析 CBF 监督训练效率。
  - 报表生成：扩展 report_generate.py，整合 suite/validation 指标表、曲线与自动生成的 PNG 图 (validation_stage_metrics.png、validation_stage_history.png)，可一键输出完整 PDF。
  - 蒸馏模板：新增 tools/run_distill_suite.py，预置轻量/重度/stop-gradient 三套蒸馏课程组合，支持 --execute 真跑或 --fast 冒烟。
  - 指标报警：新增 tools/check_regression.py，可对任意输出目录检测最大违约、松弛均值与成功率；生成 artifacts/regression_summary.json 并在超阈时返回失败码。

  验证

  - python tools/run_safety_suite.py --dry-run --fast
  - python tools/pretrain_cbf.py --dataset-size 64 --batch-size 16 --steps 10 --log-every 5 --output outputs/cbf_pretrain_test.pkl（含 --no-jit/默认 JIT 两种）
  - python report_generate.py --output artifacts/test_report.pdf
  - python tools/run_distill_suite.py --fast
  - python tools/check_regression.py outputs/validation_stage --max-violation 5 --max-relax 0.1 --min-success 0.0

• 下一步重点计划

  - 批量长程训练执行：使用 tools/run_safety_suite.py --execute 在高性能环境跑满预设实验（解析基线、混合方案、激进噪声），训练前准备 GPU/CPU 配置，训练后立即运行 tools/check_regression.py 做指
    标验收，并同步生成 PDF 报告。
  - 教师策略准备与蒸馏实验：整理可靠的教师策略参数（或从历史运行中挑选 best checkpoint），通过 tools/run_distill_suite.py --execute --distill-policy <path> 执行轻量/重度蒸馏组合。重点观察冻结
    步数、蒸馏权重对成功率与违约的影响；出现异常时用 check_regression.py 复核。
  - CBF 预训练数据加强：利用向量化 tools/pretrain_cbf.py 扩充数据集（特别是困难障碍物/边界样本），分阶段保存 loss 曲线；将最佳权重作为 train_safe_policy.py 的初始 CBF 参数，比较是否减少训练中
    的松弛使用。
  - 自动化报告流水线：将安全 suite、蒸馏、验证阶段的结果统一调用 report_generate.py 输出最新 PDF，并在仓库中维护报告版本号或日期，方便团队复盘。
  - 上线前回归清单：整理一个 shell/脚本（例如 bash scripts/regression_check.sh）串联 pytest -q tests/test_smoke.py、tools/check_regression.py、report_generate.py 等关键命令，作为后续每次大改动前的快速验收流程。




2025.10.17下午

阶段执行概览

  - 安全训练套件：以 --fast 模式完成 tools/run_safety_suite.py 预设（解析基线、混合/激进方案），自动收集 metrics.json/csv 和 suite_summary.{json,csv}；新增 outputs/safety_suite/… 数据，可在高
    性能环境去掉 --fast 运行长程实验。
  - 蒸馏流程样例：执行简化蒸馏训练（5×5 episodes）并落地 outputs/distill_sweep/distill_light，配合 collect_metrics.py 与 stage_summary.py 验证指标采集；tools/run_distill_suite.py 可直接生成完
    整命令。
  - CBF 预训练扩展：使用向量化后的 tools/pretrain_cbf.py（JIT 批处理）完成 dataset_size=256 的预训练，输出 outputs/cbf_pretrained_vectorized.pkl，提升后续训练初始稳定度。
  - 自动化报告：report_generate.py 已整合套件 + 验证指标，生成 artifacts/combined_report.pdf，包含曲线、表格、实验概览。
  - 回归监控：新增 tools/check_regression.py 并对 outputs/safety_suite/* 运行验证，生成 artifacts/regression_summary.json，阈值可自定义（违约、松弛、成功率）。

  产出快照

  - 安全套件汇总：outputs/safety_suite/suite_summary.{json,csv}
  - 验证阶段图表：outputs/plots/validation_stage_metrics.png, validation_stage_history.png
  - 综合报告：artifacts/combined_report.pdf
  - 新 CBF 权重：outputs/cbf_pretrained_vectorized.pkl
  - 回归结果：artifacts/regression_summary.json

  下一步

  1. 在 GPU/长时间环境运行 tools/run_safety_suite.py、tools/run_distill_suite.py 去掉 --fast，获得高质量成功率数据，再通过 report_generate.py 更新报告。
  2. 将 tools/check_regression.py 与 pytest -q tests/test_smoke.py、report_generate.py 打包成一键脚本，作为提交前回归流程。
  3. 使用扩展后的 cbf_pretrained_vectorized.pkl 作为 train_safe_policy.py 的 --cbf-params 输入，比较松弛使用率与违约改善情况。

 
2025.10.18

 综合概况

  - 系统现状: 当前仓库实现了 JAX 可微分动力学、Flax 策略、GNN-CBF 与 qpax 安全层、lax.scan BPTT 循环及多套批处理脚本，形成效率→安全→噪声/蒸馏的完整训练流
    水线。
  - 资产结构: 代码主体位于 core/、tools/、train_safe_policy.py；outputs/ 含大量训练产物（pkl/json/csv/png）；artifacts/ 聚合报告；二进制日志未逐条解析但已
    确认存在。
  - 运行记录: 10.1-10.17工作记录日志.md 详述各阶段成果与不足，明确效率阶段成功率低、神经 CBF 在噪声下不稳、自动化与向量化工具正在完善。

  主要风险

  - 效率基线不足: outputs/efficiency_test 等仍欠长程训练与成功率统计，后续安全阶段在欠收敛策略上叠加约束会加剧震荡。
  - 神经 CBF 可靠性: 噪声课程实验 (safety_noise_cbf 等) 违约仍高（>0.2），虽有解析 fallback，但 blend/backoff 逻辑可能导致长时间依赖解析项，神经网络难以真正接管。
  - 超参/配置漂移: configs/default_config.py 暴露大量未被主脚本消费的字段（如curriculum、optimisation 等），易引发认知偏差，CI/脚本间参数不一致风险高。
  - QP 稳定性监控弱: safety_filter 在 qpax 返回非有限值时仅回退名义控制且无额外惩罚，日志虽记录 constraint_violation，但缺乏集中报警与场景重放。
  - 数据资产冗杂: outputs/ 中历史实验繁多，指标分散在 metrics.json/csv、training_results.pkl，缺乏统一索引与版本标记，团队协作取用成本高。
  - 自动化链条未闭环: 虽有 run_safety_suite.py 与 run_distill_suite.py，但效率阶段、回归检测、报告生成尚未串成“一键回归+报告”脚本；回滚策略依赖手动观察。

  改进建议

  - 强化效率阶段: 制定 ≥1000 episode 的纯效率训练基线，固定随机种子与噪声，记录成功率/终距/控制平滑指标；作为后续安全阶段的入场门槛。
  - 升级 CBF 预训练: 利用向量化后的 tools/pretrain_cbf.py 扩充高难度样本（贴边障碍、窄缝、动态障碍），并引入梯度监督或多温度 soft-min，提升网络在噪声环境下的 Lipschitz 性能。
  - 参数治理: 精简 default_config 至实际使用字段；将 CLI 输入与配置快照写入每个输出目录，配合 collect_metrics.py 自动汇总，避免手动比对。
  - 安全层监控: 在 safety_filter 回落时增加惩罚或触发 adaptive/penalty_boost，并将 qpax 迭代失败计数写入日志，以便 check_regression.py 报警。
  - 资产整理: 为 outputs/ 引入 manifest（例如 outputs/index.json）记录实验名称、参数、阶段、指标；并建立命名规范（日期+阶段），便于回滚/发表。
  - 自动化回归: 组合 pytest、run_safety_suite.py --fast、check_regression.py、report_generate.py 为单独脚本（如 scripts/run_regression.sh），用于大改前后快速验收。

  后续路线

  - 阶段一: 先完成长程效率训练与指标固化，更新 outputs/efficiency_long_run*，确保成功率 ≥90%、终距 <0.2m。
  - 阶段二: 基于新效率策略运行 tools/run_safety_suite.py --execute（GPU/长时环境），对不同噪声/blend 组合获取稳定数据；使用 check_regression.py 阈值监控。
  - 阶段三: 以强化后的解析/神经 CBF 权重，实施噪声课程 + 蒸馏联合实验；对比distill_light/heavy 成效，验证 blend_backoff 策略。
  - 阶段四: 将回归流程脚本化，生成最新版 artifacts/combined_report.pdf 与manifest，确保团队可复现、可审计，并为后续硬件迁移或多机扩展搭建标准基线

结果概览

  - 训练命令: 在 outputs/efficiency_longform_cpu 路径完成 1000 集、80 步长的纯效率训练（安全层全关、噪声 0），运行环境为本机 CPU。
  - 学习曲线: 总效率损失自 1.2e5 量级快速下降至 7.97e2；安全相关项保持 0，证明纯效率链路可独立收敛。
  - 配置留痕: training_results.pkl 中完整记录了配置快照与 1000 条训练日志，可随时复现或继续迭代。

  关键指标

  - loss/efficiency: 797.34（最终评估批次）。
  - eff/goal_final_distance_to_goal: 0.324 m，末端高度误差 0.026 m。
  - eff/control_mean_control_magnitude: 1.33，控制能耗 276.33。
  - eff/smoothness_control_variation: 0.179，控制变化较温和。
  - eval/success_rate: 0.875（最后 64 次评估平均，距离 0.1 m 阈值）。
  - eval/max_violation / eval/relax_mean: 均为 0（无安全层）。

  下一步

  - 再延长训练或调权重: 当前成功率接近 0.9，但距离 0.95 目标仍有差距，可尝试更长集数或调高 goal_weight、降学习率，进一步压低终距。
  - 封装指标快照: 将本次评估结果纳入指标表（如 metrics.json/csv）或更新 curriculum_summary，方便后续对比。
  - 后续流程: 以该基线为起点，再展开安全层/噪声课程实验；若进入安全阶段前需更稳的收敛，可先微调超参或加梯度裁剪以降低 2.95e4 的梯度范数。


2025.10.18下午

效率训练调整

  - 损失权重: 在 core/simple_training.py 中提升目标相关权重（goal_weight 16、final_goal_weight 80）、增强 Z 轴与悬停惩罚，略微下调控制/平滑惩罚，以强化位置精度与末端刹停。
  - 长程训练: 以 outputs/efficiency_longform_cpu_refine2 策略为起点，执行 1500 集纯效率训练（outputs/efficiency_longform_cpu_refine3，学习率 7e-5）。最终评估成功率 1.0，终点距离约 0.032 m，平均距离 ≈0.97 m，控制均值 1.42，平滑变化 0.26。
  - 指标固化: 更新 outputs/efficiency_metrics.json/.csv，包含三轮效率实验的成功率与关键指标，用于后续回归对比。

  当前要点

  - outputs/efficiency_longform_cpu_refine3/training_results.pkl 保存最新策略参数与 1500 条训练日志，可用于安全阶段或进一步调参。
  - 由于加大目标相关权重，总效率损失（≈890）大于旧值，但成功率与终点精度已达要求；若需兼顾损失规模，可在保留当前参数前提下再微调学习率或重新权衡权重。

  如需继续压缩平均距离或回归原始权重，请告知下一步需求。


• 基线归档完成

  - 策略快照: 将 outputs/efficiency_longform_cpu_refine3/training_results.pkl 中的策略参数导出到 outputs/efficiency_baseline_policy.pkl，作为后续安全/噪声训练的起点。
  - 基线配置: 生成 outputs/efficiency_baseline_config.json，记录来源实验、训练配置以及关联的指标快照，供未来引用。
  - 指标标记: 在 outputs/efficiency_metrics.json 为 efficiency_longform_cpu_refine3 打上 baseline: true，并保留之前各轮效率训练结果，方便回归对比。

  现在可以直接以 outputs/efficiency_baseline_policy.pkl 作为安全/噪声阶段的策略基线；如需加载，只要在后续训练命令中指定 --policy-params outputs/efficiency_baseline_policy.pkl 即可。

10.19重新审视

项目

  - 目标总结: 将 GCBF+ 的形式化安全证书与 DiffPhysDrone 的可微物理端到端学习融合，产出在复杂环境中兼顾安全与效率的单机无人机控制系统。
  - 核心路线: 感知图编码→策略网络→qpax 安全层→JAX 原生物理引擎→jax.lax.scan BPTT，多目标损失与课程/噪声调度驱动训练。

现有情况

  - 代码骨干: core/* 已实现 GNN CBF、QP 安全滤波、点质量动力学、效率损失、训练主循环；工具链覆盖预训练、批量实验、指标收集与报告生成。
  - 实验成果: outputs/ 存在效率阶段高成功率基线、若干安全/课程/蒸馏试验记录、回归检测摘要；artifacts/ 汇总 PDF 和回归 JSON。
  - 文档记录: method.md、research_digest.md、10.1-10.18工作记录日志.md 梳理了战略、架构、日常实验进展；仓库缺失预期的 README.md。

我们目前面临的关键风险

  - 安全阶段收敛不稳: 多个安全实验 (exp_curriculum_* 等) 在 eval/success_rate 中仍为 0，random_violation_max 高达 30~70，说明神经 CBF 与 qpax 尚未形成可靠约束。
  - CBF 监督不足: 当前预训练主要依赖 soft-min 解析屏障，样本分布与真实扰动/噪声不匹配，cbf_value 常需 nan_to_num 填充，掩盖数值失稳根因。
  - QP 稳定性监控缺口: safety_filter 遇到异常即回退名义控制，但缺少集中日志与失败计数，regression_summary.json 显示成功率 0 仍标记 OK，风险未显式暴露。
  - DiffPhys 特性缺失: 目前物理引擎仅含简化加速度模型，缺乏速度拖滞、渲染梯度、并行环境等 DiffPhysDrone 关键要素，难以复现论文级性能。
  - 配置不一致: default_config 中梯度衰减 α=0.92，而训练入口默认 0.4，课程/噪声/混合权重在多处重复定义，易造成实验间不可比与回归。
  - 数据资产管理风险: outputs/ 体量庞大但缺少 manifest/版本标签，stage_summary 需人工筛检，难以快速定位最新可信基线。

我们计划的解决方案

  - 强化安全指标管控: 在训练循环及工具脚本中记录 qpax 迭代失败次数、松弛激活比例、最大违约，并在阈值超标时中止或自适应增益，确保问题显性化。
  - 扩展 CBF 数据与正则: 使用 tools/pretrain_cbf.py 新增贴边/窄缝/动态障碍样本，引入梯度与 Hessian 监督，调高 soft-min 温度自适应，并在端到端损失中增加 Lipschitz 限制。
  - 对齐 DiffPhys 机制: 将拖滞/推进延迟、控制低通、梯度衰减 α 与论文设定一致；补充深度图渲染或最小化替代，以便复用原始损失组合和并行环境策略。
  - 清理与版本化配置: 统一 TrainingConfig 与 default_config 字段来源，生成训练快照（配置+Git hash+权重）写入 outputs/index.json，配合 collect_metrics.py 自动更新。
  - 暴露 NaN 根因: 将 jnp.nan_to_num 前置检查改为显式抛错或日志记录；为 CBF 网络输出加入幅值裁剪与梯度惩罚，避免数值漂移被静默处理。
 

我们制定的阶段总览

  - 阶段 0：诊断与守门修复（1 周）
    完成 tools/check_regression.py 阈值整改、qpax 失败率/松弛率日志化，统一 TrainingConfig 梯度衰减/安全参数；输出最新效率基线验证报告，成功率 ≥0.95、违约 0。
  - 阶段 1：安全基线构建（1–2 周）
    在零噪声条件下启用解析 CBF + qpax，训练到成功率 ≥0.9、max_violation <20、qpax 失败率 <0.5%；固化权重 policy_safe_v1 并写入配置快照与评估 JSON。
  - 阶段 2：神经 CBF 融合（2 周）
    扩充 tools/pretrain_cbf.py 数据（贴边、狭缝、动态障碍）并加入梯度/Hessian 监督；开展解析-神经混合训练（blend α 从 0.3→0.8），完成蒸馏损失接入，要求鲁棒评估成功率 ≥0.85、qpax 失败率 <1%，生成 cbf_neural_v1。
  - 阶段 3：噪声与对抗鲁棒（2–3 周）
    逐步打开观测噪声、点云扰动、控制频率抖动，训练阶段化课程和 ACC；通过 run_safety_suite.py （执行模式）取得成功率 ≥0.8、max_violation <25、紧急制动触发率 <5%，更新 suite_summary 和回归报告。
  - 阶段 4：DiffPhys 扩展与并行训练（3 周）
    在 core/physics.py 补齐推力迟滞、阻力、渲染梯度及并行环境；迁移 DiffPhys 多重损失/时间衰减调度，完成 GPU 大批量训练，目标是单卡 8 环境下稳定收敛、性能与阶段 3 持平。
  - 阶段 5：部署与验证（3+ 周）
    建立硬件在环脚本、浮点差异调参、紧急策略验证；压缩模型、导出换算（ONNX/XLA AOT），完成实测飞行与最终技术报告/演示。

跨阶段支撑

  - 指标守门：每阶段强制执行回归检测、生成 metrics.json/PDF 报告并归档 artifacts/。
  - 资产管理：为新模型/配置建立 manifest，清理过时 outputs/，保持仓库与远程同步。
  - 风险监控：持续关注 qpax 数值稳定、CBF NaN 出现率、训练振荡；必要时回退上一阶段权重。


10.19下午

阶段一冒烟最新状态

  - 执行调整：新增跨平台配置 --config-name stage1_analytic、放宽 goal_tolerance、增大松弛上限/降低惩罚、引入基线策略预热，并在评估时打印成功率与违约值，方便 macOS、Windows、Kaggle 统一排查。
  - 冒烟结果：
      - 直接训练（无预热）与加载 outputs/efficiency_baseline_policy.pkl 后运行，eval/success_rate 最高仅达 0.625，eval/max_violation 持续远大于 20（峰值 ≈ 263.9）。
      - 即便放宽松弛（max_relaxation = 8），QP 仍频繁触发回退，残余违约大幅超标，说明解析 CBF + qpax 的组合在当前动力学/点云设定下不可行。
  - 诊断推断：constraint_violation 的巨大残差多半来自 qpax 返回 NaN/不可行解，导致回退到名义控制后再计算残差，这说明解析 CBF 的导数结构或松弛设计与动力学不匹配（约束本身无法满足），需要进一步检查 _analytic_cbf_statistics 或 _assemble_qp。

我们打算做的下一步

  1. 记录 QP 失败率：在 safety_filter 中新增计数与日志（例如 qp_failed 标志），确认是否每步都在 fallback。
  2. 单步诊断脚本：编写快速测试（可在 debug_step_by_step.py 中追加）输出 h_value、grad、psi1、rhs 等关键信息，定位约束为何不可行。
  3. 调整解析 CBF：必要时重新设计安全半径/软最小温度，或在解析模式下临时放宽违约阈值，明确与真实需求的差距。
  4. 确认成功判定逻辑：评估阶段使用 config.success_tolerance（当前 0.3），若后续调小目标距离或延长 horizon，再重新测试。


10.20

阶段一现状

  - qpax 数值诊断：新增 qp_failed 记录与半空间投影回退后，所有样本都能返回有限的约束违约（≈0），说明安全层不会再因为求解器崩溃导致大幅度违约。
  - 调参尝试：尝试缩短目标距离 (target_position=1.5,0,1.5)、引入策略蒸馏 (distill_weight=0.2)、延长 horizon、使用效率策略预热等多种组合，训练均可跑完并生成日志，但 eval/success_rate 仍为 0；debug_step_by_step 显示名义策略几乎没有推进，最终距
    离 >100 米。
  - 结论：效率阶段策略在当前 JAX 物理+观察输入下无法驱动无人机向目标移动，安全层只是将名义控制裁剪/回退，因此成功率为 0。阶段一要达成指标，必须先解决策略层“无动作”的根因（观测拼接、损失权重、初始化等），否则安全训练无从谈起。

我们打算的执行顺序

  1. 重建效率阶段基线（高优先级）
      - 验证 train_safe_policy.py --disable-safety --target-position 4 0 1.5 是否仍能获得成功率 >0.9；若失败，说明效率模块需重写或恢复老版本。
      - 若成功，再在同参数下启用 CBF，检查成功率下降原因。
  2. 策略观测诊断
      - 打印策略前向输入（位置、速度、目标偏差、CBF 值）是否合理；必要时重新归一化或恢复 DiffPhys 原始编码（深度图+状态拼接）。
  3. 蒸馏链路验证
      - 直接生成若干关键点的教师动作与学生动作对比，确认蒸馏损失是否发挥作用；如有必要使用更高权重或冻结安全层，使策略先恢复移动能力。
  4. 阶段一重复验证
      - 当效率阶段重新达到成功率 ≥0.9 后，再启用解析 CBF + qpax，重新执行冒烟和正式训练；若仍有违约超标，再针对 CBF 参数微调。


10.20下午实时进展

 效率阶段复验

  - 已执行无安全层训练（--disable-safety）并加载 outputs/efficiency_baseline_policy.pkl；结果 eval/success_rate = 1.0、最大违约为 0，效率阶段在当前配置下仍能稳定达成目标，说明策略主体链路正常。
  - 训练产物保存在 outputs/efficiency_recheck，并用 tools/check_regression.py 验证通过。最终距离约 0.12 米，佐证策略能够到位。

  待解决问题

  - 启用安全层后成功率降至 0，qp_failed 频繁触发，说明解析 CBF/QP 组合对策略动作仍过于保守，导致实际控制被松弛吸收并失去推进能力。
  - 需进一步诊断策略观测和蒸馏链路，可能包括：检查 CBF 值注入后的输入分布、调整安全损失权重、或在安全阶段逐步加大目标（例如先让安全层学习近距目标，再过渡到远距）。
  

策略+安全诊断结果

  - 效率基线：重新在 --disable-safety 下训练（输出 outputs/efficiency_recheck），eval/success_rate = 1.0，证明策略网络与物理链路在无安全层时完全正常。
  - 加入安全层后的行为：通过新脚本 tools/inspect_policy_inputs.py 逐步打印观测、策略动作与安全层回退：
      - 前几步 qpax 求解正常，之后立即出现 qp_failed = 1；
      - 可微 QP 返回 NaN 后我临时以松弛量兜底（delta 可达 ~6），保持名义动作不变；
      - 尽管约束残差保持 0，评估阶段仍记录大额 “最大违约”(>100)，原因是松弛饱和导致数据层认为“约束被强制放宽”，成功率因此为 0。
  - debug_step_by_step 观测：60 步 roll-out 后位置仍远离目标（甚至出现负坐标），说明在安全层介入后策略实际轨迹被完全扭曲；qp_failed 统计 10/60，松弛均值 0.28，解析 CBF 最小值为 -0.86——解析屏障本身对当前动态极不适配。
  - 关键根因：解析 soft-min CBF 产生的导数与 Hessian 令 qpax 难以收敛（求解直接返回 NaN）；虽然 fallback 保证了数值稳定，但松弛量巨大，评估时仍被视为“严重违约”，导致成功率为 0。

  
10.21上午进展
  阶段一推进建议

  1. 修正解析 CBF：需要重新设计/剪裁 _analytic_cbf_statistics（例如放弃 Hessian、调高安全半径、限制 ψ₁ 推进项），让 qpax 在常见状态下能直接收敛；这是阶段一最紧急的阻塞点。
  2. 逐步收紧松弛：在 QP 稳定后，再逐步下降 max_relaxation、恢复高惩罚，确保安全层不依赖大尺度松弛。
  3. 继续使用 inspect_policy_inputs.py：任何调整后先用该脚本对比“策略动作 vs. 安全动作”，快速判断是否仍出现 qp_failed，必要时再跑完整训练。

当前日志和脚本已全部保留、


阶段一整改进度

  - 重构解析 CBF：改为以 LiDAR 平滑最小距离为核心，提供解析梯度并伪正定 Hessian，同时大幅提高 α₁、α₀ 和 Hessian 系数，显著缓解 qpax 数值异常问题。
  - 安全层回退策略更新：当 qpax 失败时改为“裁剪名义控制 + 精确松弛兜底”，避免随机 NaN，并将 qp_failed、松弛率等指标透出到训练日志。
  - 物理与策略同步降速：将 max_thrust、policy.action_limit、safety.max_acceleration 下调（均为 3.0 / 2.0），以限制策略过激指令；
    新的效率基线（outputs/efficiency_baseline_new/training_results.pkl）在这一设定下重新训练完毕，未来可进一步导出纯策略参数用于蒸馏。
  - 诊断工具链完善：tools/inspect_policy_inputs.py 和 tools/diagnose_qp.py 可实时观察策略观测、QP 约束、松弛触发等细节；多处调参均通过该工具验证。

  仍待完成

  1. 提取新策略权重：从 outputs/efficiency_baseline_new/training_results.pkl 中导出 params_policy（可通过简单脚本保存为 efficiency_baseline_new_policy.pkl），并在安全阶段使用它作为教师/初始策略。
  2. 调整松弛守门：目前评估依旧将平均松弛视为违例，需要结合新策略重新训练后再观察；如放宽到 0.4~0.5 仍超标，说明策略尚未真正学会在受限加速度下逼近目标。
  3. 重新训练短距目标：在更新策略权重后，重新运行 train_safe_policy.py（短距 1.5 米），验证 eval/success_rate ≥ 0.9 且 eval/relax_mean < 0.4；若仍偏高，可继续调大 relaxation_penalty 或延长训练步数（让策略适应新的动作上限）。

  

10.22上午实时进展

阶段一调整进度

  - 从最新效率基线 (outputs/efficiency_baseline_scaled) 提取了策略权重，并在安全阶段训练中启用了更强的松弛惩罚、蒸馏以及解析 CBF 的替换方案（平滑距离 + 强正定 Hessian）。
  - 降低了物理/策略的最大加速度（max_thrust=2.0、action_limit=1.5、safety.max_acceleration=2.0），同时将安全损失、松弛惩罚权重调高，并将 relaxation_alert 限制在 0.4。
  - 多轮短距离目标（1.5 m）训练已完成，松弛均值相比最初显著下降，但最新 run (outputs/stage1_shorttarget) 仍显示过半评估的 eval/relax_mean 在 0.6~1.3 之间，eval/success_rate 最高 0.875、末尾 0.125，尚未达到阶段一标准（成功率 ≥0.9 且 relax
    ≤0.4）。

  下一步操作

  1. 进一步限制策略动作：尝试继续下调 action_limit 至 1.0、max_thrust 至 1.5，并在安全阶段前加入额外的效率预训，确保策略在新约束下仍能到达目标。
  2. 增加松弛权重与训练时长：可将 --relaxation-weight 提高到 10 甚至 20，并延长训练 episode（例如 200），同时启用自适应课程（先短距离 1.5 m，成功后再放宽至 4.0 m）。
  3. 观察策略/安全协同：利用 tools/inspect_policy_inputs.py，重点查看 30~40 步附近的松弛使用情况，定位造成高松弛的速度/姿态，必要时在损失函数中加入速度约束或惩罚策略的纵向速度。
  4. 考虑引入神经 CBF 蒸馏：若解析 CBF 过于保守，可对 qpax 输出进行采样监督，再训练一个轻量神经 CBF 替换解析部分，减轻高速度场景下的松弛需求。

  
  10.22下午实时进展

• 阶段一最新状态

  - 已完成效率阶段重新训练（efficiency_scaled_lowact），并导出适配 max_thrust=1.5 / action_limit=1.0 的策略权重。
  - 将该策略用于短距安全训练（outputs/stage1_phaseA，episodes=200，松弛惩罚 20），解析 CBF + qpax 的数值运行稳定，未再出现 NaN。
  - 仍未达标：评估成功率 ≈ 0.25，eval/relax_mean 长期在 0.6~1.3。说明在当前约束下策略仍依赖较大松弛，安全层无法保证实际成功。

  后续执行规划

  1. 再次降低动作幅度：尝试将 policy.action_limit、physics.control.max_thrust、safety.max_acceleration 再降至 1.0 / 1.2，并重新训练效率基线，逐步逼近策略可行域。
  2. 阶段化课程：按照原计划设置两阶段目标（先 1.5 米 → 达标后到 4.0 米），同时使用 --relaxation-weight 高值 + 自适应课程脚本，使策略在低速条件下先收敛。
  3. 松弛监控脚本：利用 tools/inspect_policy_inputs.py --seed 多次采样，聚焦本文 30~40 步时的速度/姿态，定位松弛暴涨原因，并考虑在损失中加入横向速度或高度偏差惩罚。
  4. 神经 CBF 蒸馏准备：着手采样 qpax 优良轨迹，设计轻量 GNN 训练靶值（h 与导数），后续可逐步替换解析 CBF，以降低松弛需求


10.23上午实时进展

阶段一动作幅度缩减进展

  - 配置下调：将 max_thrust、policy.action_limit、safety.max_acceleration 依次降至 1.2/1.0/0.8→1.0/0.6/0.6，现已运行 efficiency_microact（动作上限 0.6、推力 1.0）。无安全层训练稳定，但成功率仍未超过 0.9（多次评估维持在 0.5~0.75），说明策略需
    要更长训练和蒸馏强化。
  - 阶段一短距训练（outputs/stage1_phaseA，动作 0.6、推力 1.0、松弛权重 20）：qpax 可解；松弛均值显著降低，但仍 >0.5，成功率在 0.125 附近。

  下一步规划

  1. 继续训练效率基线：延长 episode（>400）并提高蒸馏权重 0.3→0.5，使小推力策略能到达 4 m 目标。
  2. 阶段一短距策略：同样延长 200→400，减小学习率或加入速度约束损失，重点降低松弛使用。
  3. 松弛热点分析：结合 inspect_policy_inputs.py —seed，定位松弛使用高的时间段（常在 60~80 步），补充惩罚项（例如正向速度或高度误差），并记录调参结果。



10.23下午实时进展
• 效率阶段强化结果

  - 在最新配置（max_thrust=1.0、action_limit=0.6、无安全层、distill_weight=0.5、episodes=450）下重新训练，路径 outputs/efficiency_microact_long：最终 eval/success_rate = 1.0、最大违约为 0，证明小推力限制下的策略已经恢复到 4 m 目标的成功率。
  - 已导出对应权重 outputs/efficiency_microact_long_policy.pkl，后续安全阶段可直接加载该策略作为初始/教师。

  打算继续进行阶段一短距安全训练或下一步（松弛权重调优、松弛热点分析、神经 CBF 蒸馏等）


10.24上午进展

团队重新讨论

 总体认知

  - 架构融合: 当前训练管线以 train_safe_policy.py 为中心，串联 core/perception 图神经 CBF、core/policy 策略 MLP、core/safety 中基于 qpax 的 CBF-QP 及 core/physics 点质量动力学，整体沿用 GCBF+ 的形式化安全思路与 DiffPhysDrone 的可微物理和时间
    梯度衰减。
  - 数据与工具链: 感知侧仍由固定圆环点云组合而成，通过 sample_augmented_point_cloud 做轻量扰动；训练与分析依赖 Optax、Flax、JAX、qpax，工具脚本覆盖 CBF 预训练、阶段汇总、诊断、蒸馏等。
  - 现有成果: outputs/ 下保存了效率与安全阶段的众多实验快照，工作记录日志.md 详尽记录了动作幅度缩减、松弛惩罚调优等迭代历程，但成功率与松弛指标仍未满足里程碑要求。

  主要风险

  - 点云场景过于静态: 主流程使用固定圆环点云，缺乏 DiffPhys 式的随机障碍、密度变化和多视角感知，导致神经/解析 CBF 都只在单一几何上被监督，泛化能力极弱。
  - 安全层与物理假设不一致: 解析 CBF Hessian 被硬编码为常数矩阵，SafetyConfig 与 PhysicsParams 的加速度/推力上限多次手工调节后仍不一致，造成阶段一对松弛的过度依赖。
  - 数值稳定性不足: qpax 偶发 NaN 仍只靠 nan_to_num 和松弛 fallback，缺少按日志自动降级与阈值报警；core.loop.RolloutStepOutput 内部字段 qp_failed 重复定义，暴露出未完善的诊断结构。
  - 跨平台部署风险: macOS CPU 训练可复现，但 Windows + GPU/Kaggle 下需额外处理 jaxlib/qpax 版本、VS Build Tools、CUDA 驱动等；目前缺乏一份统一的环境脚本与依赖校验。
  - 实验资产管理松散: outputs/ 目录堆积大量 training_results.pkl，缺乏索引与关键指标汇总，后续复现或回滚难度大。

  解决思路

  - 构建多样化可微场景: 参考 DiffPhysDrone 的 CUDA 环境，将障碍类型/密度/扰动的随机生成迁移到 JAX（可先在 CPU 版完成，再在 Kaggle 上验证），同时扩展点云采样为多环、随机稀疏、遮挡和动态物体；引入来自解析 CBF 的安全/不安全标签，丰富预训练数
    据集。
  - 校准安全层参数链: 统一 PhysicsParams 与 SafetyConfig 的加速度/松弛配置；基于真实点云重推解析 CBF 的梯度与 Hessian（可用 soft-min 二阶解析或数值雅可比），并建立单元测试验证 h, ∇h, ∇²h 与物理一致；将高松弛 episode 自动回写到再训练数据集中。
  - 强化数值监控与回退: 扩展 SafetyDiagnostics，记录 nan 次数、松弛超阈值事件、QP 迭代统计；在训练环节对 qp_failed_rate 设置硬性停止条件，并根据失败次数动态降低动作上限或提升松弛惩罚；修正 RolloutStepOutput 重复字段，确保日志结构清晰。
  - 制定跨平台环境手册: 维护 environment/ 脚本，分别给出 macOS（CPU）、Windows（CPU/GPU）、Kaggle（GPU，pip install "jax[cuda12_pip]" qpax）的完整安装步骤与版本锁定；补充 WSL2/conda 方案与 VS Build Tools 检查脚本，保障团队成员可一键拉起环境。
  - 整理实验资产: 编写 tools/index_runs.py（或在 stage_summary 中扩展）批量生成 CSV/JSON 索引，记录各 run 的配置、成功率、松弛均值、最大违约等；定期清理/归档历史 run，并在 artifacts/ 内维护阶段性报告和最新模型快照。

  后续规划

  - 短期（本周内）: 修复 RolloutStepOutput 字段、补充安全层数值诊断；在 macOS 上实现随机障碍点云生成并跑数个短程训练，验证松弛是否下降；编写跨平台环境检查脚本雏形。
  - 中期（1–3 周）: 在 Kaggle GPU 上迁移多样化场景训练，完成新的效率→安全课程；同时构建解析 CBF 数据增广与神经 CBF 续训流程；整理 outputs/ 元数据、生成自动报告草稿。
  - 长期（3 周以上）: 将可微场景扩展到深度图渲染与多无人机场景，为后续团队在 Windows 真机上部署做准备；建立 CI 流程（macOS CPU 快速单测 + Kaggle 离线训练检查），确保跨平台改动可持续交付。


• 阶段A：稳定性与工具补齐（本周内）

  - 修复日志结构: 调整 core/loop.RolloutStepOutput 的 qp_failed 重复字段，保证单一来源；同步修改依赖解析的脚本或单测。
  - 安全诊断增强: 扩展 SafetyDiagnostics，记录 NaN 次数、松弛超阈值事件、QP 迭代状态；在训练循环中加入硬性阈值触发的 early stop 与参数回退。
  - 随机点云生成: 实现圆柱/方块/噪声点云生成函数并接入 train_safe_policy.py，支持阶段化开关；在 macOS CPU 上完成 smoke 测试，确保 CBF 与 QP 正常工作。
  - 跨平台环境脚本雏形: 编写 scripts/setup_mac.sh、setup_windows.ps1、setup_kaggle.sh 三套检测与安装脚本，验证核心依赖（Python/JAX/jaxlib/qpax/VS Build Tools/CUDA）。
  - 实验资产索引: 新建 tools/index_runs.py，遍历 outputs/ 生成 CSV/JSON，总结每个 run 的配置、成功率、松弛均值、最大违约与时间戳。

  阶段B：多样化场景与安全闭环（1-3 周）

  - 点云增强并入主循环: 将随机多环、遮挡、动态噪声注入主训练流程，增加配置项控制，使课程阶段可选不同模式。
  - 解析 CBF 校准: 重新推导 soft-min CBF 的梯度/Hessian，保证与 SafetyConfig、PhysicsParams 的加速度/松弛界一致；编写单元测试检验数值。
  - Kaggle GPU 训练复现: 基于更新配置跑效率→安全→噪声课程，记录成功率、松弛均值、QP 失败率，生成阶段报告初稿。
  - 神经 CBF 续训管线: 扩展 tools/pretrain_cbf.py 支持增广数据、断点续训与批量化；在主训练入口中加入加载/更新神经 CBF 的选项。
  - 报告自动化: 更新 report_generate.py，整合新指标（随机场景表现、鲁棒性曲线），输出 PDF 供阶段评审。

  阶段C：跨平台部署与性能冲刺（3-6 周）

  - Windows/WSL2 适配: 完成 GPU 环境验证（jaxlib + CUDA + qpax），记录安装与排错步骤，编写自动检测脚本；准备真实设备部署手册。
  - 感知链扩展: 实现可微深度图渲染或多视角点云接口，使训练能模拟 DiffPhysDrone 的视觉输入；优化策略网络以适配更真实数据。
  - 鲁棒性套件升级: 扩展 tools/run_safety_suite.py，加入长时间噪声注入、自动回退策略与失败案例捕获；定期输出汇总表。
  - 动作课程脚本化: 固化“低推力→高推力”课程策略，封装 CLI 参数组合，减少人工调参；将经验写入配置手册。
  - 阶段文档沉淀: 撰写技术白皮书（方法论、实现细节、实验数据、环境指南），为团队交接与外部展示做准备。

  持续保障工作

  - 环境一致性: 每阶段结束运行环境脚本验证三端依赖，更新 README 与安装说明。
  - 数据管理: 约定命名规范，重要 run 迁移到 artifacts/ 并附带 JSON 元数据；过期 run 定期归档或清理。
  - 评估守门: 固定成功率 ≥0.9、松弛均值 ≤0.4、QP 失败率 <1% 的阈值，未达标时阻止阶段推进并触发诊断。


10.24下午
已完成

  - 扩展 SafetyDiagnostics，新增 NaN 检测、松弛超阈值与 QP 迭代/状态指标，并让 safety_filter 在成功/回退两条路径都记录这些诊断信息。
  - core/loop.RolloutStepOutput 与训练日志同步增加上述字段，非安全模式默认返回零值，便于统一统计。
  - 安全层改用 qpax.solve_qp 获取原始解同时保留迭代次数与状态码，所有诊断量均做 stop_gradient 处理保证训练稳定。
  - train_safe_policy.compute_total_loss 新增安全统计（nan 率、松弛超限率、QP 迭代均值/峰值、状态码等），并引入可配置的硬阈值；若超限则立即回退到最佳参数、打标 hard_abort 并终止训练。
  - 默认配置与阶段一配置补充 safety.relaxation_alert 及新的硬阈值参数，SafetyConfig/TrainingConfig 均更新相应字段。

已完成

• 完成内容

  - 新增圆环、圆柱、方块、噪声与混合结构的点云生成器，并通过 build_point_cloud_for_mode 按阶段切换；支持与原有增强逻辑叠加。
  - TrainingConfig/CLI 增加 point_cloud_modes 配置，课程阶段可独立指定点云模式；训练日志新增 curriculum/point_mode_id 便于追踪。
  - 训练循环、成功评估与鲁棒统计全面接入阶段化点云模式，确保评估与训练一致；当阶段启用随机点云时，所有评估也同步使用对应模式。
  - sample_augmented_point_cloud 现返回 (cloud, rng)，并同步更新主流程与 tools/evaluate_policy.py。
  - Mac（CPU）环境下 pytest tests/test_smoke.py 通过，确认 CBF 与 QP 正常工作。


10.25

进行了短程长跑结果

  - 运行 stage1_analytic 配置（400 episode）时，在前 120 个 episode 内处于硬阈值热身期，训练日志显示随机点云下松弛、QP 状态均能被记录，阶段性评估也正常输出。
  - 热身期结束后（第 120 步左右），safety/qp_nan_rate ≈ 0.375、safety/relaxation_exceeded_rate ≈ 0.20、safety/qp_fail_rate ≈ 0.375 超过目标阈值，触发硬中止并回滚；训练产物已保存至 outputs/stage1_long_form/training_results.pkl。
  - 当前流程验证了：
      - 训练/评估均使用阶段化随机点云；
      - 安全诊断、阈值调节与硬中止协同正常；
      - 松弛/QP 仍偏高，需进一步调参或延长 warmup/降低课程难度后再冲击成功率目标。

我们提出了调参路线

  - 阶段化点云/噪声：
      - point_cloud_modes: 改为 ["ring", "ring", "cylinder", "mixed"]，先在熟悉的圆环学习，再引入复杂体。
      - noise_levels: 调低首阶段噪声至 0.0，第二阶段 0.01，第三阶段 0.02；待松弛趋稳后再回升。
  - 热身与硬阈值：
      - hard_abort_warmup_episodes: 提升到 200（覆盖整个阶段1 + 阶段2初期），避免刚起步就被硬中止。
      - 阈值设定：安装阶段性 schedule——例如热身期 nan_rate=0.4、relax_exceed=0.3、qp_fail=0.4，热身结束后逐步收紧至 0.1/0.15/0.2，最后阶段再收敛到目标 0.05/0.1/0.1。
  - 松弛与惩罚：
      - 降低初期松弛惩罚权重：relaxation_weight、relaxation_usage_weight 分别砍半（例如 0.5、0.2），待策略能基本到点再逐渐恢复。
      - relax_penalty_boost：首阶段设置较温和的 1.2，第二阶段恢复 1.5；relaxation_alert 由 0.1 提至 0.2，避免轻微波动频繁触发。
  - 课程长度与目标：
      - stage_steps: 调整为 (200, 150, 150)，降低进入混合点云时的压力。
      - success_threshold: 初期以 0.8 评估，通过后再收紧到 0.9；配合 success_patience=5，给更多容错空间。
  - 策略先行预训：
      - 先运行 --disable-safety + 双阶段课程（例如 200 episode）获取最新效率策略，再带入安全训练，并调高 policy_distill_weight (0.2) 让安全阶段沿用效率策略。
      - 如果仍高松弛，考虑在 tools/pretrain_cbf.py 中加入新点云的监督数据，强化神经 CBF 解析能力。
  - 监控脚本：
      - 使用 tools/inspect_policy_inputs.py --mode ring/cylinder 定期抽样检查动作/松弛分布，定位具体时间段。
      - 自定义阈值日志（如 adaptive/hard_threshold_*）应在解析中重点关注，确保热身结束后逐渐收紧。

  以上参数变更可以先在本地短程验证（例如 150 episode），观察松弛均值是否稳定下降，再逐步延长到长程 run


配置与代码更新

  - 课程配置：stage1_analytic 改为 (200,150,150) 三阶段课程，噪声阶梯 0.0→0.01→0.02，点云模式 ring → cylinder → mixed，并降低损失系数（控制/碰撞）与放松松弛惩罚、提高松弛告警阈值。
  - 安全阈值：引入 hard_*_schedule 阶段化阈值，热身时阈值放宽（0.4/0.3/0.4），逐阶收紧至 0.1/0.1/0.15；热身 episode 延长至 200。训练日志新增基础阈值、有效阈值、是否处于热身期等指标。
  - 随机点云：compute_total_loss 统一调度 ring/cylinder/box/noise/mixed 多种点云生成，并在训练/评估/鲁棒性检查中保持一致。
  - CLI 扩展：新参数支持自定义硬阈值 schedule、暖机长度、点云模式/噪声设定等，便于后续跨平台配置。

  短程验证

  - 运行 train_safe_policy.py --episodes 200 --config-name stage1_analytic，训练能顺利完成并保存结果 outputs/stage1_tune_run/training_results.pkl。硬阈值不再提前触发。
  - 由于目前仍处于第一阶段，松弛均值持续过高（>0.2），成功率暂为 0，后续需继续调低初期难度或加强效率预训/蒸馏，方能进入安全阶段收敛。

  如需下一步（例如调低首阶段松弛权重、先跑 --disable-safety 预训或自动生成分析报告），我可以继续协助。


10.26

 配置与实现

  - 课程重构: stage1_analytic 改为 200/150/150 的三阶段课程，噪声阶梯 0.0→0.01→0.02，点云模式依次为 ring → cylinder → mixed，配套降低控制/碰撞损失系数，提升松弛告警阈值，并为松弛/求解惩罚设置基础缩放 0.5→1.0→1.2 与 0.8→1.0→1.2。
  - 安全监控强化: 为硬阈值新增阶段化调度 hard_*_schedule，热身 200 episode 后再逐步收紧；训练日志新增基础阈值、放宽阈值与热身状态字段。
  - 随机点云统一调度: 新增圆柱/方块/噪声/混合生成函数，训练、评估、鲁棒性流程共用同一模式；点云增强接口返回 RNG 以避免状态漂移。
  - CLI 扩展: 支持参数化点云模式、噪声/阈值/松弛缩放等 schedule；_ensure_param_dict 能识别 training_results.pkl 中的策略参数。

  实验运行

  - 效率预训（无安全层）: outputs/efficiency_stage1_pretrain/，150 episode；成功率仍为 0，说明在当前 horizon=40、目标 4m 下仍未输出有效推进。
  - 安全训练: 载入预训策略后运行 200 episode（outputs/stage1_safe_finetune/）。安全诊断未再触发硬中止，但松弛均值在阶段 0 内长时间高于 0.2，成功率仍为 0；说明策略尚未学会有效位移，安全层也未产生有效约束。
  - 自动化分析: 生成 outputs/.../summary.json 并用 report_generate.py 合成 artifacts/stage1_safe_report.pdf；可直接查看日志、指标与阈值曲线。

  结论与建议

  - 当前课程仍过难：策略在首阶段即停滞，安全层只看到高松弛，难以收敛。建议进一步降低首阶段目标距离或延长 horizon（如 80/100），并在效率预训中加入教师引导或减小初始噪声。
  - 可考虑：启用 --disable-safety 较长预训（300+ episodes）并确认成功率>0.8 后，再把权重作为教师蒸馏；同时可缩小第一阶段目标（1.5 m）或增加奖励对目标速度的引导。


可以判断目前瓶颈就卡在效率训练阶段。即便在关闭安全层、只跑效率损失的条件下（--disable-safety），策略仍然无法把无人机推向目标——成功率一直是 0，说明纯效率策略就没学会有效位移。这样一来，后续安全训练即使叠加预训练权重，也只能对“静止策
  略”做安全过滤，自然松弛和成功率都起不来。

阶段目标

  - 让 “效率阶段” 在关闭安全层 (--disable-safety) 时独立收敛，成功率 ≥95%、平均到达误差 <0.3m、速度平稳。
  - 完成后输出可复用的策略权重 (params_policy) 和指标报告，为后续安全训练/蒸馏提供高质量教师。

  ———

  ### 一、任务场景调优

  - 初始与目标配置
      - 起始位置范围：x,y ∈ [-1,1]，z ∈ [0.8,1.2]。
      - 首阶段目标：距离固定为 2m，高度差 ≤0.3m；成功率达到 95% 后逐步拓展到 3m/4m。
      - 目标阈值 success_tolerance 暂设 0.2m，便于策略先收敛；后续再收紧到 0.1m。
  - 控制和物理
      - horizon 提升到 80（或 100），给策略足够步数加速。
      - 限制 policy.action_limit 和 physics.control.max_thrust 在 1.0~1.5 范围，避免首次训练的高加速度导致振荡。

  ———

  ### 二、损失函数与奖励塑形

  - 目标到达项：大幅提高 goal_weight 与 final_goal_weight（如 20/80），确保主要驱动力是减少目标误差。
  - 速度惩罚：增加“到达后速度衰减”或“朝目标方向的投影速度奖励”，鼓励保持目标方向位移并在末尾减速。
  - 控制正则：control_weight 初期下调至 0.02~0.05，让策略敢于发力；待收敛后微调回 0.1 防止抖动。
  - 平滑项：smoothness_weight 保留（0.3~0.5），防止过度震荡；若观察到抖动可略微调高。

  ———

  ### 三、课程式训练流程

  1. 阶段0（基础冲刺）
      - 目标距离固定 2m，无噪声。
      - 运行 --disable-safety，episodes 300~400；每 20 episode 做一次滚动评估。
      - 成功率 ≥0.95、平均松弛（此阶段没有）≈0 即通过。
  2. 阶段1（距离渐进）
      - 采用距离课程：2m → 3m → 4m，每段 150~200 episode；通过后转入下一段。
      - 打开轻微随机噪声（0.005~0.01）只作用于起点，提高泛化。
  3. 阶段2（加强扰动与动作限制）
      - 调回 control_weight、smoothness_weight 至正常值。
      - 保持成功率，减少末尾超调；若成功率下降，则适当延长训练或调低噪声。

 ### 四、收尾与验收

  - 模型固化
      - 将成功率 ≥0.95 的策略 params_policy 另存 artifacts/efficiency_policy.pkl。
      - 输出 summary.json、report.pdf（通过 report_generate.py）记录关键指标和曲线。
  - 验收流程
      1. tools/evaluate_policy.py --trials 200，确认成功率 ≥0.95。
      2. 检查平均距离、速度、动作幅值，确保不会违反安全层限制。
      3. 记录成功的课程设定，以便后续安全训练和团队成员复现实验。
  - 预防措施
      - 在进入安全训练前，保存当前损失权重与课程参数；后续调试安全层时随时可以回滚。
      - 若 plan 过程中任何阶段失败或过慢，及时调整课程跨度或增加提示性损失（如与目标方向的速度投影）。

  按此计划执行，可逐步把效率阶段调至成功率 >95%，为后续安全训练提供稳固的 baseline。



10.27

配置扩展：在 train_safe_policy.py 新增目标距离课程、点云模式/松弛缩放 schedule，创建 configs/stage1_efficiency.py（阶段长度 150/150/150、目标距离 2→3→4 m、噪声 0→0.005→0.01、horizon 支持 80），便于关闭安全层的效率预训。
  - 训练逻辑更新：compute_total_loss/evaluate_success/evaluate_robustness 等流程均可接收阶段化目标点；日志新增 curriculum/target_distance、安全阈值基础值及热身状态等指标。
  - 阶段0训练：执行 python train_safe_policy.py --config-name stage1_efficiency --disable-safety --episodes 150 --horizon 80 --policy-freeze-steps 0 --output-dir outputs/efficiency_stage0_run1，课程评估在训练末期给出 success≈1.0（八次滚动评
    估）；模型与摘要已保存至 outputs/efficiency_stage0_run1/。
  - 评估记录：调用 tools/evaluate_policy.py（已支持 --target-distance）在 200 次 rollout 下得到 success_rate=0.0、distance_mean≈3.14 m，表明策略仍未稳定学会推进；评估结果写入 outputs/efficiency_stage0_run1/eval.json。

  结论与后续建议

  - 虽然课程日志提示短暂成功，但离线评估仍为 0%，说明效率阶段需要进一步调参：应增大目标损失权重/速度奖励、缩小起始范围或加入教师蒸馏，使策略在无安全层环境下真正实现 ≥95% 成功率。
  - 建议下一步针对效率损失与课程再做调优，或设计辅助控制器进行蒸馏；待效率阶段扎实后，再进入安全阶段。


10.27下午

阶段A 调参执行小结

  - 完成效率阶段的全量改造：
      - 在 train_safe_policy.py 为课程引入目标距离、点云模式、松弛/求解器缩放 schedule，并加入教师蒸馏与速度对齐损失。
      - 新增配置 configs/stage1_efficiency.py（阶段 150/150/150、目标距离 1.5→2.5→3.5 m、噪声 0→0.005→0.01、teacher 控制与 horizon=80）。
      - 扩展训练日志与 CLI 参数，保留所有关键指标（安全阈值、目标距离、教师损失等）。
      - 更新评估脚本 tools/evaluate_policy.py 以支持目标距离覆盖。
  - 执行效率阶段基础训练（--disable-safety，300 episode）。课程内部评估显示成功率逐步升至 1.0，日志与模型已保存到 outputs/efficiency_stage0_run2/。
  - 离线评估：tools/evaluate_policy.py 在 200 次 rollout 下成功率仍为 0，平均终点与目标存在较大偏差，说明策略在新课程下仍未真正掌握前进任务（教师蒸馏的力度不足，或目标/损失设计需继续加强）。

  下一步建议

  1. 进一步调节教师策略（增大 teacher_gain_d、teacher_weight，或加入终端减速逻辑）以确保策略真正到达目标；必要时对首阶段目标距离再缩短。
  2. 检查 DEFAULT_EFFICIENCY_CONFIG 或为效率阶段定制损失权重（更强的 goal_weight、更大终点约束），并在 compute_efficiency_loss 中增加对末梯度/平均速度的约束。
  3. 确认离线评估与训练评估使用完全一致的初始条件，必要时在评估脚本中引入与课程相同的起点/教师。
  4. 在策略能稳定达到 ≥95% 成功率后，再保存教师权重并进入安全阶段。


10.27晚
执行内容

  - 在训练主流程中新增教师蒸馏、速度对齐与末端速度惩罚：TrainingConfig 扩展 teacher_gain_{p,d}、teacher_weight、velocity_alignment_weight、desired_speed、final_velocity_weight 等字段；compute_total_loss 融合新的损失项并记录到日志。
  - 更新 CLI 以支持对应超参，从阶段配置中可直接调度课程、教师增益、目标距离等参数。
  - 修改 configs/stage1_efficiency.py：将目标距离调整为 1.2→1.8→2.6 m，减小早期噪声、降低学习率，增强教师控制 (P=1.5/D=0.7/权重=1.0)，并设置 desired_speed=0.6、末端速度惩罚 0.5。
  - 执行效率阶段训练 train_safe_policy.py --config-name stage1_efficiency --disable-safety --episodes 400 --horizon 80，结果存于 outputs/efficiency_stage0_run3/；课程内部成功率波动，但离线评估 (tools/evaluate_policy.py --target-distance
    1.5/3.5) 仍为 0%，最终轨迹未能可靠到达目标。

  结论

  - 教师权重增大和目标距离缩短后，课程评估虽有短暂提升，但离线 200 次 rollouts 仍表现为失败，说明现有教师/损失尚不足以驱动策略稳定到点。
  - 下一步建议：
  

10.28早上

 调整执行概况

  - 扩展训练主流程：
      - TrainingConfig 新增教师控制、速度对齐、终点奖励、初始位姿等字段；compute_total_loss 加入教师蒸馏、速度对齐、末端速度惩罚、终点距离奖励。
      - sample_initial_state 支持自定义 XY／Z 范围，课程评估、鲁棒性评估同步使用。
      - CLI 与 stage1_efficiency 配置更新：目标距离课程改为 1.2→1.8→2.6 m，噪声收敛、教师增益调大、速度阈值/末端速度惩罚、终点奖励等均启用；初始位姿缩窄到 XY±0.15 m、Z∈[0.9,1.1]。
      - core/simple_training 增加终点距离硬惩罚；tools/evaluate_policy.py 可覆盖目标距离。
  - 运行效率阶段训练（outputs/efficiency_stage0_run4/，400 episode）。课程内部成功率略有提升，但离线 200 次 rollout 的成功率依旧为 0，说明策略仍未可靠抵达目标。

  现状与建议

  - 即便加入教师/奖励，策略仍停滞，需进一步调整：
      1. 显式位置奖励：在损失中直接加入末端欧氏距离（加权）以及沿目标方向的积分奖励，强化朝目标移动。
      2. 提升教师信号：使用明确的 PD 控制回放（带终点减速），或在前若干 episode 内直接模仿教师输出。
      3. 减少初始扰动：更强地限定初始速度=0、方向朝目标，并延长 horizon（100+）或降低动作正则，让策略有充足时间推进。
      4. 调高评估/训练一致性：在评估脚本中使用与课程相同的起点/噪声设定，观察单次 rollout 轨迹以定位失败原因。


更新内容

  - TrainingConfig 增加 trajectory_projection_weight 字段；compute_total_loss 新增沿目标方向速度积分奖励、终点距离奖励、更新日志及总损失组合。
  - core/simple_training 中 EfficiencyLossConfig 支持终点硬惩罚；tools/evaluate_policy.py 扩展可覆盖目标距离。
  - sample_initial_state 支持自定义初始范围；课程评估/鲁棒性评估同步使用。
  - configs/stage1_efficiency.py 现设置目标距离 1.2 → 1.8 → 2.6 m、噪声低起步、教师增益 1.5/0.7、速度期望 0.6 等。

  训练结果

  - outputs/efficiency_stage0_run5（400 episode，关闭安全层）完成；课程日志显示成功率阶段性提升，但离线 200 次评估成功率仍为 0，说明策略尚未真正学会朝目标推进。
  - 生成离线评估结果 outputs/efficiency_stage0_run5/eval_1p4.json。


调整与当前结果

  - 在效率训练损失中新增：
      - 沿目标方向运行的速度积分奖励（trajectory_projection_weight）。
      - 终点距离奖励与阶段距离惩罚（distance_bonus_weight、distance_tracking_weight）。
      - core/simple_training 引入终点距离硬惩罚。
  - 更新 TrainingConfig/CLI/课程配置（stage1_efficiency.py），缩小初始位置范围、增大教师增益、强化速度对齐与终点惩罚。
  - 运行 train_safe_policy.py --config-name stage1_efficiency --disable-safety --episodes 400 生成 outputs/efficiency_stage0_run6。课程评估中成功率有所提升，但离线评估（tools/evaluate_policy.py，200 rollouts）成功率仍为 0。



执行概况

  - 损失/课程调整：在效率阶段的损失中加入沿目标方向速度奖励、终点距离奖励、距离跟踪项；新增教师蒸馏超参与初始位置约束（缩窄 XY/Z 范围），强化前进信号。stage1_efficiency.py 配合更新（教师增益、距离课程、惩罚权重等）。
  - 训练运行：执行 train_safe_policy.py --config-name stage1_efficiency --disable-safety --episodes 400 --horizon 80 生成 outputs/efficiency_stage0_run6，阶段评估中成功率有小幅提升，关键损失已记录在训练日志。
  - 离线评估：tools/evaluate_policy.py 在 200 次 rollout、目标距 1.4 m 情况下仍 success_rate = 0，平均终点距离 6.25 m，说明策略未稳定学会到达目标。

  结论与建议

  - 当前显式奖励/教师加强不足以让策略在基础任务上稳定收敛；需进一步引入更强的基础约束或直接采用教师轨迹模仿（前期固定控制、后期恢复自主优化），并继续调整损失权重与课程设置。




10.28下午

整个团队开始重新审视整个项目

现有评估
  - 配置残留：default_config.py 仍携带多智能体与尚未实现的字段（如 optimization.use_scan），容易误导；memory_optimization.get_debug_config 写入 config.optimization 等字段，对依赖默认配置以外的场景会抛异常。
  - 安全层差距：safety_filter 只实现“qpax→裁剪回退”两级，未覆盖方法论强调的“线性近似→紧急制动”三段降级，也未对 qp 失败触发自动增大松弛/降低动作幅度。
  - CBF 训练：tools/pretrain_cbf.py 仅用解析 soft-min 监督，未采集 look-ahead / safe-unsafe 标签，更没有与策略的联合训练；compute_cbf_statistics 中对梯度/Hessian stop_gradient，策略无法利用安全层梯度指导，和“可微安全优先”不符。
  - 评估指标准备：tools/evaluate_policy.py 仅判定终点欧氏距离，未检测松弛/违约阈值；curriculum_summary.json 多个实验 eval/max_violation > 50 仍记为成功，严重背离“指标守门”。
  - 跨平台准备：requirements.txt 未给出 Kaggle/Windows 安装脚本且 qpax 目前仅官方支持 Linux；在 Windows 真机阶段需要明确替代求解器或提前编译说明。

核心问题
  - 效率阶段始终零成功率
      - 证据：outputs/efficiency_stage0_run6/eval_1p4.json 成功率 0，终点距离均值 6.25 m；训练日志第一轮 loss/grad_norm_policy≈4.9e4、eff/goal_loss≈3.2e4。
      - 根因： (1) DEFAULT_EFFICIENCY_CONFIG 中 goal_weight=16、final_goal_weight=80、z_axis_weight_multiplier=12 导致巨量梯度；(2) 训练时始终 clip_by_global_norm(grad_clip=1.0) 再乘上学习率 5e-5，每步权重变化 ≤5e-5，400 episode 下累计改变量
        不足 0.02，参数几乎不更新；(3) 单样本 BPTT + 无教师强制执行，使策略长期输出近零，轨迹停在原地但因 anisotropic 距离加权而产生巨额损失。
  - 安全阶段指标虚高
      - 证据：curriculum_summary.json 中 exp_curriculum_A/F 虽 clean/random_success_rate=1.0，却有 eval/max_violation 介于 52~64；loss/constraint_violation 仍 ~0。
      - 根因：评估时 success_tolerance=0.1 未与违约阈值联动，只看终点距离；compute_total_loss 中违约罚项 violation_penalty 仅统计软违约平均值，没有对最大违约或松弛使用率加大惩罚。
  - 方法论与实现脱节
      - GCBF+ 里的 Graph look-ahead、安全标签生成、QP 迭代监控均未落地，导致神经 CBF 在噪声课程中易失稳（日志证实 约束违背≈0.28）；
      - DiffPhys 的可微渲染、CNN+GRU 架构、随机并行环境尚未实现，策略仅是 MLP，难以在真实视觉输入上复现论文表现。
  - 工具链可靠性不足
      - utils/batch_pytree.generate_training_batch 生成的目标距离最高 0.3×min_target，使测试用例与真实课程距离分布不一致；
      - tools/run_safety_suite.py 默认命令在 Mac CPU 环境会因 qpax 未安装直接失败，目前未加检测/降级提示。

整体改进的建议
  - 效率阶段修复
      - 调参：将 grad_clip 提升至 ≥5 或先禁用裁剪，学习率恢复至 3e-4；goal_weight / final_goal_weight 至少同时缩放 10×，z_axis_weight_multiplier 降至 2~3，防止初始损失爆炸。
      - 训练策略：引入前 N episode 的“强制模仿”模式（直接注入 PD 动作或热启动策略），随后再开放优化；同时记录 rollout 轨迹到 artifacts/ 以直观检查是否朝目标运动。
      - 损失拆分：把 distance_tracking_weight 从均值平方距离改为沿目标方向的积分奖励，另行记录 goal_xy / goal_z 以便调试各向异性贡献。
  - 安全指标守门
      - 在 compute_total_loss 中追加 loss/max_violation 与 loss/relax_usage，训练时超过阈值直接增大松弛惩罚或提前终止；evaluate_policy.py 需把 violation_max、relax_mean 与阈值联动，若不达标直接判失败。
      - safety_filter 添加三段回退：qpax → 线性近似 → 紧急制动（反向加速度+垂直上升），并在诊断中记录触发次数，符合方法论。
  - 神经 CBF 强化
      - 补齐 look-ahead 蒙特卡洛生成：沿 GCBF+ 公式对未来 h 序列判定 safe / unsafe，构造 loss_safe / loss_unsafe；
      - 训练时将 CBF 梯度与策略损失联动（保留 grad_pos 的梯度通路，但限制为截断或权重缩放），真正实现“可微安全优先”。
  - 代码结构清理
      - 精简 default_config.py 中未使用的多智能体字段，并将 Mac/Windows/Kaggle 差异关键项（QPAX、JAX[XLA]安装方式）写成 README/脚本；
      - 为 utils/memory_optimization 添加字段存在性检查，避免在裁剪配置时访问不存在的 config.optimization。
  - 平台协同
      - Kaggle 训练：准备 environment.yml / pip install 脚本，区分 CPU/GPU；在 Windows 真机阶段，如无法使用 qpax，需评估 CasADi/OSQP 等替代或预编译二进制。
      - 建议在 Mac 上保持 CPU 冒烟测试（短 horizon），把长程训练在 Kaggle GPU 上执行，输出统一的 artifact 结构（模型、指标 JSON、轨迹 npy）便于团队共享


我们做了一个很重要的决策:

用可跨平台的自定义 QP 解法（解析/OSQP）替换 qpax，并在 JAX 侧实现 custom_vjp
  - 一致性与维护：继续沿用 qpax 意味着训练和部署要走两条安全层实现，Windows 端需要特殊兜底；一旦策略在 Windows 上表现异常，很难追踪到底是策略、硬件还是安全层引入误差。自研一个在 Mac/Kaggle/Windows 都能跑的安全层（解析闭式或 OSQP）可以让训练、
    验证、部署共用完全相同的代码路径，排查更容易，也避免未来 qpax 升级或平台支持变化带来的不确定性。
  - 实现可行性：当前 QP 规模仅 4 个决策变量（3 个控制 + 1 个松弛），约束结构固定，可以在 JAX 内部写出解析求解或极简 active-set，配合 jax.custom_vjp 直接保留梯度；或调用 OSQP、qpOASES 这类跨平台库，只在自定义 vjp 中传回我们期望的梯度（与现有
    safety_filter 一样，对 CBF 输入阻断梯度）。工作量集中在安全层模块，其他子系统无需改动。
  - 长期收益：摆脱 qpax 后，Windows、Kaggle、macOS 都能用同一套依赖，后续 CI/自动化评估也不会因为平台差异卡住；一旦需要扩展到真机部署或嵌入式，都可以继续沿用解析/OSQP 版本，兼顾实时与可靠性。


于是我们把“method.md”里的方法论也重新整改了一下



之后,我们对效率训练存在的问题进行深度的剖析:

问题定位

  - 损失权重极不平衡：DEFAULT_EFFICIENCY_CONFIG 将 goal_weight=16、final_goal_weight=80、z_axis_weight_multiplier=12；首个 episode 的 eff/goal_loss≈3.2e4，远超其他损失三个数量级，导致网络把大部分梯度耗在垂直方向。
  - 优化器几乎“锁死”：主循环采用 grad_clip=1.0 后再乘 policy_lr=5e-5，每步参数改变量 ≤5×10⁻⁵；400 episode（约 400 次迭代）累计更新不超过 0.02，策略基本保持初始化值。
  - 教师信号与课程脱节：虽然启用了 PD 教师，但其权重（teacher_weight=1.0）远小于主损失，且没有在训练初期强制执行“教师行动”；策略始终输出近零控制，环境得到的反馈为“原地不动”，对应损失又被巨权重放大，形成恶性循环。
  - 评估与训练不一致：离线评估固定目标距离 1.4 m，而课程阶段分别是 1.2→1.8→2.6 m；训练日志显示成功率提升的“阶段评估”是课程内部统计，离线评估仍按照 1.4 m 起点随机化，导致指标持续为 0。
  - 缺少轨迹级诊断：当前只保存损失标量，缺乏动作、位置随时间的曲线与离线回放，很难快速判断策略是否真的在移动。

解决方案

  - 重新设定损失权重
      - 将 goal_weight 调整至 2.0~4.0、final_goal_weight 调整至 5.0~10.0，z_axis_weight_multiplier 降至 2.0，防止垂直误差过度放大。
      - 将 distance_tracking_weight 改为沿目标方向的积分奖励（例如对 velocity_projection 使用 jnp.mean(jnn.relu(desired_speed - v_proj))），同时将 distance_bonus_weight 下调至 0.2~0.3。
      - 这些数值可写入新的 configs/stage1_efficiency.py 并在 outputs/… 新建 run 进行验证。
  - 放宽优化器超参
      - 将 policy_lr 恢复至 3e-4，grad_clip 提升到 5.0（或先关闭裁剪观察梯度分布）。
      - 增加 adamw 的 beta2（例如 0.95）以平滑大梯度；必要时启用 gradient_accumulation（例如每 4 次 rollouts 累积一次）减少噪声。
  - 阶段化教师强制
      - 在前 100~150 个 episode 内直接使用教师动作（或者使用 teacher_force_schedule=(1.0,0.5,0.0)），并记录实际误差；之后再逐步放开让策略优化。
      - 同时把 teacher_gain_p / teacher_gain_d 输出限制在物理上限内，保证轨迹可行。
  - 课程与评估对齐
      - 在 evaluate_policy.py 中允许指定阶段目标距离，并批量导出生效的指标（成功率、终点距离、最大违约）。
      - 或新增脚本 tools/evaluate_stage.py，对 stage1/2/3 逐一使用各自距离与噪声配置，确保离线评估与训练环境一致。
  - 增加轨迹调试
      - 在训练过程中每隔 N episode 保存一次 positions/u_safe 序列（可写入 outputs/.../traj_episode_<idx>.npz），并提供可视化脚本 tools/plot_efficiency_traj.py。
      - 结合 debug_step_by_step.py，挑选几组初始状态离线运行 5~10 次，肉眼检查是否朝目标推进。
      - 若发现策略仍停在原地，可直接在训练期引入小幅 reward shaping（例如朝目标方向投影的速度奖励追加到损失中）。


我们做出的调整内容

  - 下调效率损失中的关键权重：goal_weight→3.0、final_goal_weight→8.0、Z 轴权重乘子→2.0，避免垂直误差主导训练。
  - 将 distance_tracking_weight 作用对象改为目标方向速度缺口，distance_bonus_weight 降至 0.25，并保持轨迹投影奖励为较小正向系数。
  - 对应更新 configs/stage1_efficiency.py（关闭额外的 velocity_alignment_weight，重新设定奖励/惩罚权重），便于新的训练 run 直接生效。

短程结果

  - 使用新权重运行 40 episode 的效率训练（stage1_efficiency，关闭安全层）生成产物 outputs/sanity_stage1/training_results.pkl。
  - 日志显示总损失从 ~154 回落至 ~141，但单次短训后策略仍未接近目标：终点距离 10.2 m，平均距离 9.8 m，说明初步调整尚未带来明显正向行为。
  - 梯度范数约 9.9，确认梯度已不再被完全裁剪；但仅 40 个 episode 难以检验新超参。





我们进一步作出的调整内容

  - 使用新损失权重（goal_weight=3.0、final_goal_weight=8.0、z_axis_multiplier=2.0、distance_bonus_weight=0.25、distance_tracking_weight=1.0）重新跑了效率阶段 200 episode（--disable-safety、horizon=40）。
  - 训练产出保存在 ~/outputs/efficiency_sanity_run2/training_results.pkl，并在同目录生成了 3 组轨迹文件（trajectories/stage{1,2,3}_traj.npz，包含 positions、velocities、controls、target、init_position）。

结果观察

  - 损失仍在 140~180 区间震荡，loss/grad_norm_policy≈0（日志显示 0，原因是策略在前几百步全程冻结，见“问题”）。
  - 轨迹分析：阶段 1 rollout 的终点距离约 1.77 m（init≈[-0.07,0.09,1.05] → final≈[-0.48,0.55,1.46]，目标 [1.2,0,1.5]），仍未朝目标靠近，动作幅值平均仅 0.23，策略基本停留在低幅输出。

暴露的瓶颈

  - policy_freeze_steps 默认等于 stage1_steps=200，在效率阶段训练时整个策略被锁死；loss/grad_norm_policy 一直为 0。
  - 重新权重后损失规模已合理，但由于冻结策略，教师信号和新权重没机会起作用。





我们继续的调整内容:继续修改配置并重新跑带冻结修复的 run

结果:
重新训练设定：在效率阶段将 policy_freeze_steps 设为 0、policy_lr=3e-4、horizon=40、episode=200，保持调整后的损失权重。在 macOS CPU 环境（无安全层）运行，输出保存于 ~/outputs/efficiency_sanity_run3/（注意路径位于 ~/outputs/...，实际在工作
    目录下的 ./~/outputs/...）。
  - 损失走势：训练日志首尾对比显示 loss/total 由约 134 降至 72，loss/efficiency 同步下降；eval/success_rate 在阶段评估里由 0 提升至 1.0，说明策略学习到稳定推进。
  - 轨迹诊断：
      - 阶段 1（目标距离 1.2 m）：终点距目标 0.084 m，显著靠近。
      - 阶段 2（1.8 m）：终点距目标 0.230 m，保持可接受误差。
      - 阶段 3（2.6 m）：终点距目标 0.692 m，虽仍偏高，但已能沿目标方向接近，较之前停滞大幅改善。
        轨迹文件已记录在 ~/outputs/efficiency_sanity_run3/trajectories/stage{1,2,3}_traj.npz，含 positions、velocities、controls、target、init_position。




我们下一步:把刚刚这次跑的策略在 tools/evaluate_policy.py 中做 200 次离线验证

结果:
运行结果

  - 使用新配置（policy_freeze_steps=0, policy_lr=3e-4, 调整后的损失权重）重新训练效率阶段 200 episode，产物位于 ~/outputs/efficiency_sanity_run3/。
  - 训练曲线显示 loss/total 由约 134 下降至 ~72，阶段评估成功率在 40 episode 后迅速达到 1.0。

  轨迹检查

  - 在 ~/outputs/efficiency_sanity_run3/trajectories/ 保存了阶段 1~3 的轨迹数据（stageX_traj.npz，含位置、速度、控制、目标）。
      - Stage 1 终点距离目标 ≈ 0.084 m（明显接近目标）。
      - Stage 2 终点距目标 ≈ 0.230 m。
      - Stage 3 终点距目标 ≈ 0.692 m（仍有一定误差，但已不再停滞）。
  - 这些结果确认策略在训练环境中确实朝目标推进。

  离线评估

  - 使用 tools/evaluate_policy.py，在目标距离 1.2/1.8/2.6 m 上分别进行 200 次 rollouts，结果如下：
      - 所有场景的 success_rate 仍为 0，晋级阈值严格（默认成功判定需要距目标 <0.1 m），导致阶段 2/3 未达标。
      - 平均终点距离分别约 0.75 m、1.04 m、1.41 m，显示策略在课程中虽接近目标，但离线评估更苛刻，成功定义尚未满足。
  - 这表明：训练内的“成功”来自课程内部统计（可能使用更宽松的阈值或固定起点），离线评估需进一步对齐（例如将成功阈值调宽或改善策略末端控制）。





继续下一步:
  - 在 stage1_efficiency.py 中保持训练用的 goal_tolerance=0.2，并确保评估阈值与训练一致（仍使用相同配置，不再额外放宽）。
  - 在 tools/evaluate_policy.py 新增 --success-threshold 选项，评估时可指定成功判定的距离阈值（默认使用训练配置中的 success_tolerance），以便与训练标准对齐。
  - 在效率阶段训练中引入末端位置惩罚：在 TrainingConfig 新增 final_distance_weight，compute_total_loss 中增加终点距离平方损失，并记录到 loss/final_distance。
  - 更新 CLI 参数解析，支持 --final-distance-weight；在 stage1_efficiency 中设定 cfg.training.final_distance_weight = 2.5。
  - 使用新配置（horizon=80、policy_freeze_steps=0 等）重新训练 200 episode，产出保存于 ~/outputs/efficiency_sanity_run5/。

结果
  - 训练损失从约 160 下降至 ~75；阶段评估成功率逐步达到 1.0。
  - 轨迹 (~/outputs/efficiency_sanity_run5/trajectories/stage*.npz) 显示终点距离：阶段 1 ≈ 0.03 m、阶段 2 ≈ 0.22 m、阶段 3 ≈ 0.50 m，相比原方案（约 0.69 m）有所改善，但仍未进入 0.2 m 以内。
  - 离线评估 (evaluate_policy.py --success-threshold 0.2) 对阶段 3 仍给出 success_rate=0，反映终点误差仍超出 0.2 m；平均终点距离降至 ~0.45 m（较之前 0.69 m 有提升）。




继续下一步:

  - 扩大学习范围并加强末端约束：stage1_efficiency 中将每阶段长度提升至 300/300/300，缩小初始位置扰动（XY±0.08 m，Z∈[0.95,1.05]），增大教师控制增益（P=2.0/D=0.9，权重 1.2），并提高末端惩罚（final_velocity_weight=0.7、
    final_distance_weight=5.0）。
  - 重新训练效率阶段（episode=400, horizon=80, policy_lr=3e-4, policy_freeze_steps=0）→ 输出位于 ~/outputs/efficiency_sanity_run6/。
  - 保存阶段 1~3 的轨迹到 ~/outputs/efficiency_sanity_run6/trajectories/stage{1,2,3}_traj.npz。

结果

  - 训练损失从首批次约 450 下降至末尾 ~66，阶段评估成功率在 100 episode 后维持 1.0。
  - 轨迹终点误差显著降低：阶段 3 的终点距目标仅 ~0.18 m（阶段 2 约 0.021 m）。
  - 使用 tools/evaluate_policy.py --success-threshold 0.2 --target-distance 2.6 进行 200 次离线验证，success_rate=1.0，平均终点距离 ~0.153 m，满足严格的 0.2 m 阈值。




然后我们继续进行噪声鲁棒性复测

  - 使用 tools/evaluate_policy.py 对 ~/outputs/efficiency_sanity_run6/training_results.pkl 进行 200 次 rollout，参数：--noise 0.02、--random-point-cloud、--target-distance 2.6、--success-threshold 0.2、--seed 99。
  - 结果：success_rate=1.000，平均终点距离 ≈0.152 m（95% 分位 ≈0.164 m），无安全层松弛或违约触发。
  - 说明：即便在随机点云和噪声环境下，效率策略仍能稳定抵达目标，可作为后续安全层训练的可靠名义策略。

所以,目前的效率训练基线（efficiency_sanity_run6）已经在无噪声和有噪声（随机点云 + noise=0.02）两种情况下都达到了 100% 成功率，终点误差稳定维持在 0.2 米以内，我们认为足以支撑后续安全层训练。






接下来我们一起进入安全层的训练,在训练之前,我们先认真找寻之前安全层训练失败的根本原因,并且根据我们现在真实的情况,以及改变(解析/OSQP 版本),提出详细具体的安全层训练方案:
失败根源回顾

  - 名义策略不成熟：早期效率阶段成功率低，训练日志中 eval/success_rate 长期为 0，安全阶段在此基础上只能频繁触发松弛/紧急制动。
  - 成功判据不一致：训练内部使用 0.2 m 容差，离线评估固定 0.1 m，导致“训练成功、评估失败”的错判，安全阶段无法判断是否真正达标。
  - qpax 数值不稳定：qpax 在 macOS/CPU 下频繁返回 NaN，需要多次回退；当神经 CBF 尚未学会正确梯度时，qpax 触发松弛→松弛惩罚被放大→训练崩溃。
  - 神经 CBF 训练不足：tools/pretrain_cbf.py 只用解析 soft-min 监督，没有 look-ahead safe/unsafe 标签、也没对梯度/Hessian 做规范化，噪声课程中 loss/cbf_value、loss/cbf_hdot 波动大导致违约。
  - 守门指标缺失：旧版 tools/check_regression.py 只看违约/松弛，没有统计 QP 降级率、紧急制动率，安全层不稳定的现象被延迟发现。

  当前前提

  - 已获得鲁棒的效率基线：~/outputs/efficiency_sanity_run6 在随机点云 + noise=0.02 下成功率 100%，终点误差 <0.2 m。
  - 方法论已切换至“解析主动集 + OSQP 兜底 + 紧急制动”的跨平台安全层方案，并在 method.md 中明确。

  ———

  ### 安全层训练整体方案

  1. 安全层实现落地
      - 将 core/safety.py 中的 qpax 替换为解析主动集 + OSQP 求解，并通过 jax.custom_vjp 返回梯度；保留 solver_mode、solver_switch_count、emergency_count 等日志字段。
      - requirements.txt 添加 osqp；在 Windows/Mac 路径上测试 forward → backward。
      - 训练脚本中若检测到解析/OSQP 切换率 >1% 或紧急制动频率 >0.5%，立即提高松弛权重/缩小动作上限并报警。
  2. 神经 CBF 预训练强化
      - 更新 tools/pretrain_cbf.py：采集 look-ahead safe/unsafe 样本，构建 (h, ḣ, safe_label, unsafe_label)；对梯度/Hessian 做幅值裁剪，并记录 Lipschitz 约束。
      - 先在仿真数据集（多障碍、对抗场景）训练到 loss_safe/unsafe < 0.05、loss_hdot < 0.1。
      - 导出参数 cbf_pretrained_osqp.pkl，供安全阶段加载。
  3. 阶段化主训练
      - Stage A（解析 CBF）
          - 安全层只用解析 CBF，开启解析/OSQP 求解器，噪声关闭。
          - 目标：success_rate ≥ 0.95、solver_mode=analytic 占比 > 99%、max_violation < 10。
          - 失败则滚回效率策略或调节松弛。
      - Stage B（混合）
          - 逐步引入神经 CBF（blend_alpha 从 0.2→0.8），噪声 0.01。
          - 持续监控 solver_mode=osqp/emergency 的比例，若 >1% 则回退 blend 或提升预训练。
      - Stage C（全神经 + 噪声）
          - blend_alpha=1.0，噪声/随机点云开启（0.02~0.03）；评估 64/128 rollout 并记录 success_rate, max_violation, solver_emergency_rate。
          - 若指标符合 success_rate ≥ 0.9、max_violation ≤ 20、solver_emergency_rate ≤ 1% 即为训练成功。
  4. 评估与守门
      - tools/evaluate_policy.py 已接受 --success-threshold，可按训练容差（0.2 m）判定；需保留噪声/随机点云测试。
      - tools/check_regression.py 扩展字段：solver_emergency_rate、solver_switch_rate、final_distance_mean，若超阈值自动失败。
      - report_generate.py 追加安全层指标图（违约、松弛、紧急制动）。
  5. 运行流程
      - 将新策略 efficiency_sanity_run6 复制到项目 outputs/ 并改名 efficiency_baseline_osqp，更新训练脚本默认加载路径。
      - Stage A→C 按顺序运行，每阶段保存在单独目录（例如 safety_osqp_stageA、safety_osqp_stageB、safety_osqp_stageC）。
      - 每阶段结束后调用 tools/evaluate_policy.py（噪声/随机点云）和 tools/check_regression.py 做守门，再决定是否进入下一阶段。
  6. 跨平台验证
      - 在 Mac（CPU）、Kaggle（GPU）分别跑 Stage A 快速轮，确认解析/OSQP 解算器一致。

  - Windows 团队加载 Stage C 结果，检查解析/OSQP 日志是否稳定；若有紧急制动，记录场景落差。





  我们首先做的第一步:

• 已完成的调整

  - 安全层实现：将 core/safety.py 替换为“解析主动集 → OSQP 兜底 → 紧急制动”三段回退逻辑，并用 jax.custom_vjp 封装，保证梯度仅对名义控制传播。新增诊断字段 solver_mode、solver_status 等，便于统计解析/OSQP/紧急分支的使用情况。
  - 依赖更新：在 requirements.txt 中移除 qpax，加入 osqp，同时仍依赖 scipy。若尚未安装，可运行 pip install osqp。
  - 相关引用同步：修正 train_safe_policy.py、core/loop.py 中的注释，避免继续提及 qpax；其他代码无需改动即可兼容新的诊断结构。

  注意事项
  - 当前环境尚未安装 OSQP 时会自动回退到紧急制动路径，建议尽快完成 pip install osqp 以发挥解析/OSQP 分支的优势。
  - 可在后续训练中关注新增的 solver_mode 指标，确认解析或 OSQP 是否高频退化



