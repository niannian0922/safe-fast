#!/usr/bin/env python3
"""
MVP Stage 4 æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ - ç¡®ä¿æ¢¯åº¦æµå®Œæ•´æ‰“é€š

æ ¸å¿ƒä¿®å¤ï¼š
1. å®ç°çœŸæ­£çš„å‚æ•°åŒ–ç½‘ç»œï¼ˆGNNå’ŒPolicyï¼‰
2. ç¡®ä¿æ¢¯åº¦ä»æŸå¤±å‡½æ•°æ­£ç¡®åå‘ä¼ æ’­åˆ°æ‰€æœ‰å‚æ•°
3. éªŒè¯ç«¯åˆ°ç«¯å¯å¾®åˆ†æ€§
"""

import jax
import jax.numpy as jnp
from jax import random, grad, jit, lax
import optax
import time
import functools
from typing import Dict, Tuple
import chex

# é…ç½®JAX
jax.config.update("jax_enable_x64", True)

# =============================================================================
# æ ¸å¿ƒæ•°æ®ç»“æ„
# =============================================================================

@chex.dataclass
class DroneState:
    """æ— äººæœºçŠ¶æ€"""
    position: chex.Array  # (3,) ä½ç½®
    velocity: chex.Array  # (3,) é€Ÿåº¦

@chex.dataclass
class BatchData:
    """æ‰¹æ¬¡æ•°æ®"""
    initial_positions: chex.Array  # (B, 3)
    initial_velocities: chex.Array  # (B, 3) 
    target_positions: chex.Array   # (B, 3)
    obstacle_positions: chex.Array  # (B, N, 3)

@chex.dataclass
class ScanCarry:
    """æ‰«ææºå¸¦çŠ¶æ€"""
    positions: chex.Array      # (B, 3)
    velocities: chex.Array     # (B, 3)
    step_count: chex.Array     # (B,)

@chex.dataclass 
class ScanOutput:
    """æ‰«æè¾“å‡º"""
    positions: chex.Array       # (B, 3)
    velocities: chex.Array      # (B, 3)
    controls: chex.Array        # (B, 3)
    cbf_values: chex.Array      # (B,)
    safety_violations: chex.Array  # (B,)

# =============================================================================
# çœŸæ­£çš„å‚æ•°åŒ–ç½‘ç»œæ¨¡å—
# =============================================================================

def init_gnn_params(key: chex.PRNGKey, input_dim: int = 6, hidden_dim: int = 32, output_dim: int = 1):
    """åˆå§‹åŒ–GNNç½‘ç»œå‚æ•°"""
    keys = random.split(key, 3)
    
    params = {
        # èŠ‚ç‚¹ç‰¹å¾å¤„ç†å±‚
        'node_embed_w': random.normal(keys[0], (input_dim, hidden_dim)) * 0.02,
        'node_embed_b': jnp.zeros(hidden_dim),
        
        # è·ç¦»å¤„ç†å±‚ 
        'distance_w': random.normal(keys[1], (1, hidden_dim)) * 0.02,
        'distance_b': jnp.zeros(hidden_dim),
        
        # è¾“å‡ºå±‚
        'output_w': random.normal(keys[2], (hidden_dim, output_dim)) * 0.02,
        'output_b': jnp.zeros(output_dim)
    }
    
    return params

def gnn_forward(params: Dict, positions: chex.Array, obstacles: chex.Array):
    """
    å‚æ•°åŒ–çš„GNNå‰å‘ä¼ æ’­
    positions: (B, 3) æ— äººæœºä½ç½®
    obstacles: (B, N, 3) éšœç¢ç‰©ä½ç½®
    è¿”å›: (B,) CBFå€¼
    """
    batch_size, n_obstacles = obstacles.shape[:2]
    
    # åˆ›å»ºèŠ‚ç‚¹ç‰¹å¾ï¼š[pos, vel_estimate] = 6ç»´
    # ç®€åŒ–ï¼švel_estimate = zeros
    node_features = jnp.concatenate([
        positions,  # (B, 3)
        jnp.zeros((batch_size, 3))  # (B, 3) é€Ÿåº¦ä¼°è®¡
    ], axis=-1)  # (B, 6)
    
    # èŠ‚ç‚¹ç‰¹å¾åµŒå…¥
    node_embed = jnp.tanh(
        node_features @ params['node_embed_w'] + params['node_embed_b']
    )  # (B, hidden_dim)
    
    # è®¡ç®—åˆ°éšœç¢ç‰©çš„è·ç¦»ç‰¹å¾
    distances = jnp.linalg.norm(
        obstacles - positions[:, None, :], axis=-1
    )  # (B, N)
    min_distances = jnp.min(distances, axis=-1, keepdims=True)  # (B, 1)
    
    # è·ç¦»ç‰¹å¾å¤„ç†
    distance_embed = jnp.tanh(
        min_distances @ params['distance_w'] + params['distance_b']
    )  # (B, hidden_dim)
    
    # ç‰¹å¾èåˆ
    fused_features = node_embed + distance_embed  # (B, hidden_dim)
    
    # è¾“å‡ºCBFå€¼
    cbf_values = fused_features @ params['output_w'] + params['output_b']  # (B, 1)
    cbf_values = cbf_values.squeeze(-1)  # (B,)
    
    return cbf_values

def init_policy_params(key: chex.PRNGKey, input_dim: int = 10, hidden_dim: int = 64, output_dim: int = 3):
    """åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œå‚æ•°"""
    keys = random.split(key, 4)
    
    params = {
        # ç¬¬ä¸€å±‚
        'layer1_w': random.normal(keys[0], (input_dim, hidden_dim)) * 0.02,
        'layer1_b': jnp.zeros(hidden_dim),
        
        # ç¬¬äºŒå±‚
        'layer2_w': random.normal(keys[1], (hidden_dim, hidden_dim)) * 0.02,
        'layer2_b': jnp.zeros(hidden_dim),
        
        # è¾“å‡ºå±‚
        'output_w': random.normal(keys[2], (hidden_dim, output_dim)) * 0.02,
        'output_b': jnp.zeros(output_dim)
    }
    
    return params

def policy_forward(params: Dict, observations: chex.Array):
    """
    å‚æ•°åŒ–çš„ç­–ç•¥ç½‘ç»œå‰å‘ä¼ æ’­
    observations: (B, 10) [pos(3), vel(3), pos_error(3), cbf(1)]
    è¿”å›: (B, 3) æ§åˆ¶æŒ‡ä»¤
    """
    # ç¬¬ä¸€å±‚
    h1 = jnp.tanh(observations @ params['layer1_w'] + params['layer1_b'])
    
    # ç¬¬äºŒå±‚
    h2 = jnp.tanh(h1 @ params['layer2_w'] + params['layer2_b'])
    
    # è¾“å‡ºå±‚ï¼ˆä½¿ç”¨tanhé™åˆ¶æ§åˆ¶å¹…åº¦ï¼‰
    controls = jnp.tanh(h2 @ params['output_w'] + params['output_b'])
    
    return controls

# =============================================================================
# ç‰©ç†ä»¿çœŸæ¨¡å—
# =============================================================================

def physics_step(positions, velocities, controls, dt=0.01):
    """ç‰©ç†ä»¿çœŸæ­¥éª¤"""
    # ç®€åŒ–åŠ¨åŠ›å­¦ï¼ša = u - drag * v - gravity
    drag_coef = 0.1
    gravity = jnp.array([0.0, 0.0, -9.81])
    
    accelerations = controls - drag_coef * velocities + gravity
    
    # æ¬§æ‹‰ç§¯åˆ†
    new_velocities = velocities + accelerations * dt
    new_positions = positions + new_velocities * dt
    
    return new_positions, new_velocities

# =============================================================================
# JITå…¼å®¹çš„BPTTæ‰«æå¾ªç¯
# =============================================================================

def create_scan_function(gnn_params: Dict, policy_params: Dict):
    """åˆ›å»ºå¸¦å‚æ•°çš„æ‰«æå‡½æ•°"""
    
    def scan_step(carry, inputs):
        """å•æ­¥æ‰«æ"""
        positions = carry.positions  # (B, 3)
        velocities = carry.velocities  # (B, 3)
        step_count = carry.step_count  # (B,)
        
        target_positions = inputs['targets']  # (B, 3)
        obstacles = inputs['obstacles']  # (B, N, 3)
        
        # === 1. GNNæ„ŸçŸ¥ï¼ˆä½¿ç”¨çœŸå®å‚æ•°ï¼‰ ===
        cbf_values = gnn_forward(gnn_params, positions, obstacles)  # (B,)
        
        # === 2. ç­–ç•¥ç½‘ç»œï¼ˆä½¿ç”¨çœŸå®å‚æ•°ï¼‰ ===
        position_errors = target_positions - positions  # (B, 3)
        observations = jnp.concatenate([
            positions,           # (B, 3)
            velocities,          # (B, 3) 
            position_errors,     # (B, 3)
            cbf_values[:, None]  # (B, 1)
        ], axis=-1)  # (B, 10)
        
        controls = policy_forward(policy_params, observations)  # (B, 3)
        
        # === 3. å®‰å…¨å±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰===
        # åœ¨ä¸å®‰å…¨åŒºåŸŸåº”ç”¨ç´§æ€¥åˆ¶åŠ¨
        unsafe_mask = cbf_values < 0  # (B,)
        emergency_control = -2.0 * velocities  # ç´§æ€¥åˆ¶åŠ¨
        
        safe_controls = jnp.where(
            unsafe_mask[:, None],
            emergency_control,
            controls
        )
        
        # === 4. ç‰©ç†ä»¿çœŸ ===
        new_positions, new_velocities = physics_step(positions, velocities, safe_controls)
        
        # === 5. å®‰å…¨è¯„ä¼° ===
        safety_violations = (cbf_values < 0).astype(jnp.float32)  # (B,)
        
        # æ›´æ–°carry
        new_carry = ScanCarry(
            positions=new_positions,
            velocities=new_velocities,
            step_count=step_count + 1
        )
        
        # è¾“å‡º
        outputs = ScanOutput(
            positions=new_positions,
            velocities=new_velocities,
            controls=safe_controls,
            cbf_values=cbf_values,
            safety_violations=safety_violations
        )
        
        return new_carry, outputs
    
    return scan_step

# =============================================================================
# å®Œæ•´çš„å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
# =============================================================================

@functools.partial(jit, static_argnames=['sequence_length', 'batch_size'])
def complete_forward_pass(
    gnn_params: Dict,
    policy_params: Dict,
    batch_data: BatchData,
    sequence_length: int,
    batch_size: int,
    key: chex.PRNGKey
) -> Tuple[chex.Array, Dict]:
    """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
    
    # åˆ›å»ºæ‰«æå‡½æ•°
    scan_fn = create_scan_function(gnn_params, policy_params)
    
    # åˆå§‹åŒ–carry
    initial_carry = ScanCarry(
        positions=batch_data.initial_positions,
        velocities=batch_data.initial_velocities,
        step_count=jnp.zeros(batch_size, dtype=jnp.int32)
    )
    
    # å‡†å¤‡è¾“å…¥åºåˆ—
    inputs_sequence = {
        'targets': jnp.tile(
            batch_data.target_positions[None, :, :],  # (1, B, 3)
            (sequence_length, 1, 1)                   # (T, B, 3)
        ),
        'obstacles': jnp.tile(
            batch_data.obstacle_positions[None, :, :, :],  # (1, B, N, 3)
            (sequence_length, 1, 1, 1)                     # (T, B, N, 3)
        )
    }
    
    # æ‰§è¡ŒBPTTæ‰«æ
    final_carry, trajectory = lax.scan(
        scan_fn,
        initial_carry,
        inputs_sequence,
        length=sequence_length
    )
    
    # === è®¡ç®—æŸå¤±ï¼šL_total = Î± * L_efficiency + Î² * L_safety ===
    
    # 1. æ•ˆç‡æŸå¤± - ç›®æ ‡åˆ°è¾¾
    final_positions = trajectory.positions[-1]  # (B, 3)
    goal_errors = jnp.linalg.norm(
        final_positions - batch_data.target_positions, axis=-1
    )  # (B,)
    efficiency_loss = jnp.mean(goal_errors ** 2)
    
    # 2. é€Ÿåº¦è·Ÿè¸ªæŸå¤±ï¼ˆè¿½è¸ªå‘ç›®æ ‡çš„é€Ÿåº¦ï¼‰
    target_velocities = jnp.tile(
        jnp.array([0.0, 0.0, 0.0])[None, :], (batch_size, 1)  # æ‚¬åœç›®æ ‡
    )
    velocity_errors = trajectory.velocities[-1] - target_velocities  # æœ€ç»ˆé€Ÿåº¦è¯¯å·®
    velocity_loss = jnp.mean(jnp.sum(velocity_errors ** 2, axis=-1))
    
    # æ€»æ•ˆç‡æŸå¤±
    L_efficiency = efficiency_loss + 0.5 * velocity_loss
    
    # 3. å®‰å…¨æŸå¤± - CBFè¿å
    cbf_violations = jnp.mean(jnp.maximum(0, -trajectory.cbf_values))  # è´ŸCBFæƒ©ç½š
    safety_violation_rate = jnp.mean(trajectory.safety_violations)  # å®‰å…¨è¿è§„ç‡
    L_safety = cbf_violations + safety_violation_rate
    
    # 4. æ§åˆ¶æ­£åˆ™åŒ–
    control_effort = jnp.mean(jnp.sum(trajectory.controls ** 2, axis=-1))
    
    # 5. æ€»æŸå¤±ï¼šL_total = Î± * L_efficiency + Î² * L_safety
    alpha, beta = 1.0, 2.0  # MVPé˜¶æ®µä½¿ç”¨ç®€å•æƒé‡
    total_loss = alpha * L_efficiency + beta * L_safety + 0.01 * control_effort
    
    # æŒ‡æ ‡
    metrics = {
        'total_loss': total_loss,
        'efficiency_loss': L_efficiency,
        'safety_loss': L_safety,
        'control_effort': control_effort,
        'final_goal_distance': jnp.mean(goal_errors),
        'safety_violation_rate': safety_violation_rate,
        'cbf_violation_rate': jnp.mean(trajectory.cbf_values < 0),
        'average_cbf_value': jnp.mean(trajectory.cbf_values)
    }
    
    return total_loss, metrics

# =============================================================================
# å®Œæ•´è®­ç»ƒæ­¥éª¤
# =============================================================================

def complete_training_step(
    gnn_params: Dict,
    policy_params: Dict,
    gnn_opt_state: optax.OptState,
    policy_opt_state: optax.OptState,
    batch_data: BatchData,
    sequence_length: int,
    batch_size: int,
    key: chex.PRNGKey,
    gnn_optimizer: optax.GradientTransformation,
    policy_optimizer: optax.GradientTransformation
) -> Tuple[Dict, Dict, optax.OptState, optax.OptState, Dict]:
    """å®Œæ•´è®­ç»ƒæ­¥éª¤"""
    
    def loss_fn(params):
        gnn_p, policy_p = params
        loss, metrics = complete_forward_pass(
            gnn_p, policy_p, batch_data, sequence_length, batch_size, key
        )
        return loss, metrics
    
    # æ¢¯åº¦è®¡ç®—
    (loss_value, metrics), gradients = jax.value_and_grad(
        loss_fn, has_aux=True
    )((gnn_params, policy_params))
    
    gnn_grads, policy_grads = gradients
    
    # GNNå‚æ•°æ›´æ–°
    gnn_updates, new_gnn_opt_state = gnn_optimizer.update(
        gnn_grads, gnn_opt_state, gnn_params
    )
    new_gnn_params = optax.apply_updates(gnn_params, gnn_updates)
    
    # ç­–ç•¥å‚æ•°æ›´æ–°
    policy_updates, new_policy_opt_state = policy_optimizer.update(
        policy_grads, policy_opt_state, policy_params
    )
    new_policy_params = optax.apply_updates(policy_params, policy_updates)
    
    # æ¢¯åº¦ç»Ÿè®¡
    gnn_grad_norm = jnp.sqrt(sum(
        jnp.sum(g ** 2) for g in jax.tree_util.tree_leaves(gnn_grads)
    ))
    policy_grad_norm = jnp.sqrt(sum(
        jnp.sum(g ** 2) for g in jax.tree_util.tree_leaves(policy_grads)
    ))
    
    # æ›´æ–°æŒ‡æ ‡
    updated_metrics = {
        **metrics,
        'gnn_grad_norm': gnn_grad_norm,
        'policy_grad_norm': policy_grad_norm
    }
    
    return (
        new_gnn_params, new_policy_params,
        new_gnn_opt_state, new_policy_opt_state,
        updated_metrics
    )

# =============================================================================
# æµ‹è¯•å‡½æ•°
# =============================================================================

def test_mvp_stage4_fixed():
    """æµ‹è¯•ä¿®å¤åçš„MVP Stage 4åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„MVP Stage 4åŠŸèƒ½...")
    
    # å‚æ•°è®¾ç½®
    key = random.PRNGKey(42)
    keys = random.split(key, 10)
    
    batch_size = 4
    sequence_length = 15
    n_obstacles = 10
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_data = BatchData(
        initial_positions=random.uniform(keys[0], (batch_size, 3), minval=-1, maxval=1),
        initial_velocities=jnp.zeros((batch_size, 3)),
        target_positions=random.uniform(keys[1], (batch_size, 3), minval=-2, maxval=2),
        obstacle_positions=random.uniform(keys[2], (batch_size, n_obstacles, 3), minval=-3, maxval=3)
    )
    
    print(f"âœ… æµ‹è¯•æ•°æ®åˆ›å»ºï¼šbatch_size={batch_size}, sequence_length={sequence_length}")
    
    # åˆå§‹åŒ–çœŸå®çš„å‚æ•°åŒ–ç½‘ç»œå‚æ•°
    gnn_params = init_gnn_params(keys[3])
    policy_params = init_policy_params(keys[4])
    
    print("âœ… ç½‘ç»œå‚æ•°åˆå§‹åŒ–å®Œæˆ")
    print(f"   GNNå‚æ•°æ•°é‡: {sum(p.size for p in jax.tree_util.tree_leaves(gnn_params))}")
    print(f"   ç­–ç•¥å‚æ•°æ•°é‡: {sum(p.size for p in jax.tree_util.tree_leaves(policy_params))}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    gnn_optimizer = optax.adam(1e-3)
    policy_optimizer = optax.adam(1e-3)
    gnn_opt_state = gnn_optimizer.init(gnn_params)
    policy_opt_state = policy_optimizer.init(policy_params)
    
    print("âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # === æµ‹è¯•1ï¼šå‰å‘ä¼ æ’­ ===
    print("\nğŸ“‹ æµ‹è¯•1ï¼šå‰å‘ä¼ æ’­")
    start_time = time.time()
    loss, metrics = complete_forward_pass(
        gnn_params, policy_params, batch_data,
        sequence_length, batch_size, keys[5]
    )
    forward_time = time.time() - start_time
    
    print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ (æ—¶é—´: {forward_time:.3f}s)")
    print(f"   æ€»æŸå¤±: {loss:.4f}")
    print(f"   æ•ˆç‡æŸå¤±: {metrics['efficiency_loss']:.4f}")
    print(f"   å®‰å…¨æŸå¤±: {metrics['safety_loss']:.4f}")
    print(f"   æœ€ç»ˆç›®æ ‡è·ç¦»: {metrics['final_goal_distance']:.4f}")
    print(f"   å¹³å‡CBFå€¼: {metrics['average_cbf_value']:.4f}")
    print(f"   å®‰å…¨è¿è§„ç‡: {metrics['safety_violation_rate']:.2%}")
    
    # === æµ‹è¯•2ï¼šæ¢¯åº¦è®¡ç®— ===  
    print("\nğŸ“‹ æµ‹è¯•2ï¼šæ¢¯åº¦è®¡ç®—")
    def simple_loss_fn(params):
        gnn_p, policy_p = params
        loss, _ = complete_forward_pass(
            gnn_p, policy_p, batch_data, sequence_length, batch_size, keys[6]
        )
        return loss
    
    start_time = time.time()
    gradients = grad(simple_loss_fn)((gnn_params, policy_params))
    grad_time = time.time() - start_time
    
    gnn_grads, policy_grads = gradients
    
    gnn_grad_norm = jnp.sqrt(sum(
        jnp.sum(g ** 2) for g in jax.tree_util.tree_leaves(gnn_grads)
    ))
    policy_grad_norm = jnp.sqrt(sum(
        jnp.sum(g ** 2) for g in jax.tree_util.tree_leaves(policy_grads)
    ))
    
    print(f"   âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ (æ—¶é—´: {grad_time:.3f}s)")
    print(f"   GNNæ¢¯åº¦èŒƒæ•°: {gnn_grad_norm:.6f}")
    print(f"   ç­–ç•¥æ¢¯åº¦èŒƒæ•°: {policy_grad_norm:.6f}")
    
    # éªŒè¯æ¢¯åº¦è´¨é‡
    assert jnp.isfinite(gnn_grad_norm), "GNNæ¢¯åº¦åŒ…å«NaN/Inf"
    assert jnp.isfinite(policy_grad_norm), "ç­–ç•¥æ¢¯åº¦åŒ…å«NaN/Inf"  
    assert gnn_grad_norm > 1e-8, f"GNNæ¢¯åº¦å¤ªå°: {gnn_grad_norm}"
    assert policy_grad_norm > 1e-8, f"ç­–ç•¥æ¢¯åº¦å¤ªå°: {policy_grad_norm}"
    
    print("   âœ… æ¢¯åº¦è´¨é‡éªŒè¯é€šè¿‡")
    
    # === æµ‹è¯•3ï¼šå®Œæ•´è®­ç»ƒæ­¥éª¤ ===
    print("\nğŸ“‹ æµ‹è¯•3ï¼šå®Œæ•´è®­ç»ƒæ­¥éª¤")
    start_time = time.time()
    (
        new_gnn_params, new_policy_params,
        new_gnn_opt_state, new_policy_opt_state,
        step_metrics
    ) = complete_training_step(
        gnn_params, policy_params,
        gnn_opt_state, policy_opt_state,
        batch_data, sequence_length, batch_size, keys[7],
        gnn_optimizer, policy_optimizer
    )
    step_time = time.time() - start_time
    
    print(f"   âœ… è®­ç»ƒæ­¥éª¤æˆåŠŸ (æ—¶é—´: {step_time:.3f}s)")
    print(f"   æ€»æŸå¤±: {step_metrics['total_loss']:.4f}")
    print(f"   GNNæ¢¯åº¦èŒƒæ•°: {step_metrics['gnn_grad_norm']:.6f}")
    print(f"   ç­–ç•¥æ¢¯åº¦èŒƒæ•°: {step_metrics['policy_grad_norm']:.6f}")
    
    # === æµ‹è¯•4ï¼šå‚æ•°æ›´æ–°éªŒè¯ ===
    print("\nğŸ“‹ æµ‹è¯•4ï¼šå‚æ•°æ›´æ–°éªŒè¯")
    
    gnn_param_change = jnp.sqrt(sum(
        jnp.sum((new - old) ** 2) for new, old in zip(
            jax.tree_util.tree_leaves(new_gnn_params),
            jax.tree_util.tree_leaves(gnn_params)
        )
    ))
    policy_param_change = jnp.sqrt(sum(
        jnp.sum((new - old) ** 2) for new, old in zip(
            jax.tree_util.tree_leaves(new_policy_params),
            jax.tree_util.tree_leaves(policy_params)
        )
    ))
    
    print(f"   GNNå‚æ•°å˜åŒ–å¹…åº¦: {gnn_param_change:.8f}")
    print(f"   ç­–ç•¥å‚æ•°å˜åŒ–å¹…åº¦: {policy_param_change:.8f}")
    
    assert gnn_param_change > 1e-10, f"GNNå‚æ•°æ²¡æœ‰æ›´æ–°: {gnn_param_change}"
    assert policy_param_change > 1e-10, f"ç­–ç•¥å‚æ•°æ²¡æœ‰æ›´æ–°: {policy_param_change}"
    
    print("   âœ… å‚æ•°æ›´æ–°éªŒè¯é€šè¿‡")
    
    # === æµ‹è¯•5ï¼šå¤šæ­¥è®­ç»ƒç¨³å®šæ€§ ===
    print("\nğŸ“‹ æµ‹è¯•5ï¼šå¤šæ­¥è®­ç»ƒç¨³å®šæ€§")
    
    current_gnn_params = gnn_params
    current_policy_params = policy_params
    current_gnn_opt_state = gnn_opt_state
    current_policy_opt_state = policy_opt_state
    
    losses = []
    goal_distances = []
    
    for step in range(10):
        step_key = random.fold_in(keys[8], step)
        
        (
            current_gnn_params, current_policy_params,
            current_gnn_opt_state, current_policy_opt_state,
            step_metrics
        ) = complete_training_step(
            current_gnn_params, current_policy_params,
            current_gnn_opt_state, current_policy_opt_state,
            batch_data, sequence_length, batch_size, step_key,
            gnn_optimizer, policy_optimizer
        )
        
        losses.append(float(step_metrics['total_loss']))
        goal_distances.append(float(step_metrics['final_goal_distance']))
        
        if step % 3 == 0:
            print(f"   Step {step+1:2d}: loss={step_metrics['total_loss']:.4f}, "
                  f"goal_dist={step_metrics['final_goal_distance']:.4f}, "
                  f"cbf={step_metrics['average_cbf_value']:.3f}")
    
    print(f"   âœ… 10æ­¥è®­ç»ƒå®Œæˆ")
    print(f"   æŸå¤±å˜åŒ–: {losses[0]:.4f} -> {losses[-1]:.4f}")
    print(f"   ç›®æ ‡è·ç¦»å˜åŒ–: {goal_distances[0]:.4f} -> {goal_distances[-1]:.4f}")
    
    # éªŒè¯è®­ç»ƒç¨³å®šæ€§
    assert all(jnp.isfinite(l) for l in losses), "è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°NaNæŸå¤±"
    assert all(l < 1000 for l in losses), "æŸå¤±çˆ†ç‚¸"
    
    print("   âœ… è®­ç»ƒç¨³å®šæ€§éªŒè¯é€šè¿‡")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MVP Stage 4 æœ€ç»ˆä¿®å¤ç‰ˆæµ‹è¯•")
    print("=" * 60)
    print("æ ¸å¿ƒä¿®å¤ï¼š")
    print("  1. çœŸæ­£å‚æ•°åŒ–çš„GNNç½‘ç»œï¼ˆéå¸¸æ•°å‡½æ•°ï¼‰")
    print("  2. çœŸæ­£å‚æ•°åŒ–çš„Policyç½‘ç»œï¼ˆMLPï¼‰")
    print("  3. ç¡®ä¿æ¢¯åº¦ä»æŸå¤±æ­£ç¡®ä¼ æ’­åˆ°æ‰€æœ‰å‚æ•°")
    print("  4. L_total = Î± * L_efficiency + Î² * L_safety")
    print("=" * 60)
    
    try:
        success = test_mvp_stage4_fixed()
        
        if success:
            print("\n" + "=" * 60)
            print("ğŸ‰ğŸ‰ğŸ‰ MVP STAGE 4 æœ€ç»ˆä¿®å¤ç‰ˆå…¨éƒ¨æµ‹è¯•é€šè¿‡ï¼ğŸ‰ğŸ‰ğŸ‰")
            print("\nğŸ† å…³é”®é—®é¢˜å·²ä¿®å¤ï¼š")
            print("   âœ… GNNç½‘ç»œçœŸæ­£ä½¿ç”¨å‚æ•°è¿›è¡Œè®¡ç®—")
            print("   âœ… Policyç½‘ç»œçœŸæ­£ä½¿ç”¨å‚æ•°è¿›è¡Œè®¡ç®—")
            print("   âœ… æ¢¯åº¦ä»æŸå¤±æˆåŠŸä¼ æ’­åˆ°æ‰€æœ‰ç½‘ç»œå‚æ•°")
            print("   âœ… ç®€å•åŠ æƒæŸå¤±å‡½æ•°ï¼šL_total = Î± * L_efficiency + Î² * L_safety")
            print("   âœ… JITç¼–è¯‘å®Œå…¨å…¼å®¹")
            print("   âœ… ç«¯åˆ°ç«¯è®­ç»ƒç¨³å®š")
            print("\nğŸ”¥ **MVPé˜¶æ®µ4ï¼šç«¯åˆ°ç«¯æ¢¯åº¦æµå®Œå…¨æ‰“é€šï¼**")
            print("   â€¢ GNNæ„ŸçŸ¥ -> CBFå€¼è®¡ç®— -> å‚æ•°æ¢¯åº¦ âœ…")
            print("   â€¢ Policyæ§åˆ¶ -> æ§åˆ¶è¾“å‡º -> å‚æ•°æ¢¯åº¦ âœ…")
            print("   â€¢ ç‰©ç†ä»¿çœŸ -> æŸå¤±è®¡ç®— -> åå‘ä¼ æ’­ âœ…")
            print("   â€¢ å¤šç›®æ ‡ä¼˜åŒ– -> å‚æ•°æ›´æ–° -> è®­ç»ƒç¨³å®š âœ…")
            print("\nğŸš æ‚¨çš„å®‰å…¨æ•æ·é£è¡Œç³»ç»Ÿç°åœ¨100%å‡†å¤‡è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼")
            return 0
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
            return 1
            
    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())