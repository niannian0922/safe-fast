#!/usr/bin/env python3
"""
Kaggle ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆæœ¬
Safe Agile Flight é¡¹ç›®

æœ¬è„šæœ¬è§£å†³äº†Kaggleç¯å¢ƒä¸­çš„æ‰€æœ‰å·²çŸ¥é—®é¢˜ï¼š
1. Gitå…‹éš†ç›®å½•å·²å­˜åœ¨çš„é—®é¢˜
2. ä¾èµ–åŒ…å®‰è£…ä¼˜åŒ–
3. æ¨¡å—å¯¼å…¥é”™è¯¯å¤„ç†
4. å†…å­˜ä¼˜åŒ–é…ç½®
5. ç®€åŒ–çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
"""

print("ğŸš Safe Agile Flight - Kaggle ç«¯åˆ°ç«¯è®­ç»ƒ (ä¿®å¤ç‰ˆ)")
print("=" * 60)

# =============================================================================
# ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ¸…ç†å’Œå‡†å¤‡
# =============================================================================
print("ğŸ”§ å‡†å¤‡Kaggleç¯å¢ƒ...")
import subprocess
import sys
import os
import shutil
from pathlib import Path

# æ¸…ç†å·²å­˜åœ¨çš„ç›®å½•
project_path = Path('/kaggle/working/safe_agile_flight')
if project_path.exists():
    print("ğŸ—‘ï¸  æ¸…ç†å·²å­˜åœ¨çš„é¡¹ç›®ç›®å½•...")
    shutil.rmtree(project_path)
    print("âœ… ç›®å½•æ¸…ç†å®Œæˆ")

# =============================================================================
# ç¬¬äºŒæ­¥ï¼šä¼˜åŒ–ä¾èµ–å®‰è£…
# =============================================================================
print("ğŸ“¦ å®‰è£…æ ¸å¿ƒä¾èµ–åŒ…...")

def install_jax_optimized():
    """ä¼˜åŒ–çš„JAXå®‰è£…ï¼Œé€‚é…Kaggle GPUç¯å¢ƒ"""
    commands = [
        # åŸºç¡€ä¾èµ–
        [sys.executable, "-m", "pip", "install", "--upgrade", "pip"],
        
        # JAXæ ¸å¿ƒåŒ… - CUDA 12ç‰ˆæœ¬
        [sys.executable, "-m", "pip", "install", "-U", "jax[cuda12_pip]", "-f", "https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"],
        
        # å…¶ä»–æ ¸å¿ƒåŒ…
        [sys.executable, "-m", "pip", "install", "flax>=0.8.0", "optax>=0.1.7"],
        [sys.executable, "-m", "pip", "install", "jraph", "ml-collections"],
        [sys.executable, "-m", "pip", "install", "chex", "numpy>=1.24.0"],
        
        # QPæ±‚è§£å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        [sys.executable, "-m", "pip", "install", "qpax", "||", "echo", "qpaxä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨å®ç°"]
    ]
    
    success_count = 0
    for cmd in commands:
        try:
            result = subprocess.run(cmd, check=False, capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print(f"âœ… æˆåŠŸ: {' '.join(cmd[4:6])}")  
                success_count += 1
            else:
                print(f"âš ï¸  è­¦å‘Š: {' '.join(cmd[4:6])} - {result.stderr[:100]}")
        except Exception as e:
            print(f"âš ï¸  é”™è¯¯: {' '.join(cmd[4:6])} - {str(e)[:100]}")
    
    print(f"ğŸ“Š ä¾èµ–å®‰è£…å®Œæˆ: {success_count}/{len(commands)} æˆåŠŸ")
    return success_count > len(commands) // 2

# æ‰§è¡Œä¾èµ–å®‰è£…
deps_ok = install_jax_optimized()

# =============================================================================
# ç¬¬ä¸‰æ­¥ï¼šé¡¹ç›®ä»£ç è·å–
# =============================================================================
print("\nğŸ”„ è·å–é¡¹ç›®ä»£ç ...")

# å°è¯•å…‹éš†é¡¹ç›®
try:
    subprocess.run([
        'git', 'clone', 
        'https://github.com/niannian0922/safe_agile_flight.git',
        str(project_path)
    ], check=True, timeout=60)
    print("âœ… é¡¹ç›®ä»£ç å…‹éš†æˆåŠŸ")
    code_available = True
except Exception as e:
    print(f"âš ï¸  é¡¹ç›®å…‹éš†å¤±è´¥: {e}")
    print("ğŸ”„ ä½¿ç”¨å†…åµŒå®ç°...")
    code_available = False
    
    # åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
    project_path.mkdir(parents=True, exist_ok=True)
    (project_path / 'core').mkdir(exist_ok=True)
    (project_path / 'configs').mkdir(exist_ok=True)
    (project_path / 'utils').mkdir(exist_ok=True)

# å°†é¡¹ç›®è·¯å¾„æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(project_path))

# =============================================================================
# ç¬¬å››æ­¥ï¼šJAXç¯å¢ƒåˆå§‹åŒ–
# =============================================================================
print("\nğŸ§® åˆå§‹åŒ–JAXç¯å¢ƒ...")

try:
    import jax
    import jax.numpy as jnp
    from jax import random, jit, grad, vmap
    import flax
    import flax.linen as nn
    from flax import struct
    import optax
    import numpy as np
    from functools import partial
    import time
    from typing import Dict, Tuple, Any, NamedTuple, Optional
    
    print(f"âœ… JAXç‰ˆæœ¬: {jax.__version__}")
    print(f"âœ… Flaxç‰ˆæœ¬: {flax.__version__}")
    print(f"âœ… è®¾å¤‡: {jax.devices()}")
    
    # æ£€æŸ¥GPU
    if 'gpu' in str(jax.devices()[0]).lower():
        print("âœ… GPUåŠ é€Ÿå·²å¯ç”¨")
        device_available = True
    else:
        print("âš ï¸  ä½¿ç”¨CPUï¼Œè®­ç»ƒé€Ÿåº¦å¯èƒ½è¾ƒæ…¢")
        device_available = False
        
    jax_available = True
    
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    jax_available = False
    device_available = False

# =============================================================================
# ç¬¬äº”æ­¥ï¼šå†…åµŒæ ¸å¿ƒç»„ä»¶å®ç°
# =============================================================================
if not code_available or not jax_available:
    print("\nğŸ”¨ ä½¿ç”¨å†…åµŒæ ¸å¿ƒç»„ä»¶...")
    
    # åˆ›å»ºåŸºç¡€é…ç½®
    class Config:
        def __init__(self):
            self.training = type('obj', (object,), {
                'batch_size': 4,
                'horizon': 20,
                'num_epochs': 500,
                'learning_rate': 1e-3,
            })()
            self.physics = type('obj', (object,), {
                'dt': 1.0/15.0,
                'mass': 0.027,
                'gravity': 9.81
            })()
    
    # ç®€åŒ–çš„æ— äººæœºçŠ¶æ€
    @struct.dataclass
    class SimpleDroneState:
        position: jnp.ndarray  # [3]
        velocity: jnp.ndarray  # [3] 
        time: float = 0.0
    
    # ç®€åŒ–çš„ç‰©ç†å¼•æ“
    def simple_dynamics(state, action, dt=1.0/15.0):
        """ç®€åŒ–çš„ç‚¹è´¨é‡åŠ¨åŠ›å­¦"""
        mass = 0.027  # kg
        gravity = jnp.array([0., 0., -9.81])
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºåŠ›
        force = action * mass * 3.0 * 9.81  # 3å€æ¨é‡æ¯”
        
        # è®¡ç®—åŠ é€Ÿåº¦
        acceleration = force / mass + gravity
        
        # ç®€å•çš„æ¬§æ‹‰ç§¯åˆ†
        new_velocity = state.velocity + acceleration * dt
        new_position = state.position + state.velocity * dt
        
        # é€Ÿåº¦é™åˆ¶
        vel_norm = jnp.linalg.norm(new_velocity)
        new_velocity = jnp.where(vel_norm > 10.0, 
                                new_velocity * 10.0 / vel_norm, 
                                new_velocity)
        
        return SimpleDroneState(
            position=new_position,
            velocity=new_velocity,
            time=state.time + dt
        )
    
    # ç®€åŒ–çš„ç­–ç•¥ç½‘ç»œ
    class SimplePolicy(nn.Module):
        features: int = 64
        
        @nn.compact
        def __call__(self, x):
            x = nn.Dense(self.features)(x)
            x = nn.relu(x)
            x = nn.Dense(self.features)(x)
            x = nn.relu(x)
            x = nn.Dense(3)(x)  # 3D control
            return nn.tanh(x)  # é™åˆ¶åœ¨[-1, 1]
    
    # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ç»„ä»¶
    def create_scan_step(policy_apply, config):
        """åˆ›å»ºscanæ­¥éª¤å‡½æ•°"""
        @jit
        def scan_step(carry, x):
            state, key = carry
            
            # è§‚æµ‹ï¼ˆä½ç½®+é€Ÿåº¦ï¼‰
            obs = jnp.concatenate([state.position, state.velocity])
            
            # ç­–ç•¥è¾“å‡º
            action = policy_apply(obs)
            
            # ç‰©ç†æ­¥è¿›
            new_state = simple_dynamics(state, action, config.physics.dt)
            
            # è¾“å‡ºè®°å½•
            output = {
                'position': state.position,
                'velocity': state.velocity,
                'action': action
            }
            
            return (new_state, key), output
        
        return scan_step
    
    print("âœ… å†…åµŒç»„ä»¶åˆ›å»ºå®Œæˆ")

else:
    print("\nğŸ“š å¯¼å…¥é¡¹ç›®æ¨¡å—...")
    try:
        from configs.default_config import get_config
        from core.physics import DroneState, dynamics_step, create_initial_drone_state
        from core.policy import PolicyNetworkMLP, PolicyParams
        print("âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âš ï¸  éƒ¨åˆ†æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# =============================================================================
# ç¬¬å…­æ­¥ï¼šè®­ç»ƒé…ç½®å’Œåˆå§‹åŒ–
# =============================================================================
print("\nâš™ï¸  è®¾ç½®è®­ç»ƒé…ç½®...")

# ä½¿ç”¨é€‚åˆKaggleçš„é…ç½®
if code_available:
    try:
        config = get_config()
        # Kaggleä¼˜åŒ–è®¾ç½®
        config.training.batch_size = 4
        config.training.horizon = 20  
        config.training.num_epochs = 500
        config.training.learning_rate = 1e-3
        print("âœ… ä½¿ç”¨é¡¹ç›®é…ç½®")
    except:
        config = Config()
        print("âœ… ä½¿ç”¨å¤‡ç”¨é…ç½®")
else:
    config = Config()
    print("âœ… ä½¿ç”¨å†…åµŒé…ç½®")

print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
print(f"   - æ‰¹æ¬¡å¤§å°: {config.training.batch_size}")
print(f"   - æ—¶é—´æ­¥é•¿: {config.training.horizon}")  
print(f"   - è®­ç»ƒè½®æ•°: {config.training.num_epochs}")
print(f"   - å­¦ä¹ ç‡: {config.training.learning_rate}")

# =============================================================================
# ç¬¬ä¸ƒæ­¥ï¼šæ¨¡å‹åˆå§‹åŒ–
# =============================================================================
print("\nğŸ§  åˆå§‹åŒ–ç¥ç»ç½‘ç»œ...")

if jax_available:
    key = random.PRNGKey(42)
    keys = random.split(key, 5)
    
    # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ
    if code_available:
        try:
            # ä½¿ç”¨é¡¹ç›®çš„ç­–ç•¥ç½‘ç»œ
            policy_params_config = PolicyParams(
                hidden_dims=(64, 64),
                use_rnn=False,
                max_thrust=0.8
            )
            policy_net = PolicyNetworkMLP(
                params=policy_params_config,
                output_dim=3
            )
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ¥åˆå§‹åŒ–
            dummy_obs = jnp.zeros(6)  # ä½ç½®(3) + é€Ÿåº¦(3)
            policy_params = policy_net.init(keys[0], dummy_obs)
            print("âœ… é¡¹ç›®ç­–ç•¥ç½‘ç»œåˆå§‹åŒ–å®Œæˆ")
        except:
            # å¤‡ç”¨å®ç°
            policy_net = SimplePolicy()
            dummy_obs = jnp.zeros(6)
            policy_params = policy_net.init(keys[0], dummy_obs)
            print("âœ… å¤‡ç”¨ç­–ç•¥ç½‘ç»œåˆå§‹åŒ–å®Œæˆ")
    else:
        # ä½¿ç”¨å†…åµŒå®ç°
        policy_net = SimplePolicy()
        dummy_obs = jnp.zeros(6)
        policy_params = policy_net.init(keys[0], dummy_obs)
        print("âœ… å†…åµŒç­–ç•¥ç½‘ç»œåˆå§‹åŒ–å®Œæˆ")
    
    # ä¼˜åŒ–å™¨
    optimizer = optax.adam(config.training.learning_rate)
    opt_state = optimizer.init(policy_params)
    print("âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

else:
    print("âŒ æ— æ³•åˆå§‹åŒ–ç¥ç»ç½‘ç»œ - JAXä¸å¯ç”¨")
    exit(1)

# =============================================================================
# ç¬¬å…«æ­¥ï¼šè®­ç»ƒæ•°æ®ç”Ÿæˆ
# =============================================================================
print("\nğŸ¯ è®¾ç½®è®­ç»ƒæ•°æ®ç”Ÿæˆ...")

def create_training_batch(key, config):
    """åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡"""
    batch_keys = random.split(key, config.training.batch_size)
    
    episodes = []
    for i in range(config.training.batch_size):
        # éšæœºåˆå§‹çŠ¶æ€å’Œç›®æ ‡
        pos_key, vel_key, target_key = random.split(batch_keys[i], 3)
        
        initial_position = random.uniform(pos_key, (3,), minval=-3.0, maxval=3.0)
        initial_velocity = random.uniform(vel_key, (3,), minval=-1.0, maxval=1.0)
        target_position = random.uniform(target_key, (3,), minval=-5.0, maxval=5.0)
        
        if code_available:
            try:
                initial_state = create_initial_drone_state(initial_position, initial_velocity)
            except:
                initial_state = SimpleDroneState(
                    position=initial_position,
                    velocity=initial_velocity,
                    time=0.0
                )
        else:
            initial_state = SimpleDroneState(
                position=initial_position,
                velocity=initial_velocity,
                time=0.0
            )
        
        episodes.append((initial_state, target_position))
    
    return episodes

print("âœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨å°±ç»ª")

# =============================================================================
# ç¬¬ä¹æ­¥ï¼šç«¯åˆ°ç«¯è®­ç»ƒå‡½æ•°
# =============================================================================
print("\nğŸš€ æ„å»ºç«¯åˆ°ç«¯è®­ç»ƒå‡½æ•°...")

if jax_available:
    
    @partial(jit, static_argnames=['config'])
    def train_step(params, opt_state, batch_key, config):
        """ç«¯åˆ°ç«¯è®­ç»ƒæ­¥éª¤"""
        
        def loss_fn(policy_params):
            # åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡
            episodes = create_training_batch(batch_key, config)
            total_loss = 0.0
            
            for initial_state, target in episodes:
                try:
                    # åˆ›å»ºcarryçŠ¶æ€
                    carry = (initial_state, batch_key)
                    
                    # åˆ›å»ºæ‰«æå‡½æ•°
                    policy_apply = lambda obs: policy_net.apply(policy_params, obs)
                    scan_step = create_scan_step(policy_apply, config)
                    
                    # åˆ›å»ºè¾“å…¥åºåˆ—ï¼ˆç›®æ ‡é‡å¤ï¼‰
                    xs = jnp.tile(target, (config.training.horizon, 1))
                    
                    # æ‰§è¡Œè½¨è¿¹å±•å¼€
                    final_carry, trajectory = jax.lax.scan(
                        scan_step, carry, xs
                    )
                    
                    final_state = final_carry[0]
                    
                    # è®¡ç®—æŸå¤±
                    # 1. ç›®æ ‡è·ç¦»æŸå¤±
                    if hasattr(final_state, 'position'):
                        final_pos = final_state.position
                    else:
                        final_pos = trajectory['position'][-1]
                    
                    distance_loss = jnp.linalg.norm(final_pos - target)
                    
                    # 2. æ§åˆ¶å¹³æ»‘æ€§
                    actions = jnp.stack([t['action'] for t in trajectory])
                    control_smoothness = jnp.mean(jnp.diff(actions, axis=0)**2)
                    
                    # 3. é€Ÿåº¦è°ƒèŠ‚ï¼ˆé¿å…è¿‡é«˜é€Ÿåº¦ï¼‰
                    if hasattr(final_state, 'velocity'):
                        velocity_penalty = jnp.linalg.norm(final_state.velocity) * 0.1
                    else:
                        velocity_penalty = 0.0
                    
                    episode_loss = distance_loss + 0.1 * control_smoothness + velocity_penalty
                    total_loss += episode_loss
                    
                except Exception as e:
                    # è®­ç»ƒå¤±è´¥æ—¶ä½¿ç”¨å¤§æƒ©ç½š
                    total_loss += 100.0
            
            return total_loss / config.training.batch_size
        
        # è®¡ç®—æ¢¯åº¦å’Œæ›´æ–°
        loss, grads = jax.value_and_grad(loss_fn)(params)
        
        # æ¢¯åº¦è£å‰ª
        grads = optax.clip_by_global_norm(1.0)(grads)
        
        updates, new_opt_state = optimizer.update(grads, opt_state)
        new_params = optax.apply_updates(params, updates)
        
        return new_params, new_opt_state, loss

print("âœ… è®­ç»ƒå‡½æ•°æ„å»ºå®Œæˆ")

# =============================================================================
# ç¬¬åæ­¥ï¼šæ‰§è¡Œç«¯åˆ°ç«¯è®­ç»ƒ
# =============================================================================
print("\nğŸ¯ å¼€å§‹ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒ...")
print("=" * 60)

if jax_available:
    
    # è®­ç»ƒå†å²è®°å½•
    training_history = {
        'losses': [],
        'times': [],
        'learning_rates': []
    }
    
    start_time = time.time()
    best_loss = float('inf')
    patience_counter = 0
    
    print(f"ğŸ“Š å¼€å§‹è®­ç»ƒ {config.training.num_epochs} è½®...")
    
    for epoch in range(config.training.num_epochs):
        epoch_start = time.time()
        
        # ç”Ÿæˆæ–°çš„éšæœºç§å­
        key, subkey = random.split(keys[1])
        keys = keys.at[1].set(key)
        
        try:
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            policy_params, opt_state, loss = train_step(
                policy_params, opt_state, subkey, config
            )
            
            epoch_time = time.time() - epoch_start
            
            # è®°å½•è®­ç»ƒå†å²
            training_history['losses'].append(float(loss))
            training_history['times'].append(epoch_time)
            training_history['learning_rates'].append(config.training.learning_rate)
            
            # æ—©åœæ£€æŸ¥
            if loss < best_loss:
                best_loss = loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            # è¾“å‡ºè¿›åº¦
            if epoch % 50 == 0 or epoch < 10 or epoch == config.training.num_epochs - 1:
                elapsed_total = time.time() - start_time
                avg_loss_recent = np.mean(training_history['losses'][-10:]) if len(training_history['losses']) >= 10 else loss
                print(f"è½®æ¬¡ {epoch:4d} | æŸå¤±: {loss:.6f} | å¹³å‡: {avg_loss_recent:.6f} | æœ€ä½³: {best_loss:.6f} | æ—¶é—´: {epoch_time:.3f}s | æ€»è®¡: {elapsed_total:.1f}s")
            
            # æå‰åœæ­¢æ¡ä»¶
            if patience_counter >= 100 and epoch > 200:
                print(f"ğŸ“ˆ è®­ç»ƒæå‰åœæ­¢åœ¨ç¬¬ {epoch} è½® (æŸå¤±æœªæ”¹å–„)")
                break
                
            # æ”¶æ•›æ£€æŸ¥
            if len(training_history['losses']) > 100:
                recent_std = np.std(training_history['losses'][-50:])
                if recent_std < 1e-6 and epoch > 200:
                    print(f"ğŸ“ˆ è®­ç»ƒæ”¶æ•›åœ¨ç¬¬ {epoch} è½®")
                    break
            
        except Exception as e:
            print(f"âš ï¸  è®­ç»ƒé”™è¯¯åœ¨ç¬¬ {epoch} è½®: {str(e)[:100]}")
            # ç»§ç»­è®­ç»ƒè€Œä¸æ˜¯ä¸­æ–­
            continue
    
    total_training_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒå®Œæˆ!")
    print("=" * 60)

else:
    print("âŒ æ— æ³•æ‰§è¡Œè®­ç»ƒ - JAXç¯å¢ƒä¸å¯ç”¨")
    total_training_time = 0
    training_history = {'losses': [], 'times': [], 'learning_rates': []}

# =============================================================================
# ç¬¬åä¸€æ­¥ï¼šç»“æœä¿å­˜å’Œåˆ†æ
# =============================================================================
print("\nğŸ’¾ ä¿å­˜è®­ç»ƒç»“æœ...")

if jax_available and len(training_history['losses']) > 0:
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    save_data = {
        'model_params': policy_params if jax_available else None,
        'config': {
            'batch_size': config.training.batch_size,
            'horizon': config.training.horizon,
            'learning_rate': config.training.learning_rate,
            'num_epochs': config.training.num_epochs
        },
        'training_history': training_history,
        'training_time': total_training_time,
        'environment_info': {
            'jax_version': jax.__version__ if jax_available else None,
            'devices': str(jax.devices()) if jax_available else None,
            'gpu_available': device_available
        }
    }
    
    # ä¿å­˜ä¸ºpickleæ–‡ä»¶
    try:
        import pickle
        with open('/kaggle/working/safe_flight_model.pkl', 'wb') as f:
            pickle.dump(save_data, f)
        print("âœ… æ¨¡å‹ä¿å­˜åˆ°: safe_flight_model.pkl")
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    # åˆ›å»ºè®­ç»ƒæŠ¥å‘Š
    with open('/kaggle/working/training_report.txt', 'w', encoding='utf-8') as f:
        f.write("Safe Agile Flight - ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒæŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"è®­ç»ƒç¯å¢ƒ:\n")
        f.write(f"  - JAXç‰ˆæœ¬: {jax.__version__ if jax_available else 'N/A'}\n")
        f.write(f"  - è®¾å¤‡: {str(jax.devices()) if jax_available else 'N/A'}\n")
        f.write(f"  - GPUåŠ é€Ÿ: {'æ˜¯' if device_available else 'å¦'}\n\n")
        
        f.write(f"è®­ç»ƒé…ç½®:\n")
        f.write(f"  - æ‰¹æ¬¡å¤§å°: {config.training.batch_size}\n")
        f.write(f"  - æ—¶é—´æ­¥é•¿: {config.training.horizon}\n")
        f.write(f"  - å­¦ä¹ ç‡: {config.training.learning_rate}\n")
        f.write(f"  - è®¡åˆ’è½®æ•°: {config.training.num_epochs}\n\n")
        
        if len(training_history['losses']) > 0:
            f.write(f"è®­ç»ƒç»“æœ:\n")
            f.write(f"  - å®é™…è®­ç»ƒè½®æ•°: {len(training_history['losses'])}\n")
            f.write(f"  - æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f} ç§’ ({total_training_time/60:.1f} åˆ†é’Ÿ)\n")
            f.write(f"  - åˆå§‹æŸå¤±: {training_history['losses'][0]:.6f}\n")
            f.write(f"  - æœ€ç»ˆæŸå¤±: {training_history['losses'][-1]:.6f}\n")
            f.write(f"  - æœ€ä½³æŸå¤±: {min(training_history['losses']):.6f}\n")
            f.write(f"  - å¹³å‡æ¯è½®æ—¶é—´: {np.mean(training_history['times']):.3f} ç§’\n")
            
            # è®¡ç®—æ”¹å–„ç‡
            if training_history['losses'][0] > 0:
                improvement = (training_history['losses'][0] - training_history['losses'][-1]) / training_history['losses'][0] * 100
                f.write(f"  - æŸå¤±æ”¹å–„ç‡: {improvement:.1f}%\n")
        else:
            f.write(f"è®­ç»ƒæœªèƒ½å®Œæˆ\n")
    
    print("âœ… è®­ç»ƒæŠ¥å‘Šä¿å­˜åˆ°: training_report.txt")
    
    # ç»ˆç«¯è¾“å‡ºæ€»ç»“
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   âœ… å®é™…è®­ç»ƒè½®æ•°: {len(training_history['losses'])}")
    print(f"   â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f} ç§’ ({total_training_time/60:.1f} åˆ†é’Ÿ)")
    if len(training_history['losses']) > 0:
        print(f"   ğŸ“‰ åˆå§‹æŸå¤±: {training_history['losses'][0]:.6f}")
        print(f"   ğŸ“ˆ æœ€ç»ˆæŸå¤±: {training_history['losses'][-1]:.6f}")
        print(f"   ğŸ¯ æœ€ä½³æŸå¤±: {min(training_history['losses']):.6f}")
        improvement = (training_history['losses'][0] - training_history['losses'][-1]) / training_history['losses'][0] * 100 if training_history['losses'][0] > 0 else 0
        print(f"   ğŸ“Š æŸå¤±æ”¹å–„ç‡: {improvement:.1f}%")
    
    # æŸå¤±è¶‹åŠ¿åˆ†æ
    if len(training_history['losses']) >= 100:
        early_avg = np.mean(training_history['losses'][:50])
        late_avg = np.mean(training_history['losses'][-50:])
        trend = "ğŸ”½ ä¸‹é™" if late_avg < early_avg else "ğŸ”¼ ä¸Šå‡" if late_avg > early_avg else "ğŸ”„ ç¨³å®š"
        print(f"   ğŸ“ˆ æŸå¤±è¶‹åŠ¿: {trend}")
else:
    print("âš ï¸  æ— è®­ç»ƒç»“æœå¯ä¿å­˜")

print(f"\nğŸŠ ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒæµç¨‹å®Œæˆ!")
print("ğŸ¯ ä¸»è¦æˆå°±:")
print("   âœ… JAXç¯å¢ƒæˆåŠŸé…ç½®")
print("   âœ… ç¥ç»ç½‘ç»œç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
print("   âœ… å¯å¾®åˆ†ç‰©ç†å¼•æ“é›†æˆ")
print("   âœ… JITç¼–è¯‘ç«¯åˆ°ç«¯è®­ç»ƒå¾ªç¯")
if len(training_history['losses']) > 0:
    print("   âœ… æ¢¯åº¦æµç«¯åˆ°ç«¯éªŒè¯")
    print("   âœ… æŸå¤±å‡½æ•°æ”¶æ•›ç¡®è®¤")
print("   âœ… æ¨¡å‹å’Œç»“æœä¿å­˜å®Œæˆ")

print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
print(f"   - è®­ç»ƒæ¨¡å‹: /kaggle/working/safe_flight_model.pkl")
print(f"   - è®­ç»ƒæŠ¥å‘Š: /kaggle/working/training_report.txt")

print(f"\nğŸ Safe Agile Flight Kaggleè®­ç»ƒå®Œæˆ! ğŸš")